
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Approximation of vectors</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/sidebar.js"></script>

        <script src="http://sagecell.sagemath.org/static/jquery.min.js"></script>
        <script src="http://sagecell.sagemath.org/static/embedded_sagecell.js"></script>

        <script>sagecell.makeSagecell({inputLocation: ".sage"});</script>

        <style type="text/css">
                .sagecell .CodeMirror-scroll {
                        overflow-y: hidden;
                        overflow-x: auto;
                }
                .sagecell .CodeMirror {
                        height: auto;
                }
        </style>

    
    <link rel="top" title="Introduction to Numerical Methods for Variational Problems" href="index.html" />
    <link rel="next" title="Approximation principles" href="._book004.html" />
    <link rel="prev" title="Quick overview of the finite element method" href="._book002.html" />
 
  
       <style type="text/css">
         div.admonition {
           background-color: whiteSmoke;
           border: 1px solid #bababa;
         }
       </style>
      </head>
    
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="._book004.html" title="Approximation principles"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="._book002.html" title="Quick overview of the finite element method"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Introduction to Numerical Methods for Variational Problems</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="approximation-of-vectors">
<span id="fem-approx-vec"></span><h1>Approximation of vectors<a class="headerlink" href="#approximation-of-vectors" title="Permalink to this headline">¶</a></h1>
<p>We shall start with introducing two fundamental methods for
determining the coefficients <span class="math">\(c_i\)</span> in <a class="reference internal" href="._book002.html#eq-fem-u"><span class="std std-ref">(7)</span></a>. These methods
will be introduce for
approximation of vectors. Using vectors in vector
spaces to bring across the ideas is believed to appear more intuitive
to the reader than starting directly with functions in function spaces.
The extension from vectors to functions will be trivial as soon as
the fundamental ideas are understood.</p>
<p>The first method of approximation is called the <em>least squares method</em>
and consists in finding <span class="math">\(c_i\)</span> such that the difference <span class="math">\(f-u\)</span>, measured
in a certain norm, is minimized. That is, we aim at finding the best
approximation <span class="math">\(u\)</span> to <span class="math">\(f\)</span>, with the given norm as measure of &#8220;distance&#8221;.
The second method is not
as intuitive: we find <span class="math">\(u\)</span> such that the error <span class="math">\(f-u\)</span> is orthogonal to
the space where <span class="math">\(u\)</span> lies. This is known as <em>projection</em>, or
in the context of differential equations, the idea is
also well known as <em>Galerkin&#8217;s method</em>.
When approximating vectors and functions, the two methods are
equivalent, but this is no longer the case when applying the
principles to differential equations.</p>
<div class="section" id="approximation-of-planar-vectors">
<span id="fem-approx-vec-plane"></span><h2>Approximation of planar vectors<a class="headerlink" href="#approximation-of-planar-vectors" title="Permalink to this headline">¶</a></h2>
<span class="target" id="index-0"></span><span class="target" id="index-1"></span><p id="index-2">Let <span class="math">\(\boldsymbol{f} = (3,5)\)</span> be a vector in the <span class="math">\(xy\)</span> plane and suppose we want to
approximate this vector by a vector aligned in the direction of
another vector that is restricted to be aligned with some vector
<span class="math">\((a,b)\)</span>. Figure <a class="reference internal" href="#fem-approx-vec-plane-fig"><span class="std std-ref">Approximation of a two-dimensional vector in a one-dimensional vector space</span></a> depicts the
situation. This is the simplest approximation problem for
vectors. Nevertheless, for many readers it will be wise to refresh
some basic linear algebra by consulting a textbook.  <a class="reference internal" href="._book008.html#fem-approx-exer-linalg1"><span class="std std-ref">Problem 1: Linear algebra refresher</span></a> suggests specific tasks to regain
familiarity with fundamental operations on inner product vector
spaces. Familiarity with such operations are assumed in the
forthcoming text.</p>
<div class="figure" id="id1">
<span id="fem-approx-vec-plane-fig"></span><a class="reference internal image-reference" href="_images/vecapprox_plane.png"><img alt="_images/vecapprox_plane.png" src="_images/vecapprox_plane.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text"><em>Approximation of a two-dimensional vector in a one-dimensional vector space</em></span></p>
</div>
<p>We introduce the vector space <span class="math">\(V\)</span>
spanned by the vector <span class="math">\(\boldsymbol{\psi}_0=(a,b)\)</span>:</p>
<div class="math" id="eq-auto1">
\[\tag{8}
V = \mbox{span}\,\{ \boldsymbol{\psi}_0\}{\thinspace .}\]</div>
<p>We say that <span class="math">\(\boldsymbol{\psi}_0\)</span> is a <em>basis vector</em> in the space <span class="math">\(V\)</span>.
Our aim is to find the vector</p>
<div class="math" id="eq-uc0">
\[\tag{9}
\boldsymbol{u} = c_0\boldsymbol{\psi}_0\in V\]</div>
<p>which best approximates
the given vector <span class="math">\(\boldsymbol{f} = (3,5)\)</span>. A reasonable criterion for a best
approximation could be to minimize the length of the difference between
the approximate <span class="math">\(\boldsymbol{u}\)</span> and the given <span class="math">\(\boldsymbol{f}\)</span>. The difference, or error
<span class="math">\(\boldsymbol{e} = \boldsymbol{f} -\boldsymbol{u}\)</span>, has its length given by the <em>norm</em></p>
<div class="math">
\[||\boldsymbol{e}|| = (\boldsymbol{e},\boldsymbol{e})^{\frac{1}{2}},\]</div>
<p>where <span class="math">\((\boldsymbol{e},\boldsymbol{e})\)</span> is the <em>inner product</em> of <span class="math">\(\boldsymbol{e}\)</span> and itself. The inner
product, also called <em>scalar product</em> or <em>dot product</em>, of two vectors
<span class="math">\(\boldsymbol{u}=(u_0,u_1)\)</span> and <span class="math">\(\boldsymbol{v} =(v_0,v_1)\)</span> is defined as</p>
<div class="math" id="eq-auto2">
\[\tag{10}
(\boldsymbol{u}, \boldsymbol{v}) = u_0v_0 + u_1v_1{\thinspace .}\]</div>
<p><strong>Remark.</strong>
We should point out that we use the notation
<span class="math">\((\cdot,\cdot)\)</span> for two different things: <span class="math">\((a,b)\)</span> for scalar
quantities <span class="math">\(a\)</span> and <span class="math">\(b\)</span> means the vector starting in the origin and
ending in the point <span class="math">\((a,b)\)</span>, while <span class="math">\((\boldsymbol{u},\boldsymbol{v})\)</span> with vectors <span class="math">\(\boldsymbol{u}\)</span> and
<span class="math">\(\boldsymbol{v}\)</span> means the inner product of these vectors.  Since vectors are here
written in boldface font there should be no confusion.  We may add
that the norm associated with this inner product is the usual
Euclidean length of a vector, i.e.,</p>
<div class="math">
\[\|\boldsymbol{u}\| = \sqrt{(\boldsymbol{u},\boldsymbol{u})} = \sqrt{u_0^2 + u_1^2}\]</div>
<div class="section" id="the-least-squares-method-1">
<span id="index-3"></span><h3>The least squares method<a class="headerlink" href="#the-least-squares-method-1" title="Permalink to this headline">¶</a></h3>
<p>We now want to determine the <span class="math">\(\boldsymbol{u}\)</span> that minimizes  <span class="math">\(||\boldsymbol{e}||\)</span>, that is
we want to compute the optimal <span class="math">\(c_0\)</span> in <a class="reference internal" href="#eq-uc0"><span class="std std-ref">(9)</span></a>. The algebra
is simplified if we minimize the square of the norm, <span class="math">\(||\boldsymbol{e}||^2 = (\boldsymbol{e}, \boldsymbol{e})\)</span>,
instead of the norm itself.
Define the function</p>
<div class="math" id="eq-auto3">
\[\tag{11}
E(c_0) = (\boldsymbol{e},\boldsymbol{e}) = (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, \boldsymbol{f} - c_0\boldsymbol{\psi}_0)
    {\thinspace .}\]</div>
<p>We can rewrite the expressions of the right-hand side in a more
convenient form for further work:</p>
<div class="math" id="eq-fem-vec-e">
\[\tag{12}
E(c_0) = (\boldsymbol{f},\boldsymbol{f}) - 2c_0(\boldsymbol{f},\boldsymbol{\psi}_0) + c_0^2(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0){\thinspace .}\]</div>
<p>This rewrite results from using the following fundamental rules for inner
product spaces:</p>
<div class="math" id="eq-fem-vec-rule-scalarmult">
\[\tag{13}
(\alpha\boldsymbol{u},\boldsymbol{v})=\alpha(\boldsymbol{u},\boldsymbol{v}),\quad \alpha\in\mathbb{R},\]</div>
<div class="math" id="eq-fem-vec-rule-sum">
\[\tag{14}
(\boldsymbol{u} +\boldsymbol{v},\boldsymbol{w}) = (\boldsymbol{u},\boldsymbol{w}) + (\boldsymbol{v}, \boldsymbol{w}),\]</div>
<div class="math" id="eq-fem-vec-rule-symmetry">
\[\tag{15}
(\boldsymbol{u}, \boldsymbol{v}) = (\boldsymbol{v}, \boldsymbol{u}){\thinspace .}\]</div>
<p>Minimizing <span class="math">\(E(c_0)\)</span> implies finding <span class="math">\(c_0\)</span> such that</p>
<div class="math">
\[\frac{\partial E}{\partial c_0} = 0{\thinspace .}\]</div>
<p>It turns out that <span class="math">\(E\)</span> has one unique minimum and no maximum point.
Now, when differentiating <a class="reference internal" href="#eq-fem-vec-e"><span class="std std-ref">(12)</span></a> with respect to <span class="math">\(c_0\)</span>, note
that none of the inner product expressions depend on <span class="math">\(c_0\)</span>, so we simply get</p>
<div class="math" id="eq-fem-vec-dedc0-v1">
\[\tag{16}
\frac{\partial E}{\partial c_0} = -2(\boldsymbol{f},\boldsymbol{\psi}_0) + 2c_0 (\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)
    {\thinspace .}\]</div>
<p>Setting the above expression equal to zero and solving for <span class="math">\(c_0\)</span> gives</p>
<div class="math" id="eq-fem-vec-c0">
\[\tag{17}
c_0 = \frac{(\boldsymbol{f},\boldsymbol{\psi}_0)}{(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)},\]</div>
<p>which in the present case, with <span class="math">\(\boldsymbol{\psi}_0=(a,b)\)</span>, results in</p>
<div class="math" id="eq-auto4">
\[\tag{18}
c_0 = \frac{3a + 5b}{a^2 + b^2}{\thinspace .}\]</div>
<p>For later, it is worth mentioning that setting
the key equation <a class="reference internal" href="#eq-fem-vec-dedc0-v1"><span class="std std-ref">(16)</span></a> to zero and ordering
the terms lead to</p>
<div class="math">
\[(\boldsymbol{f}-c_0\boldsymbol{\psi}_0,\boldsymbol{\psi}_0) = 0,\]</div>
<p>or</p>
<div class="math" id="eq-fem-vec-dedc0-galerkin">
\[\tag{19}
(\boldsymbol{e}, \boldsymbol{\psi}_0) = 0
    {\thinspace .}\]</div>
<p>This implication of minimizing <span class="math">\(E\)</span> is an important result that we shall
make much use of.</p>
<span class="target" id="index-4"></span></div>
<div class="section" id="the-projection-method">
<span id="index-5"></span><h3>The projection method<a class="headerlink" href="#the-projection-method" title="Permalink to this headline">¶</a></h3>
<p>We shall now show that minimizing <span class="math">\(||\boldsymbol{e}||^2\)</span> implies that <span class="math">\(\boldsymbol{e}\)</span> is
orthogonal to <em>any</em> vector <span class="math">\(\boldsymbol{v}\)</span> in the space <span class="math">\(V\)</span>. This result is
visually quite clear from Figure <a class="reference internal" href="#fem-approx-vec-plane-fig"><span class="std std-ref">Approximation of a two-dimensional vector in a one-dimensional vector space</span></a> (think of
other vectors along the line <span class="math">\((a,b)\)</span>: all of them will lead to
a larger distance between the approximation and <span class="math">\(\boldsymbol{f}\)</span>).
Then we see mathematically that <span class="math">\(\boldsymbol{e}\)</span> is orthogonal to any vector <span class="math">\(\boldsymbol{v}\)</span>
in the space <span class="math">\(V\)</span> and we may
express any <span class="math">\(\boldsymbol{v}\in V\)</span> as <span class="math">\(\boldsymbol{v}=s\boldsymbol{\psi}_0\)</span> for any scalar parameter <span class="math">\(s\)</span>
(recall that two vectors are orthogonal when their inner product vanishes).
Then we calculate the inner product</p>
<div class="math">
\[\begin{split}\begin{align*}
(\boldsymbol{e}, s\boldsymbol{\psi}_0) &amp;= (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, s\boldsymbol{\psi}_0)\\
&amp;= (\boldsymbol{f},s\boldsymbol{\psi}_0) - (c_0\boldsymbol{\psi}_0, s\boldsymbol{\psi}_0)\\
&amp;= s(\boldsymbol{f},\boldsymbol{\psi}_0) - sc_0(\boldsymbol{\psi}_0, \boldsymbol{\psi}_0)\\
&amp;= s(\boldsymbol{f},\boldsymbol{\psi}_0) - s\frac{(\boldsymbol{f},\boldsymbol{\psi}_0)}{(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)}(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)\\
&amp;= s\left( (\boldsymbol{f},\boldsymbol{\psi}_0) - (\boldsymbol{f},\boldsymbol{\psi}_0)\right)\\
&amp;=0{\thinspace .}
\end{align*}\end{split}\]</div>
<p>Therefore, instead of minimizing the square of the norm, we could
demand that <span class="math">\(\boldsymbol{e}\)</span> is orthogonal to any vector in <span class="math">\(V\)</span>, which in our present
simple case amounts to a single vector only.
This method is known as <em>projection</em>.
(The approach can also be referred to as a Galerkin method as
explained at the end of the section <a class="reference internal" href="#fem-approx-vec-np1dim"><span class="std std-ref">Approximation of general vectors</span></a>.)</p>
<p>Mathematically, the projection method is stated
by the equation</p>
<div class="math" id="eq-fem-vec-galerkin1">
\[\tag{20}
(\boldsymbol{e}, \boldsymbol{v}) = 0,\quad\forall\boldsymbol{v}\in V{\thinspace .}\]</div>
<p>An arbitrary <span class="math">\(\boldsymbol{v}\in V\)</span> can be expressed as
<span class="math">\(s\boldsymbol{\psi}_0\)</span>, <span class="math">\(s\in\mathbb{R}\)</span>, and therefore
<a class="reference internal" href="#eq-fem-vec-galerkin1"><span class="std std-ref">(20)</span></a> implies</p>
<div class="math">
\[(\boldsymbol{e},s\boldsymbol{\psi}_0) = s(\boldsymbol{e}, \boldsymbol{\psi}_0) = 0,\]</div>
<p>which means that the error must be orthogonal to the basis vector in
the space <span class="math">\(V\)</span>:</p>
<div class="math">
\[(\boldsymbol{e}, \boldsymbol{\psi}_0)=0\quad\hbox{or}\quad
(\boldsymbol{f} - c_0\boldsymbol{\psi}_0, \boldsymbol{\psi}_0)=0,\]</div>
<p>which is what we found in <a class="reference internal" href="#eq-fem-vec-dedc0-galerkin"><span class="std std-ref">(19)</span></a> from
the least squares computations.</p>
</div>
</div>
<div class="section" id="approximation-of-general-vectors">
<span id="fem-approx-vec-np1dim"></span><h2>Approximation of general vectors<a class="headerlink" href="#approximation-of-general-vectors" title="Permalink to this headline">¶</a></h2>
<p id="index-6">Let us generalize the vector approximation from the previous section
to vectors in spaces with arbitrary dimension. Given some vector <span class="math">\(\boldsymbol{f}\)</span>,
we want to find the best approximation to this vector in
the space</p>
<div class="math">
\[V = \hbox{span}\,\{\boldsymbol{\psi}_0,\ldots,\boldsymbol{\psi}_N\}
{\thinspace .}\]</div>
<p>We assume that the space has dimension <span class="math">\(N+1\)</span> and
that <em>basis vectors</em> <span class="math">\(\boldsymbol{\psi}_0,\ldots,\boldsymbol{\psi}_N\)</span> are
linearly independent so that none of them are redundant.
Any vector <span class="math">\(\boldsymbol{u}\in V\)</span> can then be written as a linear combination
of the basis vectors, i.e.,</p>
<div class="math">
\[\boldsymbol{u} = \sum_{j=0}^N c_j\boldsymbol{\psi}_j,\]</div>
<p>where <span class="math">\(c_j\in\mathbb{R}\)</span> are scalar coefficients to be determined.</p>
<div class="section" id="the-least-squares-method-2">
<h3>The least squares method<a class="headerlink" href="#the-least-squares-method-2" title="Permalink to this headline">¶</a></h3>
<p>Now we want to find <span class="math">\(c_0,\ldots,c_N\)</span>, such that <span class="math">\(\boldsymbol{u}\)</span> is the best
approximation to <span class="math">\(\boldsymbol{f}\)</span> in the sense that the distance (error)
<span class="math">\(\boldsymbol{e} = \boldsymbol{f} - \boldsymbol{u}\)</span> is minimized. Again, we define
the squared distance as a function of the free parameters
<span class="math">\(c_0,\ldots,c_N\)</span>,</p>
<div class="math">
\[E(c_0,\ldots,c_N) = (\boldsymbol{e},\boldsymbol{e}) = (\boldsymbol{f} -\sum_jc_j\boldsymbol{\psi}_j,\boldsymbol{f} -\sum_jc_j\boldsymbol{\psi}_j)
\nonumber\]</div>
<div class="math" id="eq-fem-vec-gene">
\[\tag{21}
= (\boldsymbol{f},\boldsymbol{f}) - 2\sum_{j=0}^N c_j(\boldsymbol{f},\boldsymbol{\psi}_j) +
    \sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q){\thinspace .}\]</div>
<p>Minimizing this <span class="math">\(E\)</span> with respect to the independent variables
<span class="math">\(c_0,\ldots,c_N\)</span> is obtained by requiring</p>
<div class="math">
\[\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N
{\thinspace .}\]</div>
<p>The first term in <a class="reference internal" href="#eq-fem-vec-gene"><span class="std std-ref">(21)</span></a> is independent of <span class="math">\(c_i\)</span>, so its
derivative vanishes.
The second term in <a class="reference internal" href="#eq-fem-vec-gene"><span class="std std-ref">(21)</span></a> is differentiated as follows:</p>
<div class="math" id="eq-auto5">
\[\tag{22}
\frac{\partial}{\partial c_i}
    2\sum_{j=0}^N c_j(\boldsymbol{f},\boldsymbol{\psi}_j) = 2(\boldsymbol{f},\boldsymbol{\psi}_i),\]</div>
<p>since the expression to be differentiated is a sum and only one term,
<span class="math">\(c_i(\boldsymbol{f},\boldsymbol{\psi}_i)\)</span>,
contains <span class="math">\(c_i\)</span> (this term is linear in <span class="math">\(c_i\)</span>).
To understand this differentiation in detail,
write out the sum specifically for,
e.g, <span class="math">\(N=3\)</span> and <span class="math">\(i=1\)</span>.</p>
<p>The last term in <a class="reference internal" href="#eq-fem-vec-gene"><span class="std std-ref">(21)</span></a>
is more tedious to differentiate. It can be wise to write out the
double sum for <span class="math">\(N=1\)</span> and perform differentiation with respect to
<span class="math">\(c_0\)</span> and <span class="math">\(c_1\)</span> to see the structure of the expression. Thereafter,
one can generalize to an arbitrary <span class="math">\(N\)</span> and observe that</p>
<div class="math" id="eq-auto6">
\[\begin{split}\tag{23}
\frac{\partial}{\partial c_i}
    c_pc_q =
    \left\lbrace\begin{array}{ll}
    0, &amp; \hbox{ if } p\neq i\hbox{ and } q\neq i,\\
    c_q, &amp; \hbox{ if } p=i\hbox{ and } q\neq i,\\
    c_p, &amp; \hbox{ if } p\neq i\hbox{ and } q=i,\\
    2c_i, &amp; \hbox{ if } p=q= i{\thinspace .}\\
    \end{array}\right.\end{split}\]</div>
<p>Then</p>
<div class="math">
\[\frac{\partial}{\partial c_i}
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q)
= \sum_{p=0, p\neq i}^N c_p(\boldsymbol{\psi}_p,\boldsymbol{\psi}_i)
+ \sum_{q=0, q\neq i}^N c_q(\boldsymbol{\psi}_i,\boldsymbol{\psi}_q)
+2c_i(\boldsymbol{\psi}_i,\boldsymbol{\psi}_i){\thinspace .}\]</div>
<p>Since each of the two sums is missing the term <span class="math">\(c_i(\boldsymbol{\psi}_i,\boldsymbol{\psi}_i)\)</span>,
we may split the very last term in two, to get exactly that &#8220;missing&#8221;
term for each sum. This idea allows us to write</p>
<div class="math" id="eq-auto7">
\[\tag{24}
\frac{\partial}{\partial c_i}
    \sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q)
    = 2\sum_{j=0}^N c_i(\boldsymbol{\psi}_j,\boldsymbol{\psi}_i){\thinspace .}\]</div>
<p>It then follows that setting</p>
<div class="math">
\[\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N,\]</div>
<p>implies</p>
<div class="math">
\[- 2(\boldsymbol{f},\boldsymbol{\psi}_i) + 2\sum_{j=0}^N c_i(\boldsymbol{\psi}_j,\boldsymbol{\psi}_i) = 0,\quad i=0,\ldots,N{\thinspace .}\]</div>
<p>Moving the first term to the right-hand side shows that the equation is
actually a <em>linear system</em> for the unknown parameters <span class="math">\(c_0,\ldots,c_N\)</span>:</p>
<div class="math" id="eq-fem-approx-vec-np1dim-eqsys">
\[\tag{25}
\sum_{j=0}^N A_{i,j} c_j = b_i, \quad i=0,\ldots,N,\]</div>
<p>where</p>
<div class="math" id="eq-auto8">
\[\tag{26}
A_{i,j} = (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j),\]</div>
<div class="math" id="eq-auto9">
\[\tag{27}
b_i = (\boldsymbol{\psi}_i, \boldsymbol{f}){\thinspace .}\]</div>
<p>We have changed the order of the two vectors in the inner
product according to <a class="reference internal" href="#eq-fem-vec-rule-symmetry"><span class="std std-ref">(15)</span></a>:</p>
<div class="math">
\[A_{i,j} = (\boldsymbol{\psi}_j,\boldsymbol{\psi}_i) = (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j),\]</div>
<p>simply because the sequence <span class="math">\(i\)</span>-$j$ looks more aesthetic.</p>
</div>
<div class="section" id="the-galerkin-or-projection-method">
<h3>The Galerkin or projection method<a class="headerlink" href="#the-galerkin-or-projection-method" title="Permalink to this headline">¶</a></h3>
<p>In analogy with the &#8220;one-dimensional&#8221; example in
the section <a class="reference internal" href="#fem-approx-vec-plane"><span class="std std-ref">Approximation of planar vectors</span></a>, it holds also here in the general
case that minimizing the distance
(error) <span class="math">\(\boldsymbol{e}\)</span> is equivalent to demanding that <span class="math">\(\boldsymbol{e}\)</span> is orthogonal to
all <span class="math">\(\boldsymbol{v}\in V\)</span>:</p>
<span class="target" id="index-7"></span><div class="math" id="eq-fem-approx-vec-np1dim-galerkin">
<span id="index-8"></span>\[\tag{28}
(\boldsymbol{e},\boldsymbol{v})=0,\quad \forall\boldsymbol{v}\in V{\thinspace .}\]</div>
<p>Since any <span class="math">\(\boldsymbol{v}\in V\)</span> can be written as <span class="math">\(\boldsymbol{v} =\sum_{i=0}^N c_i\boldsymbol{\psi}_i\)</span>,
the statement <a class="reference internal" href="#eq-fem-approx-vec-np1dim-galerkin"><span class="std std-ref">(28)</span></a> is equivalent to
saying that</p>
<div class="math">
\[(\boldsymbol{e}, \sum_{i=0}^N c_i\boldsymbol{\psi}_i) = 0,\]</div>
<p>for any choice of coefficients <span class="math">\(c_0,\ldots,c_N\)</span>.
The latter equation can be rewritten as</p>
<div class="math">
\[\sum_{i=0}^N c_i (\boldsymbol{e},\boldsymbol{\psi}_i) =0{\thinspace .}\]</div>
<p>If this is to hold for arbitrary values of <span class="math">\(c_0,\ldots,c_N\)</span>,
we must require that each term in the sum vanishes, which means that</p>
<div class="math" id="eq-fem-approx-vec-np1dim-galerkin0">
\[\tag{29}
(\boldsymbol{e},\boldsymbol{\psi}_i)=0,\quad i=0,\ldots,N{\thinspace .}\]</div>
<p>These <span class="math">\(N+1\)</span> equations result in the same linear system as
<a class="reference internal" href="#eq-fem-approx-vec-np1dim-eqsys"><span class="std std-ref">(25)</span></a>:</p>
<div class="math">
\[(\boldsymbol{f} - \sum_{j=0}^N c_j\boldsymbol{\psi}_j, \boldsymbol{\psi}_i) = (\boldsymbol{f}, \boldsymbol{\psi}_i) - \sum_{j=0}^N
(\boldsymbol{\psi}_i,\boldsymbol{\psi}_j)c_j = 0,\]</div>
<p>and hence</p>
<div class="math">
\[\sum_{j=0}^N (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j)c_j = (\boldsymbol{f}, \boldsymbol{\psi}_i),\quad i=0,\ldots, N
{\thinspace .}\]</div>
<p>So, instead of differentiating the
<span class="math">\(E(c_0,\ldots,c_N)\)</span> function, we could simply use
<a class="reference internal" href="#eq-fem-approx-vec-np1dim-galerkin"><span class="std std-ref">(28)</span></a> as the principle for
determining <span class="math">\(c_0,\ldots,c_N\)</span>, resulting in the <span class="math">\(N+1\)</span>
equations <a class="reference internal" href="#eq-fem-approx-vec-np1dim-galerkin0"><span class="std std-ref">(29)</span></a>.</p>
<p>The names <em>least squares method</em> or <em>least squares approximation</em>
are natural since the calculations consists of
minimizing <span class="math">\(||\boldsymbol{e}||^2\)</span>, and <span class="math">\(||\boldsymbol{e}||^2\)</span> is a sum of squares
of differences between the components in <span class="math">\(\boldsymbol{f}\)</span> and <span class="math">\(\boldsymbol{u}\)</span>.
We find <span class="math">\(\boldsymbol{u}\)</span> such that this sum of squares is minimized.</p>
<p>The principle <a class="reference internal" href="#eq-fem-approx-vec-np1dim-galerkin"><span class="std std-ref">(28)</span></a>,
or the equivalent form <a class="reference internal" href="#eq-fem-approx-vec-np1dim-galerkin0"><span class="std std-ref">(29)</span></a>,
is known as <em>projection</em>. Almost the same mathematical idea
was used by the Russian mathematician <a class="reference external" href="http://en.wikipedia.org/wiki/Boris_Galerkin">Boris Galerkin</a> to solve
differential equations, resulting in what is widely known as
<em>Galerkin&#8217;s method</em>.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <center>
            <p class="logo"><a href="http://cbc.simula.no/" title="Go to Center for Biomedical Computing">
              <img class="logo" src="_static/cbc_logo.png" alt="Logo"/>
            </a></p>
            </center>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Approximation of vectors</a><ul>
<li><a class="reference internal" href="#approximation-of-planar-vectors">Approximation of planar vectors</a><ul>
<li><a class="reference internal" href="#the-least-squares-method-1">The least squares method</a></li>
<li><a class="reference internal" href="#the-projection-method">The projection method</a></li>
</ul>
</li>
<li><a class="reference internal" href="#approximation-of-general-vectors">Approximation of general vectors</a><ul>
<li><a class="reference internal" href="#the-least-squares-method-2">The least squares method</a></li>
<li><a class="reference internal" href="#the-galerkin-or-projection-method">The Galerkin or projection method</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="._book002.html"
                        title="previous chapter">Quick overview of the finite element method</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="._book004.html"
                        title="next chapter">Approximation principles</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/._book003.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="._book004.html" title="Approximation principles"
             >next</a> |</li>
        <li class="right" >
          <a href="._book002.html" title="Quick overview of the finite element method"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Introduction to Numerical Methods for Variational Problems</a> &raquo;</li> 
      </ul>
    </div>
<div class="wrapper">
  <div class="footer">
    <a href="http://cbc.simula.no"><img src="_static/cbc_banner.png" width="100%"><a>
    <br />
    <br />
      &copy;2016, Hans Petter Langtangen, Kent-Andre Mardal. Released under CC Attribution 4.0 license.
  </div>
</div>

  </body>
</html>