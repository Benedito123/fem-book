.. !split

.. _fem:approx:global:

Approximation principles
========================

.. index::
   single: approximation; of functions

Let :math:`V` be a function space spanned by a set of *basis functions*
:math:`{\psi}_0,\ldots,{\psi}_N`,

.. math::
         V = \hbox{span}\,\{{\psi}_0,\ldots,{\psi}_N\},

such that any function :math:`u\in V` can be written as a linear
combination of the basis functions:

.. _Eq:fem:approx:ufem:

.. math::

    \tag{30}
    u = \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j{\thinspace .}
        

That is, we consider functions as vectors in a vector space - a
so-called function space - and we have a finite set of basis
functions that span the space just as basis vectors or unit vectors
span a vector space.

The index set :math:`{\mathcal{I}_s}` is defined as :math:`{\mathcal{I}_s} =\{0,\ldots,N\}` and is
from now on used
both for compact notation and for flexibility in the numbering of
elements in sequences.

For now, in this introduction, we shall look at functions of a
single variable :math:`x`:
:math:`u=u(x)`, :math:`{\psi}_j={\psi}_j(x)`, :math:`j\in{\mathcal{I}_s}`. Later, we will almost
trivially extend the mathematical details
to functions of two- or three-dimensional physical spaces.
The approximation :ref:`(30) <Eq:fem:approx:ufem>` is typically used
to discretize a problem in space. Other methods, most notably
finite differences, are common for time discretization, although the
form :ref:`(30) <Eq:fem:approx:ufem>` can be used in time as well.

.. _fem:approx:LS:

The least squares method          (3)
-------------------------------------

Given a function :math:`f(x)`, how can we determine its best approximation
:math:`u(x)\in V`? A natural starting point is to apply the same reasoning
as we did for vectors in the section :ref:`fem:approx:vec:Np1dim`. That is,
we minimize the distance between :math:`u` and :math:`f`. However, this requires
a norm for measuring distances, and a norm is most conveniently
defined through an
inner product. Viewing a function as a vector of infinitely
many point values, one for each value of :math:`x`, the inner product of
two arbitrary functions :math:`f(x)` and :math:`g(x)` could
intuitively be defined as the usual summation of
pairwise "components" (values), with summation replaced by integration:

.. math::
        
        (f,g) = \int f(x)g(x)\, {\, \mathrm{d}x}
        {\thinspace .}
        

To fix the integration domain, we let :math:`f(x)` and :math:`{\psi}_i(x)`
be defined for a domain :math:`\Omega\subset\mathbb{R}`.
The inner product of two functions :math:`f(x)` and :math:`g(x)` is then

.. _Eq:fem:approx:LS:innerprod:

.. math::

    \tag{31}
    (f,g) = \int_\Omega f(x)g(x)\, {\, \mathrm{d}x}
        
        {\thinspace .}
        

The distance between :math:`f` and any function :math:`u\in V` is simply
:math:`f-u`, and the squared norm of this distance is

.. _Eq:fem:approx:LS:E:

.. math::

    \tag{32}
    E = (f(x)-\sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x), f(x)-\sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x)){\thinspace .}
        
        

Note the analogy with :ref:`(21) <Eq:fem:vec:genE>`: the given function
:math:`f` plays the role of the given vector :math:`\boldsymbol{f}`, and the basis function
:math:`{\psi}_i` plays the role of the basis vector :math:`\boldsymbol{\psi}_i`.
We can rewrite :ref:`(32) <Eq:fem:approx:LS:E>`,
through similar steps as used for the result
:ref:`(21) <Eq:fem:vec:genE>`, leading to

.. _Eq:_auto10:

.. math::

    \tag{33}
    E(c_i, \ldots, c_N) = (f,f) -2\sum_{j\in{\mathcal{I}_s}} c_j(f,{\psi}_i)
        + \sum_{p\in{\mathcal{I}_s}}\sum_{q\in{\mathcal{I}_s}} c_pc_q({\psi}_p,{\psi}_q){\thinspace .}   
        

Minimizing this function of :math:`N+1` scalar variables
:math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}`, requires differentiation
with respect to :math:`c_i`, for all :math:`i\in{\mathcal{I}_s}`. The resulting
equations are very similar to those we had in the vector case,
and we hence end up with a
linear system of the form :ref:`(25) <Eq:fem:approx:vec:Np1dim:eqsys>`, with
basically the same expressions:

.. _Eq:fem:approx:Aij:

.. math::

    \tag{34}
    A_{i,j} = ({\psi}_i,{\psi}_j),
        
        

.. _Eq:fem:approx:bi:

.. math::

    \tag{35}
    b_i = (f,{\psi}_i){\thinspace .}
        
        

The only difference from
:ref:`(25) <Eq:fem:approx:vec:Np1dim:eqsys>`
is that the inner product is defined in terms
of integration rather than summation.

The projection (or Galerkin) method
-----------------------------------

.. index::
   single: Galerkin method; functions

.. index::
   single: projection; functions

As in the section :ref:`fem:approx:vec:Np1dim`, the minimization of :math:`(e,e)`
is equivalent to

.. _Eq:fem:approx:Galerkin:

.. math::

    \tag{36}
    (e,v)=0,\quad\forall v\in V{\thinspace .}
        
        

This is known as a projection of a function :math:`f` onto the subspace :math:`V`.
We may also call it a Galerkin method for approximating functions.
Using the same reasoning as
in
:ref:`(28) <Eq:fem:approx:vec:Np1dim:Galerkin>`-:ref:`(29) <Eq:fem:approx:vec:Np1dim:Galerkin0>`,
it follows that :ref:`(36) <Eq:fem:approx:Galerkin>` is equivalent to

.. _Eq:fem:approx:Galerkin0:

.. math::

    \tag{37}
    (e,{\psi}_i)=0,\quad i\in{\mathcal{I}_s}{\thinspace .}
        
        

Inserting :math:`e=f-u` in this equation and ordering terms, as in the
multi-dimensional vector case, we end up with a linear
system with a coefficient matrix :ref:`(34) <Eq:fem:approx:Aij>` and
right-hand side vector :ref:`(35) <Eq:fem:approx:bi>`.

Whether we work with vectors in the plane, general vectors, or
functions in function spaces, the least squares principle and
the projection or Galerkin method are equivalent.

.. _fem:approx:global:linear:

Example on linear approximation
-------------------------------

Let us apply the theory in the previous section to a simple problem:
given a parabola :math:`f(x)=10(x-1)^2-1` for :math:`x\in\Omega=[1,2]`, find
the best approximation :math:`u(x)` in the space of all linear functions:

.. math::
         V = \hbox{span}\,\{1, x\}{\thinspace .}  

With our notation, :math:`{\psi}_0(x)=1`, :math:`{\psi}_1(x)=x`, and :math:`N=1`.
We seek

.. math::
         u=c_0{\psi}_0(x) + c_1{\psi}_1(x) = c_0 + c_1x,

where
:math:`c_0` and :math:`c_1` are found by solving a :math:`2\times 2` the linear system.
The coefficient matrix has elements

.. _Eq:_auto11:

.. math::

    \tag{38}
    A_{0,0} = ({\psi}_0,{\psi}_0) = \int_1^21\cdot 1\, {\, \mathrm{d}x} = 1,
        
        

.. _Eq:_auto12:

.. math::

    \tag{39}
    A_{0,1} = ({\psi}_0,{\psi}_1) = \int_1^2 1\cdot x\, {\, \mathrm{d}x} = 3/2,
        
        

.. _Eq:_auto13:

.. math::

    \tag{40}
    A_{1,0} = A_{0,1} = 3/2,
        
        

.. _Eq:_auto14:

.. math::

    \tag{41}
    A_{1,1} = ({\psi}_1,{\psi}_1) = \int_1^2 x\cdot x\,{\, \mathrm{d}x} = 7/3{\thinspace .}   
        

The corresponding right-hand side is

.. _Eq:_auto15:

.. math::

    \tag{42}
    b_1 = (f,{\psi}_0) = \int_1^2 (10(x-1)^2 - 1)\cdot 1 \, {\, \mathrm{d}x} = 7/3,
        
        

.. _Eq:_auto16:

.. math::

    \tag{43}
    b_2 = (f,{\psi}_1) = \int_1^2 (10(x-1)^2 - 1)\cdot x\, {\, \mathrm{d}x} = 13/3{\thinspace .}   
        

Solving the linear system results in

.. _Eq:_auto17:

.. math::

    \tag{44}
    c_0 = -38/3,\quad c_1 = 10,
        
        

and consequently

.. _Eq:_auto18:

.. math::

    \tag{45}
    u(x) = 10x - \frac{38}{3}{\thinspace .}   
        

Figure :ref:`fem:approx:global:fig:parabola:linear` displays the
parabola and its best approximation in the space of all linear functions.

.. _fem:approx:global:fig:parabola:linear:

.. figure:: parabola_ls_linear.png
   :width: 400

   *Best approximation of a parabola by a straight line*

.. _fem:approx:global:LS:code:

Implementation of the least squares method
------------------------------------------

Symbolic integration
~~~~~~~~~~~~~~~~~~~~

The linear system can be computed either symbolically or
numerically (a numerical integration rule is needed in the latter case).
Let us first compute the system and its solution symbolically, i.e.,
using classical "pen and paper" mathematics with symbols.
The Python package ``sympy`` can greatly help with this type of
mathematics, and will therefore be frequently used in this text.
Some basic familiarity with ``sympy`` is assumed, typically
``symbols``, ``integrate``, ``diff``, ``expand``, and ``simplify``. Much can be learned
by studying the many applications of ``sympy`` that will be presented.

Below is a function for symbolic computation of the linear system,
where :math:`f(x)` is given as a ``sympy`` expression ``f`` involving
the symbol ``x``, ``psi`` is a list of expressions for :math:`\left\{ {{\psi}}_i \right\}_{i\in{\mathcal{I}_s}}`,
and ``Omega`` is a 2-tuple/list holding the limits of the domain :math:`\Omega`:

.. code-block:: python

    import sympy as sym
    
    def least_squares(f, psi, Omega):
        N = len(psi) - 1
        A = sym.zeros((N+1, N+1))
        b = sym.zeros((N+1, 1))
        x = sym.Symbol('x')
        for i in range(N+1):
            for j in range(i, N+1):
                A[i,j] = sym.integrate(psi[i]*psi[j],
                                      (x, Omega[0], Omega[1]))
                A[j,i] = A[i,j]
            b[i,0] = sym.integrate(psi[i]*f, (x, Omega[0], Omega[1]))
        c = A.LUsolve(b)
        # Note: c is a sympy Matrix object, solution is in c[:,0]
        u = 0
        for i in range(len(psi)):
            u += c[i,0]*psi[i]
        return u, c

Observe that we exploit the symmetry of the coefficient matrix:
only the upper triangular part is computed. Symbolic integration, also in
``sympy``, is often time consuming, and (roughly) halving the
work has noticeable effect on the waiting time for the computations to
finish.


.. note::
   We remark that the symbols in ``sympy`` are created and stored in
   a symbol factory that is indexed by the expression used in the construction
   and that repeated constructions from the same expression will not create
   new objects. The following code illustrates the behavior of the
   symbol factory:
   
   .. code-block:: python
   
       >>> from sympy import *
       >>> x0 = Symbol("x")
       >>> x1 = Symbol("x")
       >>> id(x0) ==id(x1)
       True
       >>> a0 = 3.0
       >>> a1 = 3.0
       >>> id(a0) ==id(a1)
       False




Fall back on numerical integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Obviously, ``sympy`` may fail to successfully integrate
:math:`\int_\Omega{\psi}_i{\psi}_j{\, \mathrm{d}x}`, and
especially :math:`\int_\Omega f{\psi}_i{\, \mathrm{d}x}`, symbolically.
Therefore, we should extend
the ``least_squares`` function such that it falls back on
numerical integration if the symbolic integration is unsuccessful.
In the latter case, the returned value from ``sympy``'s
``integrate`` function is an object of type ``Integral``.
We can test on this type and utilize the ``mpmath`` module in
``sympy`` to perform numerical integration of high precision.
Even when ``sympy`` manages to integrate symbolically, it can
take an undesirable long time. We therefore include an
argument ``symbolic`` that governs whether or not to try
symbolic integration. Here is a complete and
improved version of the previous function ``least_squares``:

.. code-block:: python

    def least_squares(f, psi, Omega, symbolic=True):
        N = len(psi) - 1
        A = sym.zeros((N+1, N+1))
        b = sym.zeros((N+1, 1))
        x = sym.Symbol('x')
        for i in range(N+1):
            for j in range(i, N+1):
                integrand = psi[i]*psi[j]
                if symbolic:
                    I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
                if not symbolic or isinstance(I, sym.Integral):
                    # Could not integrate symbolically,
                    # fall back on numerical integration
                    integrand = sym.lambdify([x], integrand)
                    I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
                A[i,j] = A[j,i] = I
    
            integrand = psi[i]*f
            if symbolic:
                I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
            if not symbolic or isinstance(I, sym.Integral):
                integrand = sym.lambdify([x], integrand)
                I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
            b[i,0] = I
        c = A.LUsolve(b)  # symbolic solve
        # c is a sympy Matrix object, numbers are in c[i,0]
        c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]
        u = sum(c[i]*psi[i] for i in range(len(psi)))
        return u, c

The function is found in the file ``approx1D.py``.

Plotting the approximation
~~~~~~~~~~~~~~~~~~~~~~~~~~

Comparing the given :math:`f(x)` and the approximate :math:`u(x)` visually is done
by the following function, which utilizes ``sympy``'s ``lambdify`` tool to
convert a ``sympy`` expression to a Python function for numerical
computations:

.. code-block:: python

    def comparison_plot(f, u, Omega, filename='tmp.pdf'):
        x = sym.Symbol('x')
        f = sym.lambdify([x], f, modules="numpy")
        u = sym.lambdify([x], u, modules="numpy")
        resolution = 401  # no of points in plot
        xcoor  = linspace(Omega[0], Omega[1], resolution)
        exact  = f(xcoor)
        approx = u(xcoor)
        plot(xcoor, approx)
        hold('on')
        plot(xcoor, exact)
        legend(['approximation', 'exact'])
        savefig(filename)

The ``modules='numpy'`` argument to ``lambdify`` is important
if there are mathematical functions, such as ``sin`` or ``exp``
in the symbolic expressions in ``f`` or ``u``, and these
mathematical functions are to be used with vector arguments, like
``xcoor`` above.

Both the ``least_squares`` and ``comparison_plot`` functions are found in
the file `approx1D.py <http://tinyurl.com/znpudbt/approx1D.py>`__.  The
``comparison_plot`` function in this file is more advanced and flexible
than the simplistic version shown above.  The file ``ex_approx1D.py``
applies the ``approx1D`` module to accomplish the forthcoming examples.


.. note::
   We remind the reader that the code examples can be found in a
   tarball at
   `<http://hplgit.github.io/fem-book/doc/web/>`_.
   The following command shows a useful way to search for code
   
   .. code-block:: bash
   
       Terminal> find . -name '*.py' -exec grep least_squares {} \; -print
   
   Here ``'.'`` specifies the directory for the search, ``-name '*.py'`` that
   files with suffix ``*.py`` should be searched through while
   
   .. code-block:: text
   
       -exec grep least_squares {} \; -print
   
   means that all lines
   containing the text ``least_squares`` should be printed to the screen.




.. _fem:approx:global:exact1:

Perfect approximation
---------------------

Let us use the code above to recompute the problem from
the section :ref:`fem:approx:global:linear` where we want to approximate
a parabola. What happens if we add an element :math:`x^2` to the basis and test what
the best approximation is if :math:`V` is the space of all parabolic functions?
The answer is quickly found by running

.. code-block:: python

    >>> from approx1D import *
    >>> x = sym.Symbol('x')
    >>> f = 10*(x-1)**2-1
    >>> u, c = least_squares(f=f, psi=[1, x, x**2], Omega=[1, 2])
    >>> print u
    10*x**2 - 20*x + 9
    >>> print sym.expand(f)
    10*x**2 - 20*x + 9

Now, what if we use :math:`{\psi}_i(x)=x^i` for :math:`i=0,1,\ldots,N=40`?
The output from ``least_squares`` gives :math:`c_i=0` for :math:`i>2`, which
means that the method finds the perfect approximation.

In fact, we have a general result that
if :math:`f\in V`, the least squares and projection/Galerkin methods compute
the exact solution :math:`u=f`.
The proof is straightforward: if :math:`f\in V`, :math:`f` can be expanded in
terms of the basis functions, :math:`f=\sum_{j\in{\mathcal{I}_s}} d_j{\psi}_j`, for
some coefficients :math:`\left\{ {d}_j \right\}_{j\in{\mathcal{I}_s}}`,
and the right-hand side then has entries

.. math::
         b_i = (f,{\psi}_i) = \sum_{j\in{\mathcal{I}_s}} d_j({\psi}_j, {\psi}_i) = \sum_{j\in{\mathcal{I}_s}} d_jA_{i,j}
        {\thinspace .}  

The linear system :math:`\sum_jA_{i,j}c_j = b_i`, :math:`i\in{\mathcal{I}_s}`, is then

.. math::
         \sum_{j\in{\mathcal{I}_s}} c_jA_{i,j} = \sum_{j\in{\mathcal{I}_s}}d_jA_{i,j},
        \quad i\in{\mathcal{I}_s},

which implies that :math:`c_i=d_i` for :math:`i\in{\mathcal{I}_s}`.

.. _fem:approx:global:regression:

The regression method
---------------------

So far, the function to be approximated has been known in terms of
a formula :math:`f(x)`. Very often in applications, no formula is known, but
the function value is known at a set of points. If we use :math:`N+1` basis
functions and know exactly :math:`N+1` function values, we can determine the
coefficients :math:`c_i` by *interpolation* as explained in the section :ref:`fem:approx:global:interp`. The approximating function will then
equal the :math:`f` values at the points where the :math:`f` values are sampled.

However, one normally has :math:`f` sampled at a lot of points, here denoted
by :math:`x_{0},x_{1},\ldots,x_{m}`, and we assume :math:`m\gg N`. What can
we do then to determine the coefficients?  The answer is to find a
least squares approximation.  The resulting method is called
*regression* and is well known from statistics when fitting a simple
(usually polynomial) function to a set of data points.

Overdetermined equation system
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Intuitively, we would demand :math:`u` to equal :math:`f` at all the
data points :math:`x_{i}`, :math:`i0,1,\ldots,m`,

.. _Eq:_auto19:

.. math::

    \tag{46}
    u(x_{i}) = \sum_{j\in{\mathcal{I}_s}} c_j {\psi}_j(x_{i}) = f(x_{i}),
        \quad i=0,1,\ldots,m{\thinspace .}
        
        

The fundamental problem here is that we have more equations than
unknowns since there are :math:`N+1` unknowns and :math:`m+1>N+1` equations.
Such a system of equations is called an *overdetermined system*.
We can write it in matrix form as

.. _Eq:_auto20:

.. math::

    \tag{47}
    \sum_{j\in{\mathcal{I}_s}} A_{i,j}c_j = b_i,\quad i=0,1,\ldots,m,
        
        

with coefficient matrix and right-hand side vector given by

.. _Eq:fem:approx:global:regression:Aij:

.. math::

    \tag{48}
    A_{i,j} = {\psi}_j(x_{i}),
        
        

.. _Eq:fem:approx:global:regression:bi:

.. math::

    \tag{49}
    b_i = f(x_{i}){\thinspace .}
        
        

Note that the matrix is a *rectangular* :math:`(m+1)\times(N+1)`
matrix since :math:`i=0,\ldots,m` and :math:`j=0,\ldots,N`.

The normal equations derived from a least squares principle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The least squares method is a common technique for solving
overdetermined equations systems. Let us write the overdetermined
system :math:`\sum_{j\in{\mathcal{I}_s}} A_{i,j}c_j = b_i` more compactly in matrix form
as :math:`Ac=b`.  Since we have more equations than unknowns, it is (in
general) impossible to find a vector :math:`c` that fulfills :math:`Ac=b`. The
best we can do is to make the residual :math:`r=b-Ac` as small as
possible. That is, we can find :math:`c` such that it minimizes the norm
Euclidean norm of :math:`r`: :math:`||r||`.  The algebra simplifies significantly
by minimizing :math:`||r||^2` instead.  This principle corresponds to a
least squares method.

.. index:: normal equations

.. index:: A^TA=A^Tb (normal equations)

The :math:`i`-th component of :math:`r` reads :math:`r_i = b_i -\sum_jA_{i,j}c_j`,
so :math:`||r||^2 = \sum_ir_i^2`.
Minimizing :math:`||r||^2` with respect to the unknowns :math:`c_0,\ldots,c_N`
implies that

.. _Eq:fem:approx:global:regression:r2min:

.. math::

    \tag{50}
    \frac{\partial}{\partial c_k}||r||^2=0,\quad k=0,\ldots,N,
        
        

which leads to

.. math::
         \frac{\partial}{\partial c_k}\sum_i r_i^2 =
        \sum_i 2r_i\frac{\partial r_i}{\partial c_k}
        =\sum_i 2r_i \frac{\partial}{\partial c_k}(b_i -\sum_jA_{i,j}c_j)
        = 2\sum_i r_i(-A_{i,k}) = 0{\thinspace .}

By inserting :math:`r_i = b_i -\sum_jA_{i,j}c_j` in the last expression we
get

.. math::
         \sum_i\left(b_i -\sum_jA_{i,j}c_j\right)\left(-A_{i,k}\right)
        = -\sum_i b_iA_{i,k} + \sum_j (\sum_i A_{i,j}A_{i,k})c_j = 0{\thinspace .}

Introducing the transpose of :math:`A`, :math:`A^T`, we know that :math:`A^T_{i,j}=A_{j,i}`.
Therefore, the expression :math:`\sum_i A_{i,j}A_{i,k}` can be written
as :math:`\sum_i A^T_{k,i}A_{i,j}` and be recognized as the formula for the
matrix-matrix product :math:`A^TA`. Also, :math:`\sum_i b_i A_{i,k}` can be written
:math:`\sum_i A^T_{k,i}b_i` and recognized as the matrix-vector product
:math:`A^Tb`. These observations imply that :ref:`(50) <Eq:fem:approx:global:regression:r2min>`
is equivalent to the linear system

.. _Eq:fem:approx:global:regression:normal1:

.. math::

    \tag{51}
    \sum_j (\sum_i A^T_{k,i}A_{i,j})c_j=\sum_j(A^TA)_{k,j}
        c_j = \sum_i  A^T_{k,i}b_i=(A^Tb)_k,\quad k=0,\ldots,N,
        
        

or in matrix form,

.. _Eq:fem:approx:global:regression:normal2:

.. math::

    \tag{52}
    A^TA c = A^Tb{\thinspace .}
        
        

The equation system :ref:`(51) <Eq:fem:approx:global:regression:normal1>` or
:ref:`(52) <Eq:fem:approx:global:regression:normal2>` are known as the
*normal equations*.
With :math:`A` as an :math:`(m+1)\times (N+1)` matrix, :math:`A^TA` becomes an :math:`(N+1)\times (N+1)`
matrix, and :math:`A^Tb` becomes a vector of length :math:`N+1`. Often, :math:`m\gg N`,
so :math:`A^TA` is much smaller than :math:`A`.

Many prefer to write the linear system
:ref:`(51) <Eq:fem:approx:global:regression:normal1>` on the standard form
:math:`\sum_j B_{i,j}c_j=d_i`, :math:`i=0,\ldots,N`.  We can easily do so by
exchanging the :math:`i` and :math:`k` index (:math:`i\leftrightarrow k`), :math:`\sum_i
A^T_{k,i}A_{i,j} = \sum_k A^T_{i,k}A_{k,j}`, and setting :math:`B_{i,j}=\sum_k
A^T_{i,k}A_{k,j}`. Similarly, we exchange :math:`i` and :math:`k` in the right-hand
side expression and get :math:`\sum_k A^T_{i,k}b_k = d_i`.  Expressing
:math:`B_{i,j}` and :math:`d_i` in terms of the :math:`{\psi}_i` and :math:`x_{i}`, using
:ref:`(48) <Eq:fem:approx:global:regression:Aij>` and
:ref:`(49) <Eq:fem:approx:global:regression:bi>`, we end up with the formulas

.. _Eq:fem:approx:global:regression:Bij:

.. math::

    \tag{53}
    B_{i,j} = \sum_k A^T_{i,k}A_{k,j} = \sum_k A_{k,i}A_{k,j}
        =\sum_{k=0}^m{\psi}_i(x_{k}){\psi}_j(x_{k}),
        
        

.. _Eq:fem:approx:global:regression:di:

.. math::

    \tag{54}
    d_i =\sum_k A^T_{i,k}b_k = \sum_k A_{k,i}b_k =\sum_{k=0}^m
        {\psi}_i(x_{k})f(x_{k})
        
        

Implementation          (1)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following function defines the matrix entries :math:`B_{i,j}` according
to :ref:`(53) <Eq:fem:approx:global:regression:Bij>` and the right-hand side
entries :math:`d_i` according
:ref:`(54) <Eq:fem:approx:global:regression:di>`. Thereafter, it solves the
linear system :math:`\sum_jB_{i,j}c_j=d_i`.  The input data ``f`` and ``psi``
hold :math:`f(x)` and :math:`{\psi}_i`, :math:`i=0,\ldots,N`, as symbolic expression, but
since :math:`m` is thought to be much larger than :math:`N`, and there are loops
from :math:`0` to :math:`m`, we use numerical computing to speed up the
computations.

.. code-block:: python

    def regression(f, psi, points):
        N = len(psi) - 1
        m = len(points)
        # Use numpy arrays and numerical computing
        B = np.zeros((N+1, N+1))
        d = np.zeros(N+1)
        # Wrap psi and f in Python functions rather than expressions
        # so that we can evaluate psi at points[i]
        x = sym.Symbol('x')
        psi_sym = psi  # save symbolic expression
        psi = [sym.lambdify([x], psi[i]) for i in range(N+1)]
        f = sym.lambdify([x], f)
        for i in range(N+1):
            for j in range(N+1):
                B[i,j] = 0
                for k in range(m+1):
                    B[i,j] += psi[i](points[k])*psi[j](points[k])
            d[i] = 0
            for k in range(m+1):
                d[i] += psi[i](points[k])*f(points[k])
        c = np.linalg.solve(B, d)
        u = sum(c[i]*psi_sym[i] for i in range(N+1))
        return u, c

Example          (1)
~~~~~~~~~~~~~~~~~~~~

We repeat the computational example from the section :ref:`fem:approx:global:interp`, but this time with many more
points. The parabola :math:`f(x)=10(x-1)^2-1` is to be approximated by a
linear function on :math:`\Omega=[1,2]`. We divide :math:`\Omega` into :math:`m+2`
intervals and use the inner :math:`m+1` points:

.. code-block:: python

    import sympy as sym
    x = sym.Symbol('x')
    f = 10*(x-1)**2 - 1
    psi = [1, x]
    Omega = [1, 2]
    m_values = [2-1, 8-1, 64-1]
    # Create m+3 points and use the inner m+1 points
    for m in m_values:
        points = np.linspace(Omega[0], Omega[1], m+3)[1:-1]
        u, c = regression(f, psi, points)
        comparison_plot(
            f, u, Omega,
            filename='parabola_by_regression_%d' % (m+1),
            points=points,
            points_legend='%d interpolation points' % (m+1),
            legend_loc='upper left')

Figure :ref:`fem:approx:global:linear:regression:fig1` shows results for
:math:`m+1=2` (left), :math:`m+1=8` (middle), and :math:`m+1=64` (right) data points.
The approximating function is not so sensitive to the number of
points as long as they cover a significant part of the domain (the first 2 point approximation puts too much weight on the center,
while the 8 point approximation cover almost the entire
domain and produces a good approximation which is barely improved with 64 points):

.. math::
        \begin{align*}
        u(x) &= 10x - 13.2,\quad 2\hbox{ points}\\ 
        u(x) &= 10x - 12.7,\quad 8\hbox{ points}\\ 
        u(x) &= 10x - 12.7,\quad 64\hbox{ points}
        \end{align*}

.. _fem:approx:global:linear:regression:fig1:

.. figure:: parabola_by_regression.png
   :width: 800

   *Approximation of a parabola by a regression method with varying number of points*

To explicitly make the link to classical regression in statistics, we
consider :math:`f=10(x-1)^2 - 1 + \epsilon`, where :math:`\epsilon` is a random,
normally distributed variable. The goal in classical regression is
to find the straight line that best fits the data points (in a least
squares sense). The only difference from the previous setup, is that
the :math:`f(x_{i})` values are based on a function formula, here :math:`10(x-1)^2-1`,
*plus* normally distributed noise.
Figure :ref:`fem:approx:global:linear:regression:fig2` shows three sets of
data points, along with the original :math:`f(x)` function without noise, and
the straight line that is a least squares approximation to the data points.

.. python ex_approx1D.py run_noisy_parabola_by_linear_regression

.. _fem:approx:global:linear:regression:fig2:

.. figure:: noisy_parabola_by_linear_regression.png
   :width: 800

   *Approximation of a parabola with noise by a straight line*

We can fit a parabola instead of a straight line, as done in
Figure :ref:`fem:approx:global:linear:regression:fig3`. When :math:`m` becomes large,
the fitted parabola and the original parabola without noise become very close.

.. python ex_approx1D.py run_noisy_parabola_by_quaddratic_regression

.. _fem:approx:global:linear:regression:fig3:

.. figure:: noisy_parabola_by_quadratic_regression.png
   :width: 800

   *Approximation of a parabola with noise by a parabola*

The regression method is not much used for approximating differential
equations or a given function, but is central in uncertainty quantification
methods such as polynomial chaos expansions.


.. admonition:: The residual: an indirect but computationally cheap measure of the error

   When attempting to  solve
   a system :math:`A c = b`, we may question how far off a vector :math:`c_0` is.
   The error is clearly the difference between :math:`c` and :math:`c_0`, :math:`e=c-c_0`.
   The vector :math:`c_0` is obviously the solution of the problem :math:`A c_0 = b_0`,
   where :math:`b_0` is easily computable as the matrix vector product :math:`A c_0`.
   The residual can be seen as the error of the input data, :math:`b - b_0`
   and is defined
   
   .. math::
           
           r = b - A c_0 .
           
   
   Clearly, the error and the residual are related by
   
   .. math::
           
           A e = r .
           
   
   While the computation of the error requires inversion of :math:`A`,
   which may be computationally expensive, the residual
   is easily computable.
   
   [**hpl 1**: Here something is missing? What is actually the point? Try to insert a heading in the notice...]




