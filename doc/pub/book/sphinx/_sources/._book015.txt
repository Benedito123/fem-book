.. !split

.. _ch:varform:global:

Variational formulations with global basis functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The finite element method is a very flexible approach for solving partial
differential equations. Its two most attractive features are the ease
of handling domains of complex shape in two and three dimensions and
the ease of using higher-degree polynomials in the approximations.
The latter feature typically leads to errors proportional to
:math:`h^{d+1}`, where :math:`h` is the element length and :math:`d` is the polynomial
degree. When the solution is sufficiently smooth, the ability to use
larger :math:`d` creates methods that are much more computationally efficient
than standard finite difference methods (and equally efficient finite
difference methods are technically much harder to construct).

However, before we attack finite element methods, with localized basis
functions, it can be easier from a pedagogical point of view to study
approximations by global functions because the mathematics in this
case gets simpler.

.. _fem:deq:1D:principles:

Basic principles for approximating differential equations
=========================================================

The finite element method is usually applied for discretization in
space, and therefore spatial problems will be our focus in the coming
sections.  Extensions to time-dependent problems usually employs
finite difference approximations in time.

The coming section addres at how global basis functions and the least
squares, Galerkin, and collocation principles can be used to solve
differential equations.

.. _fem:deq:1D:models:

Differential equation models
----------------------------

Let us consider an abstract differential equation for a function :math:`u(x)` of
one variable, written as

.. _Eq:_auto68:

.. math::

    \tag{136}
    \mathcal{L}(u) = 0,\quad x\in\Omega{\thinspace .}   
        

Here are a few examples on possible choices of :math:`\mathcal{L}(u)`, of
increasing complexity:

.. _Eq:fem:deq:1D:L1:

.. math::

    \tag{137}
    \mathcal{L}(u) = \frac{d^2u}{dx^2} - f(x),
        
        

.. _Eq:fem:deq:1D:L2:

.. math::

    \tag{138}
    \mathcal{L}(u) = \frac{d}{dx}\left({\alpha}(x)\frac{du}{dx}\right) + f(x),
        
        

.. _Eq:fem:deq:1D:L3:

.. math::

    \tag{139}
    \mathcal{L}(u) = \frac{d}{dx}\left({\alpha}(u)\frac{du}{dx}\right) - au + f(x),
        
        

.. _Eq:fem:deq:1D:L4:

.. math::

    \tag{140}
    \mathcal{L}(u) = \frac{d}{dx}\left({\alpha}(u)\frac{du}{dx}\right) + f(u,x)
        
        {\thinspace .}
        

Both :math:`{\alpha}(x)` and :math:`f(x)` are considered as specified functions,
while :math:`a` is a prescribed parameter.  Differential equations
corresponding to :ref:`(137) <Eq:fem:deq:1D:L1>`-:ref:`(138) <Eq:fem:deq:1D:L2>` arise in
diffusion phenomena, such as stationary (time-independent)
transport of heat in solids and
flow of viscous fluids between flat plates. The form
:ref:`(139) <Eq:fem:deq:1D:L3>` arises when transient diffusion or wave
phenomena are discretized in time by finite differences. The equation
:ref:`(140) <Eq:fem:deq:1D:L4>` appears in chemical models when diffusion of a
substance is combined with chemical reactions. Also in biology,
:ref:`(140) <Eq:fem:deq:1D:L4>` plays an important role, both for spreading of
species and in models involving generation and
propagation of electrical signals.

Let :math:`\Omega =[0,L]` be the domain in one space dimension.
In addition to the differential equation, :math:`u` must fulfill
boundary conditions at the boundaries of the domain, :math:`x=0` and :math:`x=L`.
When :math:`\mathcal{L}` contains up to second-order derivatives, as in the
examples above, we need one boundary condition at each of
the (two) boundary points, here abstractly specified as

.. _Eq:_auto69:

.. math::

    \tag{141}
    \mathcal{B}_0(u)=0,\ x=0,\quad \mathcal{B}_1(u)=0,\ x=L
        
        

There are three common choices of boundary conditions:

.. _Eq:_auto70:

.. math::

    \tag{142}
    \mathcal{B}_i(u) = u - g,\quad \hbox{Dirichlet condition}
        
        

.. _Eq:_auto71:

.. math::

    \tag{143}
    \mathcal{B}_i(u) = -{\alpha} \frac{du}{dx} - g,\quad \hbox{Neumann condition}
        
        

.. _Eq:_auto72:

.. math::

    \tag{144}
    \mathcal{B}_i(u) = -{\alpha} \frac{du}{dx} - H(u-g),\quad \hbox{Robin condition}
        
        

Here, :math:`g` and :math:`H` are specified quantities.

From now on we shall use :math:`{u_{\small\mbox{e}}}(x)` as symbol for the *exact* solution,
fulfilling

.. _Eq:_auto73:

.. math::

    \tag{145}
    \mathcal{L}({u_{\small\mbox{e}}})=0,\quad x\in\Omega,
        
        

while :math:`u(x)` is our notation for an *approximate* solution of the differential
equation.


.. admonition:: Remark on notation

   In the literature about the finite element method,
   it is common to use :math:`u` as the exact solution and :math:`u_h` as the
   approximate solution, where :math:`h` is a discretization parameter. However,
   the vast part of the present text is about the approximate solutions,
   and having a subscript :math:`h` attached all the time
   is cumbersome. Of equal importance is the close correspondence between
   implementation and mathematics that we strive to achieve in this text:
   when it is natural to use ``u`` and not ``u_h`` in
   code, we let the mathematical notation be dictated by the code's
   preferred notation. In the relatively few cases where we need to work
   with the exact solution of the PDE problem we call it :math:`{u_{\small\mbox{e}}}` in
   mathematics and ``u_e`` in the code (the function for computing
   ``u_e`` is named ``u_exact``).
   
   .. After all, it is the powerful computer implementations
   
   .. of the finite element method that justifies studying the mathematical
   
   .. formulation and aspects of the method.




.. _fem:deq:1D:models:simple:

Simple model problems and their solutions
-----------------------------------------

A common model problem used much in the forthcoming examples is

.. _Eq:fem:deq:1D:model1:

.. math::

    \tag{146}
    -u''(x) = f(x),\quad x\in\Omega=[0,L],\quad u(0)=0,\ u(L)=D
        {\thinspace .}
        
        

A closely related problem with a different boundary condition at
:math:`x=0` reads

.. _Eq:fem:deq:1D:model2:

.. math::

    \tag{147}
    -u''(x) = f(x),\quad x\in\Omega=[0,L],\quad u'(0)=C,\ u(L)=D{\thinspace .}
        
        

A third variant has a variable coefficient,

.. _Eq:fem:deq:1D:model3:

.. math::

    \tag{148}
    -({\alpha}(x)u'(x))' = f(x),\quad x\in\Omega=[0,L],\quad u'(0)=C,\ u(L)=D{\thinspace .}
        
        

The solution :math:`u` to the model problem :ref:`(146) <Eq:fem:deq:1D:model1>`
can be determined as

.. math::
        \begin{align*}
        u'(x) &= -\int_0^x f(x) + c_0, \\ 
        u(x) &= \int_0^x u'(x) + c_1,
        \end{align*}

where :math:`c_0` and :math:`c_1` are determined by the boundary conditions
such that :math:`u'(0) = C` and :math:`u(L) = D`.

Computing the solution is easily done
using ``sympy``. Some common code is defined first:

.. code-block:: python

    import sympy as sym
    x, L, C, D, c_0, c_1, = sym.symbols('x L C D c_0 c_1')

The following function computes the solution
symbolically for the model problem :ref:`(146) <Eq:fem:deq:1D:model1>`:

.. code-block:: python

    def model1(f, L, D):
        """Solve -u'' = f(x), u(0)=0, u(L)=D."""
        # Integrate twice
        u_x = - sym.integrate(f, (x, 0, x)) + c_0
        u = sym.integrate(u_x, (x, 0, x)) + c_1
        # Set up 2 equations from the 2 boundary conditions and solve
        # with respect to the integration constants c_0, c_1
        r = sym.solve([u.subs(x, 0)-0,  # x=0 condition
                       u.subs(x,L)-D],  # x=L condition
                      [c_0, c_1])       # unknowns
        # Substitute the integration constants in the solution
        u = u.subs(c_0, r[c_0]).subs(c_1, r[c_1])
        u = sym.simplify(sym.expand(u))
        return u

Calling ``model1(2, L, D)`` results in the solution

.. _Eq:fem:deq:1D:model1:sol:

.. math::

    \tag{149}
    u(x) = \frac{1}{L}x \left(D + L^{2} - L x\right)
        
        

The model problem :ref:`(147) <Eq:fem:deq:1D:model2>` can be solved by

.. code-block:: python

    def model2(f, L, C, D):
        """Solve -u'' = f(x), u'(0)=C, u(L)=D."""
        u_x = - sym.integrate(f, (x, 0, x)) + c_0
        u = sym.integrate(u_x, (x, 0, x)) + c_1
        r = sym.solve([sym.diff(u,x).subs(x, 0)-C,  # x=0 cond.
                       u.subs(x,L)-D],              # x=L cond.
                      [c_0, c_1])
        u = u.subs(c_0, r[c_0]).subs(c_1, r[c_1])
        u = sym.simplify(sym.expand(u))
        return u

to yield

.. _Eq:fem:deq:1D:model2:sol:

.. math::

    \tag{150}
    u(x) = - x^{2} + C x - C L + D + L^{2},
        
        

if :math:`f(x)=2`. Model :ref:`(148) <Eq:fem:deq:1D:model3>` requires a bit more involved
code,

.. code-block:: python

    def model3(f, a, L, C, D):
        """Solve -(a*u')' = f(x), u(0)=C, u(L)=D."""
        au_x = - sym.integrate(f, (x, 0, x)) + c_0
        u = sym.integrate(au_x/a, (x, 0, x)) + c_1
        r = sym.solve([u.subs(x, 0)-C,
                       u.subs(x,L)-D],
                      [c_0, c_1])
        u = u.subs(c_0, r[c_0]).subs(c_1, r[c_1])
        u = sym.simplify(sym.expand(u))
        return u
    
    
    def demo():
        f = 2
        u = model1(f, L, D)
        print 'model1:', u, u.subs(x, 0), u.subs(x, L)
        print sym.latex(u, mode='plain')
        u = model2(f, L, C, D)
        #f = x
        #u = model2(f, L, C, D)
        print 'model2:', u, sym.diff(u, x).subs(x, 0), u.subs(x, L)
        print sym.latex(u, mode='plain')
        u = model3(0, 1+x**2, L, C, D)
        print 'model3:', u, u.subs(x, 0), u.subs(x, L)
        print sym.latex(u, mode='plain')
    
    if __name__ == '__main__':
        demo()

With :math:`f(x)=0` and :math:`{\alpha}(x)=1+x^2` we get

.. math::
         u(x) =
        \frac{C \tan^{-1}\left (L \right ) - C \tan^{-1}\left (x \right ) + D \tan^{-1}\left (x \right )}{\tan^{-1}\left (L \right )}
        

.. _fem:deq:1D:residual:min:

Forming the residual
--------------------

The fundamental idea is to seek an approximate solution
:math:`u` in some space :math:`V`,

.. math::
        
        V = \hbox{span}\{ {\psi}_0(x),\ldots,{\psi}_N(x)\},
        

which means that :math:`u` can always be expressed as a linear combination
of the basis functions :math:`\left\{ {{\psi}}_j \right\}_{j\in{\mathcal{I}_s}}`, with :math:`{\mathcal{I}_s}` as
the index set :math:`\{0,\ldots,N\}`:

.. math::
         u(x) = \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x){\thinspace .}

The coefficients :math:`\left\{ {c}_j \right\}_{j\in{\mathcal{I}_s}}` are unknowns to be computed.

(Later, in the section :ref:`fem:deq:1D:essBC`, we will see that if we specify boundary values of :math:`u` different
from zero, we must look for an approximate solution
:math:`u(x) = B(x) + \sum_{j} c_j{\psi}_j(x)`,
where :math:`\sum_{j}c_j{\psi}_j\in V` and :math:`B(x)` is some function for
incorporating the right boundary values. Because of :math:`B(x)`, :math:`u` will not
necessarily lie in :math:`V`. This modification does not imply any difficulties.)

We need principles for deriving :math:`N+1` equations to determine the
:math:`N+1` unknowns :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}`.
When approximating a given function :math:`f` by :math:`u=\sum_jc_j{\varphi}_j`,
a key idea is to minimize the square norm of the
approximation error :math:`e=u-f` or (equivalently) demand that :math:`e` is
orthogonal to :math:`V`. Working with :math:`e` is not so useful here since
the approximation error in our case is :math:`e={u_{\small\mbox{e}}} - u` and :math:`{u_{\small\mbox{e}}}` is
unknown. The only general indicator we have on the quality of the approximate
solution is to what degree :math:`u` fulfills the differential equation.
Inserting :math:`u=\sum_j c_j {\psi}_j` into :math:`\mathcal{L}(u)` reveals that the
result is not zero, because :math:`u` in general is an approximation and not identical to :math:`{u_{\small\mbox{e}}}`.
The nonzero result,

.. index:: residual

.. _Eq:_auto74:

.. math::

    \tag{151}
    R = \mathcal{L}(u) = \mathcal{L}(\sum_j c_j {\psi}_j),
        
        

is called the *residual* and measures the
error in fulfilling the governing equation.

Various principles for determining :math:`\left\{ {c}_j \right\}_{j\in{\mathcal{I}_s}}` try to minimize
:math:`R` in some sense. Note that :math:`R` varies with :math:`x` and
the :math:`\left\{ {c}_j \right\}_{j\in{\mathcal{I}_s}}` parameters. We may write this dependence
explicitly as

.. _Eq:_auto75:

.. math::

    \tag{152}
    R = R(x; c_0, \ldots, c_N){\thinspace .}   
        

Below, we present three principles for making :math:`R` small:
a least squares method, a projection or Galerkin method, and
a collocation or interpolation method.

The least squares method          (4)
-------------------------------------

The least-squares method aims to find :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}` such that
the square norm of the residual

.. _Eq:_auto76:

.. math::

    \tag{153}
    ||R|| = (R, R) = \int_{\Omega} R^2 {\, \mathrm{d}x}
        
        

is minimized. By introducing
an inner product of two functions :math:`f` and :math:`g`
on :math:`\Omega` as

.. _Eq:_auto77:

.. math::

    \tag{154}
    (f,g) = \int_{\Omega} f(x)g(x) {\, \mathrm{d}x},
        
        

the least-squares method can be defined as

.. _Eq:_auto78:

.. math::

    \tag{155}
    \min_{c_0,\ldots,c_N} E = (R,R){\thinspace .}   
        

Differentiating with respect to the free parameters :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}`
gives the :math:`N+1` equations

.. _Eq:fem:deq:1D:LS:eq1:

.. math::

    \tag{156}
    \int_{\Omega} 2R\frac{\partial R}{\partial c_i} {\, \mathrm{d}x} = 0\quad
        \Leftrightarrow\quad (R,\frac{\partial R}{\partial c_i})=0,\quad
        i\in{\mathcal{I}_s}{\thinspace .}
        
        

The Galerkin method          (1)
--------------------------------

The least-squares
principle is equivalent to demanding the error to be orthogonal to
the space :math:`V` when approximating a function :math:`f` by :math:`u\in V`.
With a differential equation
we do not know the true error so we must instead require the residual :math:`R`
to be orthogonal to :math:`V`. This idea implies
seeking :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}` such that

.. _Eq:fem:deq:1D:Galerkin0:

.. math::

    \tag{157}
    (R,v)=0,\quad \forall v\in V{\thinspace .}
        
        

This is the Galerkin method for differential equations.

.. As shown in :ref:`(28) <Eq:fem:approx:vec:Np1dim:Galerkin>` and :ref:`(29) <Eq:fem:approx:vec:Np1dim:Galerkin0>`,

The above abstract statement can be made concrete by choosing a concrete basis.
For example, the statement is equivalent to :math:`R` being orthogonal to the :math:`N+1`
basis functions :math:`\{{\psi}_i\}` spanning :math:`V` (and this is
the most convenient way to express :ref:`(157) <Eq:fem:deq:1D:Galerkin0>`:

.. _Eq:fem:deq:1D:Galerkin:

.. math::

    \tag{158}
    (R,{\psi}_i)=0,\quad i\in{\mathcal{I}_s},
        
        

resulting in :math:`N+1` equations for determining :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}`.

The method of weighted residuals
--------------------------------

.. index:: weighted residuals

.. index:: method of weighted residuals

A generalization of the Galerkin method is to demand that :math:`R`
is orthogonal to some space :math:`W`, but not necessarily the same
space as :math:`V` where we seek the unknown function.
This generalization is called the *method of weighted residuals*:

.. _Eq:fem:deq:1D:WRM0:

.. math::

    \tag{159}
    (R,v)=0,\quad \forall v\in W{\thinspace .}
        
        

If :math:`\{w_0,\ldots,w_N\}` is a basis for :math:`W`, we can equivalently
express the method of weighted residuals as

.. _Eq:fem:deq:1D:WRM:

.. math::

    \tag{160}
    (R,w_i)=0,\quad i\in{\mathcal{I}_s}{\thinspace .}
        
        

The result is :math:`N+1` equations for :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}`.

The least-squares method can also be viewed as a weighted residual
method with :math:`w_i = \partial R/\partial c_i`.

.. index:: variational formulation

.. index:: weak formulation


.. admonition:: Variational formulation of the continuous problem

   Statements like :ref:`(157) <Eq:fem:deq:1D:Galerkin0>`, :ref:`(158) <Eq:fem:deq:1D:Galerkin>`,
   :ref:`(159) <Eq:fem:deq:1D:WRM0>`, or
   :ref:`(160) <Eq:fem:deq:1D:WRM>`)
   are known as
   `weak formulations <https://en.wikipedia.org/wiki/Weak_formulation>`__
   or *variational formulations*.
   These equations are in this text primarily used for a numerical approximation
   :math:`u\in V`, where :math:`V` is a *finite-dimensional* space with dimension
   :math:`N+1`. However, we may also let the exact solution :math:`{u_{\small\mbox{e}}}` fulfill a
   variational formulation :math:`(\mathcal{L}({u_{\small\mbox{e}}}),v)=0` $\forall v\in V$,
   but the exact solution lies in general in a space with infinite
   dimensions (because an infinite number of parameters are needed to
   specify the solution). The variational formulation for :math:`{u_{\small\mbox{e}}}`
   in an infinite-dimensional space :math:`V` is
   a mathematical way of stating the problem and acts as an
   alternative to the usual (strong) formulation of a differential equation with
   initial and/or boundary conditions.
   
   Much of the literature on finite
   element methods takes a differential equation problem and first
   transforms it to a variational formulation in an infinite-dimensional space
   :math:`V`, before searching for an approximate solution in a finite-dimensional
   subspace of :math:`V`. However, we prefer the more intuitive approach with an
   approximate solution :math:`u` in a finite-dimensional space :math:`V` inserted in
   the differential equation, and then the resulting residual is demanded to be
   orthogonal to :math:`V`.





.. admonition:: Remark on terminology

   The terms weak or variational formulations often refer to a statement like
   :ref:`(157) <Eq:fem:deq:1D:Galerkin0>` or :ref:`(159) <Eq:fem:deq:1D:WRM0>`
   after *integration by parts* has been performed (the integration by
   parts technique is
   explained in the section :ref:`fem:deq:1D:varform`).
   The result after
   integration by parts is what is obtained after taking the *first
   variation* of a minimization problem (see
   the section :ref:`fem:deq:1D:optimization`).
   However, in this text we use variational formulation as a common term for
   formulations which, in contrast to the differential equation :math:`R=0`,
   instead demand that an average of :math:`R` is zero: :math:`(R,v)=0` for all :math:`v` in some space.




Test and trial functions
------------------------

.. index:: trial function

.. index:: test function

.. index:: trial space

.. index:: test space

In the context of the Galerkin method and the method of weighted residuals it is
common to use the name *trial function* for the approximate :math:`u =
\sum_j c_j {\psi}_j`.

.. Sometimes the functions that spans the space where :math:`u` lies are also called

.. trial functions.

The space containing the trial function is known as the *trial space*.
The function :math:`v` entering the orthogonality requirement in
the Galerkin method and the method of weighted residuals is called
*test function*, and so are the :math:`{\psi}_i` or :math:`w_i` functions that are
used as weights in the inner products with the residual.  The space
where the test functions comes from is naturally called the
*test space*.

We see that in the method of weighted residuals the test and trial spaces
are different and so are the test and trial functions.
In the Galerkin method the test and trial spaces are the same (so far).

.. Later in the section :ref:`fem:deq:1D:essBC` we shall see that boundary

.. conditions may lead to a difference between the test and trial spaces

.. in the Galerkin method.

The collocation method          (1)
-----------------------------------

The idea of the collocation method is to demand that :math:`R` vanishes
at :math:`N+1` selected points :math:`x_{0},\ldots,x_{N}` in :math:`\Omega`:

.. _Eq:fem:deq:1D:collocation:

.. math::

    \tag{161}
    R(x_{i}; c_0,\ldots,c_N)=0,\quad i\in{\mathcal{I}_s}{\thinspace .}
        
        

The collocation method can also be viewed as a method of weighted residuals
with Dirac delta functions as weighting functions.
Let :math:`\delta (x-x_{i})` be the Dirac delta function centered around
:math:`x=x_{i}` with the properties that :math:`\delta (x-x_{i})=0` for :math:`x\neq x_{i}`
and

.. _Eq:fem:deq:1D:Dirac:

.. math::

    \tag{162}
    \int_{\Omega} f(x)\delta (x-x_{i}) {\, \mathrm{d}x} =
        f(x_{i}),\quad x_{i}\in\Omega{\thinspace .}
        
        

Intuitively, we may think of :math:`\delta (x-x_{i})` as a very peak-shaped
function around :math:`x=x_{i}` with an integral :math:`\int_{-\infty}^\infty \delta(x-x_{i})dx` that evaluates to unity. Mathematically, it can be shown that
:math:`\delta (x-x_{i})` is the limit of a Gaussian function centered at
:math:`x=x_{i}` with a standard deviation that approaches zero.
Using this latter model, we can roughly visualize delta functions as
done in Figure :ref:`fem:deq:1D:fig:Dirac`.
Because of :ref:`(162) <Eq:fem:deq:1D:Dirac>`, we can let :math:`w_i=\delta(x-x_{i})`
be weighting functions in the method of weighted residuals,
and :ref:`(160) <Eq:fem:deq:1D:WRM>` becomes equivalent to
:ref:`(161) <Eq:fem:deq:1D:collocation>`.

.. _fem:deq:1D:fig:Dirac:

.. figure:: delta_func_weight.png
   :width: 400

   *Approximation of delta functions by narrow Gaussian functions*

The subdomain collocation method
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The idea of this approach is to demand the integral of :math:`R` to vanish
over :math:`N+1` subdomains :math:`\Omega_i` of :math:`\Omega`:

.. _Eq:_auto79:

.. math::

    \tag{163}
    \int_{\Omega_i} R\, {\, \mathrm{d}x}=0,\quad i\in{\mathcal{I}_s}{\thinspace .}   
        

This statement can also be expressed as a weighted residual method

.. _Eq:_auto80:

.. math::

    \tag{164}
    \int_{\Omega} Rw_i\, {\, \mathrm{d}x}=0,\quad i\in{\mathcal{I}_s},  
        

where :math:`w_i=1` for :math:`x\in\Omega_i` and :math:`w_i=0` otherwise.

.. _fem:deq:1D:ex:sines:

Examples on using the principles
--------------------------------

Let us now apply global basis functions to illustrate the different
principles for making the residual :math:`R` small.

The model problem
~~~~~~~~~~~~~~~~~

We consider the differential equation problem

.. _Eq:fem:deq:1D:model1b:

.. math::

    \tag{165}
    -u''(x) = f(x),\quad x\in\Omega=[0,L],\quad u(0)=0,\ u(L)=0
        {\thinspace .}
        
        

Basis functions
~~~~~~~~~~~~~~~

Our choice of basis functions :math:`{\psi}_i`
for :math:`V` is

.. _Eq:fem:deq:1D:ex:sines:psi:

.. math::

    \tag{166}
    {\psi}_i(x) = {\sin\left((i+1)\pi\frac{x}{L}\right)},\quad i\in{\mathcal{I}_s}{\thinspace .}
        
        

An important property of these functions is that :math:`{\psi}_i(0)={\psi}_i(L)=0`,
which means that the boundary conditions on :math:`u` are fulfilled:

.. math::
         u(0) = \sum_jc_j{\psi}_j(0) = 0,\quad u(L) = \sum_jc_j{\psi}_j(L) =0
        {\thinspace .} 

Another nice property is that the chosen sine functions
are orthogonal on :math:`\Omega`:

.. _Eq:_auto81:

.. math::

    \tag{167}
    \int\limits_0^L {\sin\left((i+1)\pi\frac{x}{L}\right)}{\sin\left((j+1)\pi\frac{x}{L}\right)}\, {\, \mathrm{d}x} = \left\lbrace
        \begin{array}{ll} \frac{1}{2} L & i=j  \\ 0, & i\neq j
        \end{array}\right.
        
        

provided :math:`i` and :math:`j` are integers.

.. Sympy can do this!

.. k, m, n = symbols('k m n', integer=True)

.. >>> integrate(sin(k*x)*sin(m*x), (x, 0, 2*pi))

.. 0

.. >>>integrate(sin(k*x)*sin(k*x), (x, 0, 2*pi))

.. pi

The residual
~~~~~~~~~~~~

We can readily calculate the following explicit expression for the
residual:

.. math::
        
        R(x;c_0, \ldots, c_N) = u''(x) + f(x),\nonumber
        

.. math::
          
        = \frac{d^2}{dx^2}\left(\sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x)\right)
        + f(x),\nonumber
        

.. _Eq:fem:deq:1D:ex:sines:res:

.. math::

    \tag{168}
    = \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j''(x) + f(x){\thinspace .}
        
        

The least squares method          (5)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The equations :ref:`(156) <Eq:fem:deq:1D:LS:eq1>`
in the least squares method require an expression for
:math:`\partial R/\partial c_i`. We have

.. _Eq:_auto82:

.. math::

    \tag{169}
    \frac{\partial R}{\partial c_i} =
        \frac{\partial}{\partial c_i}
        \left(\sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j''(x) + f(x)\right)
        = \sum_{j\in{\mathcal{I}_s}} \frac{\partial c_j}{\partial c_i}{\psi}_j''(x)
        = {\psi}_i''(x){\thinspace .}   
        

The governing equations for the unknown parameters :math:`\left\{ {c}_j \right\}_{j\in{\mathcal{I}_s}}` are then

.. _Eq:_auto83:

.. math::

    \tag{170}
    (\sum_j c_j {\psi}_j'' + f,{\psi}_i'')=0,\quad i\in{\mathcal{I}_s},
        
        

which can be rearranged as

.. _Eq:_auto84:

.. math::

    \tag{171}
    \sum_{j\in{\mathcal{I}_s}}({\psi}_i'',{\psi}_j'')c_j = -(f,{\psi}_i''),\quad i\in{\mathcal{I}_s}{\thinspace .}
        
        

This is nothing but a linear system

.. math::
        
        \sum_{j\in{\mathcal{I}_s}}A_{i,j}c_j = b_i,\quad i\in{\mathcal{I}_s}{\thinspace .}
        

The entries in the coefficient matrix are given by

.. math::
        \begin{align*}
        A_{i,j} &= ({\psi}_i'',{\psi}_j'')\nonumber\\ 
        & = \pi^4(i+1)^2(j+1)^2L^{-4}\int_0^L {\sin\left((i+1)\pi\frac{x}{L}\right)}{\sin\left((j+1)\pi\frac{x}{L}\right)}\, {\, \mathrm{d}x}
        \end{align*}

The orthogonality of the sine functions simplify the coefficient matrix:

.. _Eq:_auto85:

.. math::

    \tag{172}
    A_{i,j} = \left\lbrace \begin{array}{ll}
        {1\over2}L^{-3}\pi^4(i+1)^4 & i=j  \\ 
        0,                          & i\neq j
        \end{array}\right.
        
        

The right-hand side reads

.. _Eq:_auto86:

.. math::

    \tag{173}
    b_i = -(f,{\psi}_i'') = (i+1)^2\pi^2L^{-2}\int_0^Lf(x){\sin\left((i+1)\pi\frac{x}{L}\right)}\, {\, \mathrm{d}x}
        
        

Since the coefficient matrix is diagonal we can easily solve for

.. _Eq:fem:deq:1D:ex:sines:solution:

.. math::

    \tag{174}
    c_i = \frac{2L}{\pi^2(i+1)^2}\int_0^Lf(x){\sin\left((i+1)\pi\frac{x}{L}\right)}\, {\, \mathrm{d}x}{\thinspace .}
        
        

With the special choice of :math:`f(x)=2`, the coefficients
can be calculated in ``sympy`` by

.. code-block:: python

    import sympy as sym
    
    i, j = sym.symbols('i j', integer=True)
    x, L = sym.symbols('x L')
    f = 2
    a = 2*L/(sym.pi**2*(i+1)**2)
    c_i = a*sym.integrate(f*sym.sin((i+1)*sym.pi*x/L), (x, 0, L))
    c_i = simplify(c_i)
    print c_i

The answer becomes

.. math::
        
        c_i = 4 \frac{L^{2} \left(\left(-1\right)^{i} + 1\right)}{\pi^{3}
        \left(i^{3} + 3 i^{2} + 3 i + 1\right)}
        

Now, :math:`1+(-1)^i=0` for :math:`i` odd, so only the coefficients with even index
are nonzero. Introducing :math:`i=2k` for :math:`k=0,\ldots,N/2` to count the
relevant indices (for :math:`N` odd, :math:`k` goes to :math:`(N-1)/2`), we get the solution

.. _Eq:_auto87:

.. math::

    \tag{175}
    u(x) = \sum_{k=0}^{N/2} \frac{8L^2}{\pi^3(2k+1)^3}{\sin\left((2k+1)\pi\frac{x}{L}\right)}{\thinspace .}   
        

The coefficients decay very fast: :math:`c_2 = c_0/27`, :math:`c_4=c_0/125`.
The solution will therefore be dominated by the first term,

.. math::
         u(x) \approx \frac{8L^2}{\pi^3}\sin\left(\pi\frac{x}{L}\right){\thinspace .}  

The Galerkin method          (2)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Galerkin principle :ref:`(157) <Eq:fem:deq:1D:Galerkin0>`
applied to :ref:`(165) <Eq:fem:deq:1D:model1b>` consists of inserting
our special residual :ref:`(168) <Eq:fem:deq:1D:ex:sines:res>` in
:ref:`(157) <Eq:fem:deq:1D:Galerkin0>`

.. math::
        
        (u''+f,v)=0,\quad \forall v\in V,
        

or

.. _Eq:_auto88:

.. math::

    \tag{176}
    (u'',v) = -(f,v),\quad\forall v\in V{\thinspace .}   
        

This is the variational formulation, based on the Galerkin principle,
of our differential equation.
The :math:`\forall v\in V` requirement is equivalent to
demanding the equation :math:`(u'',v) = -(f,v)` to be fulfilled for all
basis functions :math:`v={\psi}_i`, :math:`i\in{\mathcal{I}_s}`, see
:ref:`(157) <Eq:fem:deq:1D:Galerkin0>` and :ref:`(158) <Eq:fem:deq:1D:Galerkin>`.
We therefore have

.. _Eq:_auto89:

.. math::

    \tag{177}
    (\sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j'', {\psi}_i)=-(f,{\psi}_i),\quad i\in{\mathcal{I}_s}{\thinspace .}   
        

This equation can be rearranged to a form that explicitly shows
that we get a linear system for the unknowns :math:`\left\{ {c}_j \right\}_{j\in{\mathcal{I}_s}}`:

.. _Eq:_auto90:

.. math::

    \tag{178}
    \sum_{j\in{\mathcal{I}_s}} ({\psi}_i,{\psi}_j'')c_j = (f, {\psi}_i),\quad i\in{\mathcal{I}_s}{\thinspace .}   
        

For the particular choice of the basis functions :ref:`(166) <Eq:fem:deq:1D:ex:sines:psi>`
we get in fact the same linear system
as in the least squares method
because :math:`{\psi}''= -(i+1)^2\pi^2L^{-2}{\psi}`.
Consequently, the solution :math:`u(x)` becomes identical to the one produced
by the least squares method.

The collocation method          (2)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the collocation method :ref:`(161) <Eq:fem:deq:1D:collocation>` we need to
decide upon a set of :math:`N+1` collocation points in :math:`\Omega`. A simple
choice is to use uniformly spaced points: :math:`x_{i}=i\Delta x`, where
:math:`\Delta x = L/N` in our case (:math:`N\geq 1`). However, these points
lead to at least two rows in the matrix consisting of zeros
(since :math:`{\psi}_i(x_{0})=0` and :math:`{\psi}_i(x_{N})=0`), thereby making the matrix
singular and non-invertible. This forces us to choose some other
collocation points, e.g., random points or points uniformly distributed
in the interior of :math:`\Omega`.
Demanding the residual to vanish
at these points leads, in our model problem :ref:`(165) <Eq:fem:deq:1D:model1b>`, to
the equations

.. _Eq:_auto91:

.. math::

    \tag{179}
    -\sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j''(x_{i}) = f(x_{i}),\quad i\in{\mathcal{I}_s},
        
        

which is seen to be a linear system with entries

.. math::
         A_{i,j}=-{\psi}_j''(x_{i})=
        (j+1)^2\pi^2L^{-2}\sin\left((j+1)\pi \frac{x_i}{L}\right),

in the coefficient matrix and entries
:math:`b_i=2` for the right-hand side (when :math:`f(x)=2`).

The special case of :math:`N=0`
can sometimes be of interest. A natural choice is then the midpoint
:math:`x_{0}=L/2` of the domain, resulting in
:math:`A_{0,0} = -{\psi}_0''(x_{0}) = \pi^2L^{-2}`, :math:`f(x_0)=2`,
and hence :math:`c_0=2L^2/\pi^2`.

Comparison
~~~~~~~~~~

In the present model problem, with :math:`f(x)=2`, the exact solution is
:math:`u(x)=x(L-x)`, while for :math:`N=0` the Galerkin and least squares method
result in :math:`u(x)=8L^2\pi^{-3}\sin (\pi x/L)` and the
collocation method leads to :math:`u(x)=2L^2\pi^{-2}\sin (\pi x/L)`.
We can quickly use ``sympy`` to verify that the maximum error
occurs at the midpoint :math:`x=L/2` and find what the errors are.
First we set up the error expressions:

.. code-block:: python

    >>> import sympy as sym
    >>> # Computing with Dirichlet conditions: -u''=2 and sines
    >>> x, L = sym.symbols('x L')
    >>> e_Galerkin = x*(L-x) - 8*L**2*sym.pi**(-3)*sym.sin(sym.pi*x/L)
    >>> e_colloc = x*(L-x) - 2*L**2*sym.pi**(-2)*sym.sin(sym.pi*x/L)

If the derivative of the errors vanish at :math:`x=L/2`, the errors reach
their maximum values here (the errors vanish at the boundary points).

.. code-block:: python

    >>> dedx_Galerkin = sym.diff(e_Galerkin, x)
    >>> dedx_Galerkin.subs(x, L/2)
    0
    >>> dedx_colloc = sym.diff(e_colloc, x)
    >>> dedx_colloc.subs(x, L/2)
    0

Finally, we can compute the maximum error at :math:`x=L/2` and evaluate
the expressions numerically with three decimals:

.. code-block:: python

    >>> sym.simplify(e_Galerkin.subs(x, L/2).evalf(n=3))
    -0.00812*L**2
    >>> sym.simplify(e_colloc.subs(x, L/2).evalf(n=3))
    0.0473*L**2

The error in the collocation method is about 6 times larger than
the error in the Galerkin or least squares method.

.. _fem:deq:1D:varform:

Integration by parts          (1)
---------------------------------

.. index:: integration by parts

A problem arises if we want to apply popular finite element functions
to solve our model problem :ref:`(165) <Eq:fem:deq:1D:model1b>`
by the standard least squares, Galerkin, or collocation methods: the piecewise
polynomials :math:`{\psi}_i(x)` have discontinuous derivatives at the
cell boundaries which makes it problematic to compute
the second-order derivative.  This fact actually makes the least squares and
collocation methods less suitable for finite element approximation of
the unknown function. (By rewriting the equation :math:`-u''=f` as a
system of two first-order equations, :math:`u'=v` and :math:`-v'=f`, the
least squares method can be applied. Also, differentiating discontinuous
functions can actually be handled by distribution theory in
mathematics.)  The Galerkin method and the method of
weighted residuals can, however, be applied together with finite
element basis functions if we use *integration by parts*
as a means for transforming a second-order derivative to a first-order
one.

Consider the model problem :ref:`(165) <Eq:fem:deq:1D:model1b>` and its
Galerkin formulation

.. math::
         -(u'',v) = (f,v)\quad\forall v\in V{\thinspace .}  

Using integration by parts in the Galerkin method,
we can "move" a derivative of :math:`u` onto :math:`v`:

.. math::
        
        \int_0^L u''(x)v(x) {\, \mathrm{d}x} = - \int_0^Lu'(x)v'(x){\, \mathrm{d}x}
        + [vu']_0^L\nonumber
        

.. _Eq:fem:deq:1D:intbyparts:

.. math::

    \tag{180}
    = - \int_0^Lu'(x)v'(x) {\, \mathrm{d}x}
        + u'(L)v(L) - u'(0)v(0){\thinspace .}
        
        

Usually, one integrates the problem at the stage where the :math:`u` and :math:`v`
functions enter the formulation.
Alternatively, but less common, we can integrate by parts in the expressions for
the matrix entries:

.. math::
        
        \int_0^L{\psi}_i(x){\psi}_j''(x) {\, \mathrm{d}x} =
        - \int_0^L{\psi}_i'(x){\psi}_j'(x) dx
        + [{\psi}_i{\psi}_j']_0^L\nonumber
        

.. _Eq:fem:deq:1D:intbyparts0:

.. math::

    \tag{181}
    = - \int_0^L{\psi}_i'(x){\psi}_j'(x) {\, \mathrm{d}x}
        + {\psi}_i(L){\psi}_j'(L) - {\psi}_i(0){\psi}_j'(0){\thinspace .}
        
        

Integration by parts serves to reduce the order of the derivatives and
to make the coefficient matrix symmetric since
:math:`({\psi}_i',{\psi}_j') = ({\psi}_j',{\psi}_i')`.
The symmetry property depends
on the type of terms that enter the differential equation.
As will be seen later in the section :ref:`fem:deq:1D:BC:nat`,
integration by parts also provides a method for implementing
boundary conditions involving :math:`u'`.

With the choice :ref:`(166) <Eq:fem:deq:1D:ex:sines:psi>` of basis functions we see
that the "boundary terms"
:math:`{\psi}_i(L){\psi}_j'(L)` and :math:`{\psi}_i(0){\psi}_j'(0)`
vanish since :math:`{\psi}_i(0)={\psi}_i(L)=0`.

.. A boundary term associated with

.. a location at the boundary where we have Dirichlet conditions will always

.. vanish because :math:`{\psi}_i=0` at such locations.

We therefore end up with the following alternative Galerkin formulation:

.. math::
         -(u'',v) = (u', v') = (f,v)\quad \forall v\in V{\thinspace .}

.. index:: weak form

.. index:: strong form

Weak form
~~~~~~~~~

Since the variational formulation after integration by parts make
weaker demands on the differentiability of :math:`u` and the basis
functions :math:`{\psi}_i`,
the resulting integral formulation is referred to as a *weak form* of
the differential equation problem. The original variational formulation
with second-order derivatives, or the differential equation problem
with second-order derivative, is then the *strong form*, with
stronger requirements on the differentiability of the functions.

For differential equations with second-order derivatives, expressed as
variational formulations and solved by finite element methods, we will
always perform integration by parts to arrive at expressions involving
only first-order derivatives.

.. _fem:deq:1D:essBC:Bfunc:

Boundary function          (1)
------------------------------

So far we have assumed zero Dirichlet boundary conditions, typically
:math:`u(0)=u(L)=0`, and we have demanded that :math:`{\psi}_i(0)={\psi}_i(L)=0`
for :math:`i\in{\mathcal{I}_s}`. What about a boundary condition like :math:`u(L)=D\neq0`?
This condition immediately faces a problem:
:math:`u = \sum_j c_j{\varphi}_j(L) = 0` since all :math:`{\varphi}_i(L)=0`.

We remark that we  faced exactly the same problem  in the section :ref:`fem:approx:global:Fourier` where
we considered Fourier series approximations of functions that where non-zero at the boundaries.
We will use the same trick as we did earlier to get around this problem.

A boundary condition of the form :math:`u(L)=D` can be implemented by
demanding that all :math:`{\psi}_i(L)=0`, but adding a
*boundary function* :math:`B(x)` with the right boundary value, :math:`B(L)=D`, to
the expansion for :math:`u`:

.. math::
         u(x) = B(x) + \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x)
        {\thinspace .}
        

This :math:`u` gets the right value at :math:`x=L`:

.. math::
         u(L) = B(L) + \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(L) = B(L) = D{\thinspace .}  

The idea is that for any boundary where :math:`u` is known we demand :math:`{\psi}_i` to
vanish and construct a function :math:`B(x)` to attain the boundary value of :math:`u`.
There are no restrictions on how :math:`B(x)` varies with :math:`x` in the interior of the
domain, so this variation needs to be constructed in some way. Exactly how
we decide the variation to be, is not important.

For example, with :math:`u(0)=0` and
:math:`u(L)=D`, we can choose :math:`B(x)=x D/L`, since this form ensures that
:math:`B(x)` fulfills the boundary conditions: :math:`B(0)=0` and :math:`B(L)=D`.
The unknown function is then sought on the form

.. _Eq:fem:deq:1D:essBC:Bfunc:u1:

.. math::

    \tag{182}
    u(x) = \frac{x}{L}D + \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x),
        
        

with :math:`{\psi}_i(0)={\psi}_i(L)=0`.

The particular shape of the :math:`B(x)` function is not important
as long as its boundary
values are correct. For example, :math:`B(x)=D(x/L)^p` for any power :math:`p`
will work fine in the above example. Another choice could be
:math:`B(x)=D\sin (\pi x/(2L))`.

As a more general example, consider a domain :math:`\Omega = [a,b]`
where the boundary conditions are :math:`u(a)=U_a` and :math:`u(b)=U_b`.  A class
of possible :math:`B(x)` functions is

.. _Eq:fem:deq:1D:essBC:Bfunc:gen:

.. math::

    \tag{183}
    B(x)=U_a + \frac{U_b-U_a}{(b-a)^p}(x-a)^p,\quad p>0
        {\thinspace .}
        
        

Real applications will most likely use the simplest version, :math:`p=1`,
but here such a :math:`p` parameter was included to demonstrate that there
are many choices of :math:`B(x)` in a problem. Fortunately, there is a general, unique
technique for constructing :math:`B(x)` when we use finite element basis functions for
:math:`V`.

[**kam 7**: in the below, I cannot really find where it is stated that we need to adjust the right-hand side as well]
[**hpl 8**: I don't understand what you mean.]


.. admonition:: How to deal with nonzero Dirichlet conditions

   The general procedure of incorporating Dirichlet boundary
   conditions goes as follows.
   Let :math:`\partial\Omega_E` be the part(s) of the boundary
   :math:`\partial\Omega` of the domain :math:`\Omega` where :math:`u` is specified.
   Set :math:`{\psi}_i=0` at the points in :math:`\partial\Omega_E` and seek :math:`u`
   as
   
   .. _Eq:fem:deq:1D:essBC:Bfunc:u2:

.. math::

    \tag{184}
    u(x) = B(x) + \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x),
           
           
   
   where :math:`B(x)` equals the boundary conditions on :math:`u` at :math:`\partial\Omega_E`.




**Remark.**
With the :math:`B(x)` term, :math:`u` does not in general lie in :math:`V=\hbox{span}\,
\{{\psi}_0,\ldots,{\psi}_N\}` anymore. Moreover, when a prescribed value
of :math:`u` at the boundary, say :math:`u(a)=U_a` is different from zero, it does
not make sense to say that :math:`u` lies in a vector space, because
this space does not obey the requirements of addition and scalar multiplication.
For example,
:math:`2u` does not lie in the space since its boundary value is :math:`2U_a`,
which is incorrect. It only makes sense to split :math:`u` in two parts,
as done above, and have the unknown part :math:`\sum_j c_j {\psi}_j` in a
proper function space.

.. Sometimes it is said that :math:`u` is in the *affine space* :math:`B+V`.

Computing with global polynomials
=================================

The next example uses global polynomials and shows
that if our solution, modulo boundary conditions, lies in the space spanned
by these polynomials, then the Galerkin method recovers the exact solution.

.. _fem:deq:1D:varform:ex:DN:case:

Computing with Dirichlet and Neumann conditions
-----------------------------------------------

.. ex_varform1D.py: case2

Let us perform the necessary calculations to solve

.. math::
        
        -u''(x)=2,\quad x\in \Omega=[0,1],\quad u'(0)=C,\ u(1)=D,
        

using a global polynomial basis :math:`{\psi}_i\sim x^i`.
The requirements on :math:`{\psi}_i` is that :math:`{\psi}_i(1)=0`, because :math:`u` is
specified at :math:`x=1`, so a proper set of polynomial basis functions can be

.. math::
         {\psi}_i(x)=(1-x)^{i+1}, \quad i\in{\mathcal{I}_s}{\thinspace .}

A suitable :math:`B(x)` function
to handle the boundary condition :math:`u(1)=D` is :math:`B(x)=Dx`.
The variational formulation becomes

.. math::
         (u',v') = (2,v) - Cv(0)\quad\forall v\in V{\thinspace .} 

From inserting :math:`u=B + \sum_{j}c_j{\psi}_j` and choosing :math:`v={\psi}_i` we get

.. math::
         \sum_{j\in{\mathcal{I}_s}} ({\psi}_j',{\psi}_i')c_j = (2,{\psi}_i)
        - (B',{\psi}_i') - C{\psi}_i(0),\quad i\in{\mathcal{I}_s}{\thinspace .}

The entries in the linear system are then

.. math::
        \begin{align*}
        A_{i,j} &= ({\psi}_j',{\psi}_i') = \int_{0}^1 {\psi}_i'(x){\psi}_j'(x){\, \mathrm{d}x}
        = \int_0^1 (i+1)(j+1)(1-x)^{i+j}{\, \mathrm{d}x}\\ 
        &= \frac{(i+1)(j+1)}{i + j + 1},\\ 
        b_i &= (2,{\psi}_i) - (D,{\psi}_i') -C{\psi}_i(0)\\ 
        &= \int_0^1\left( 2{\psi}_i(x) - D{\psi}_i'(x)\right){\, \mathrm{d}x} -C{\psi}_i(0)\\ 
        &= \int_0^1 \left( 2(1-x)^{i+1} + D(i+1)(1-x)^i\right){\, \mathrm{d}x}  -C\\ 
        &= \frac{(D-C)(i+2) + 2}{i+2} = D - C + \frac{2}{i+2}
        {\thinspace .}
        \end{align*}

Relevant ``sympy`` commands to help calculate these expressions are

.. code-block:: python

    from sympy import *
    x, C, D = symbols('x C D')
    i, j = symbols('i j', integer=True, positive=True)
    psi_i = (1-x)**(i+1)
    psi_j = psi_i.subs(i, j)
    integrand = diff(psi_i, x)*diff(psi_j, x)
    integrand = simplify(integrand)
    A_ij = integrate(integrand, (x, 0, 1))
    A_ij = simplify(A_ij)
    print 'A_ij:', A_ij
    f = 2
    b_i = integrate(f*psi_i, (x, 0, 1)) - \ 
          integrate(diff(D*x, x)*diff(psi_i, x), (x, 0, 1)) - \ 
          C*psi_i.subs(x, 0)
    b_i = simplify(b_i)
    print 'b_i:', b_i

The output becomes

.. code-block:: text

    A_ij: (i + 1)*(j + 1)/(i + j + 1)
    b_i: ((-C + D)*(i + 2) + 2)/(i + 2)

We can now choose some :math:`N` and form the linear system, say for :math:`N=1`:

.. code-block:: python

    N = 1
    A = zeros((N+1, N+1))
    b = zeros(N+1)
    print 'fresh b:', b
    for r in range(N+1):
        for s in range(N+1):
            A[r,s] = A_ij.subs(i, r).subs(j, s)
        b[r,0] = b_i.subs(i, r)

The system becomes

.. math::
        
        \left(\begin{array}{cc}
        1 & 1\\ 
        1 & 4/3
        \end{array}\right)
        \left(\begin{array}{c}
        c_0\\ 
        c_1
        \end{array}\right)
        =
        \left(\begin{array}{c}
        1-C+D\\ 
        2/3 -C + D
        \end{array}\right)
        

The solution (``c = A.LUsolve(b)``)
becomes :math:`c_0=2 -C+D` and :math:`c_1=-1`, resulting in

.. _Eq:_auto92:

.. math::

    \tag{185}
    u(x) = 1 -x^2 + D + C(x-1),
        
        

We can form this :math:`u` in ``sympy`` and check that the differential equation
and the boundary conditions are satisfied:

.. code-block:: python

    u = sum(c[r,0]*psi_i.subs(i, r) for r in range(N+1)) + D*x
    print 'u:', simplify(u)
    print "u'':", simplify(diff(u, x, x))
    print 'BC x=0:', simplify(diff(u, x).subs(x, 0))
    print 'BC x=1:', simplify(u.subs(x, 1))

The output becomes

.. code-block:: text

    u: C*x - C + D - x**2 + 1
    u'': -2
    BC x=0: C
    BC x=1: D

The complete ``sympy`` code is found in `u_xx_2_CD.py <http://tinyurl.com/znpudbt/u_xx_2_CD.py>`__.

The exact solution is found by integrating twice and applying the
boundary conditions, either by hand or using ``sympy`` as shown in
the section :ref:`fem:deq:1D:models:simple`.  It appears that the numerical
solution coincides with the exact one.  This result is to be expected
because if :math:`({u_{\small\mbox{e}}} - B)\in V`, :math:`u = {u_{\small\mbox{e}}}`, as proved next.

When the numerical method is exact
----------------------------------

We have some variational formulation: find :math:`(u-B)\in V` such that
:math:`a(u,v)=L(u)\ \forall v\in V`. The exact solution also fulfills
:math:`a({u_{\small\mbox{e}}},v)=L(v)`, but normally :math:`({u_{\small\mbox{e}}} -B)` lies in a much larger
(infinite-dimensional) space. Suppose, nevertheless, that
:math:`{u_{\small\mbox{e}}} - B = E`, where :math:`E\in V`. That is, apart from Dirichlet conditions,
:math:`{u_{\small\mbox{e}}}` lies in our finite-dimensional space :math:`V` which we use to compute :math:`u`.
Writing also :math:`u` on the same form :math:`u=B+F`, :math:`F\in V`, we have

.. math::
        \begin{align*}
        a(B+E,v) &= L(v)\quad\forall v\in V,\\ 
        a(B+F,v) &= L(v)\quad\forall v\in V{\thinspace .}
        \end{align*}

Since these are two variational statements in the same space, we
can subtract them and use the bilinear property of :math:`a(\cdot,\cdot)`:

.. math::
        \begin{align*}
        a(B+E,v) - a(B+F, v) &= L(v) - L(v)\\ 
        a(B+E-(B+F),v) &= 0\\ 
        a(E-F),v) &= 0
        \end{align*}

If :math:`a(E-F),v) = 0` for all :math:`v` in :math:`V`, then :math:`E-F` must be zero everywhere
in the domain, i.e., :math:`E=F`. Or in other words: :math:`u={u_{\small\mbox{e}}}`. This proves
that the exact solution is recovered if :math:`{u_{\small\mbox{e}}} - B` lies in :math:`V`., i.e.,
can be expressed as :math:`\sum_{j\in{\mathcal{I}_s}}d_j{\psi}_j` where :math:`\{{\psi}_j\}_{j\in{\mathcal{I}_s}}`
is a basis for :math:`V`. The method will then compute the solution :math:`c_j=d_j`,
:math:`j\in{\mathcal{I}_s}`.

The case treated in the section :ref:`fem:deq:1D:varform:ex:DN:case`
is of the type where :math:`{u_{\small\mbox{e}}} - B` is a quadratic function that is 0
at :math:`x=1`, and therefore :math:`({u_{\small\mbox{e}}} -B)\in V`, and the method
finds the exact solution.

