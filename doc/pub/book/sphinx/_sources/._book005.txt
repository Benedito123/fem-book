.. !split

Orthogonal basis functions          (1)
=======================================

Approximation of a function via orthogonal functions, especially sinusoidal
functions or orthogonal polynomials, is a very popular and successful approach.
The finite element method does not make use of orthogonal functions, but
functions that are "almost orthogonal".

.. _fem:approx:global:illconditioning:

Ill-conditioning
----------------

For basis functions that are not orthogonal the condition number
of the matrix may create problems during the solution process
due to for example round-off errors as will be illustrated in the
following.   The computational example in the section :ref:`fem:approx:global:exact1`
applies the ``least_squares`` function which invokes symbolic
methods to calculate and solve the linear system. The correct
solution :math:`c_0=9, c_1=-20, c_2=10, c_i=0` for :math:`i\geq 3` is perfectly
recovered.

Suppose we
convert the matrix and right-hand side to floating-point arrays
and then solve the system using finite-precision arithmetics, which
is what one will (almost) always do in real life. This time we
get astonishing results! Up to about :math:`N=7` we get a solution that
is reasonably close to the exact one. Increasing :math:`N` shows that
seriously wrong coefficients are computed.
Below is a table showing the solution of the linear system arising from
approximating a parabola
by functions on the form :math:`u(x)=c_0 + c_1x + c_2x^2 + \cdots + c_{10}x^{10}`.
Analytically, we know that :math:`c_j=0` for :math:`j>2`, but numerically we may get
:math:`c_j\neq 0` for :math:`j>2`.

=====  =========  ===========  ===========  
exact  ``sympy``  ``numpy32``  ``numpy64``  
=====  =========  ===========  ===========  
    9       9.62         5.57         8.98  
  -20     -23.39        -7.65       -19.93  
   10      17.74        -4.50         9.96  
    0      -9.19         4.13        -0.26  
    0       5.25         2.99         0.72  
    0       0.18        -1.21        -0.93  
    0      -2.48        -0.41         0.73  
    0       1.81       -0.013        -0.36  
    0      -0.66         0.08         0.11  
    0       0.12         0.04        -0.02  
    0     -0.001        -0.02        0.002  
=====  =========  ===========  ===========  

The exact value of :math:`c_j`, :math:`j=0,1,\ldots,10`, appears in the first
column while the other columns correspond to results obtained
by three different methods:

  * Column 2: The matrix and vector are converted to
    the data structure  ``sympy.mpmath.fp.matrix`` and the
    ``sympy.mpmath.fp.lu_solve`` function is used to solve the system.

  * Column 3: The matrix and vector are converted to
    ``numpy`` arrays with data type ``numpy.float32``
    (single precision floating-point number) and solved by
    the ``numpy.linalg.solve`` function.

  * Column 4: As column 3, but the data type is
    ``numpy.float64`` (double
    precision floating-point number).

We see from the numbers in the table that
double precision performs much better than single precision.
Nevertheless, when plotting all these solutions the curves cannot be
visually distinguished (!). This means that the approximations look
perfect, despite the partially very wrong values of the coefficients.

Increasing :math:`N` to 12 makes the numerical solver in ``numpy``
abort with the message: "matrix is numerically singular".
A matrix has to be non-singular to be invertible, which is a requirement
when solving a linear system. Already when the matrix is close to
singular, it is *ill-conditioned*, which here implies that
the numerical solution algorithms are sensitive to round-off
errors and may produce (very) inaccurate results.

The reason why the coefficient matrix is nearly singular and
ill-conditioned is that our basis functions :math:`{\psi}_i(x)=x^i` are
nearly linearly dependent for large :math:`i`.  That is, :math:`x^i` and :math:`x^{i+1}`
are very close for :math:`i` not very small. This phenomenon is
illustrated in Figure :ref:`fem:approx:global:fig:illconditioning`.
There are 15 lines in this figure, but only half of them are
visually distinguishable.
Almost linearly dependent basis functions give rise to an
ill-conditioned and almost singular matrix.  This fact can be
illustrated by computing the determinant, which is indeed very close
to zero (recall that a zero determinant implies a singular and
non-invertible matrix): :math:`10^{-65}` for :math:`N=10` and :math:`10^{-92}` for
:math:`N=12`. Already for :math:`N=28` the numerical determinant computation
returns a plain zero.

.. _fem:approx:global:fig:illconditioning:

.. figure:: ill_conditioning.png
   :width: 600

   The 15 first basis functions :math:`x^i`, :math:`i=0,\ldots,14`

On the other hand, the double precision ``numpy`` solver does run for
:math:`N=100`, resulting in answers that are not significantly worse than
those in the table above, and large powers are
associated with small coefficients (e.g., :math:`c_j < 10^{-2}` for :math:`10\leq
j\leq 20` and :math:`c<10^{-5}` for :math:`j > 20`). Even for :math:`N=100` the
approximation still lies on top of the exact curve in a plot (!).

The conclusion is that visual inspection of the quality of the approximation
may not uncover fundamental numerical problems with the computations.
However, numerical analysts have studied approximations and ill-conditioning
for decades, and it is well known that the basis :math:`\{1,x,x^2,x^3,\ldots,\}`
is a bad basis. The best basis from a matrix conditioning point of view
is to have orthogonal functions such that :math:`(\psi_i,\psi_j)=0` for
:math:`i\neq j`. There are many known sets of orthogonal polynomials and
other functions.
The functions used in the finite element methods are almost orthogonal,
and this property helps to avoid problems with solving matrix systems.
Almost orthogonal is helpful, but not enough when it comes to
partial differential equations, and ill-conditioning
of the coefficient matrix is a theme when solving large-scale matrix
systems arising from finite element discretizations.

.. _fem:approx:global:Fourier:

Fourier series
--------------

.. index::
   single: approximation; by sines

A set of sine functions is widely used for approximating functions
(note that the sines are orthogonal with respect to the :math:`L_2` inner product as can be easily
verified using ``sympy``).  Let us take

.. math::
        
        V = \hbox{span}\,\{ \sin \pi x, \sin 2\pi x,\ldots,\sin (N+1)\pi x\}
        {\thinspace .}  

That is,

.. math::
         {\psi}_i(x) = \sin ((i+1)\pi x),\quad i\in{\mathcal{I}_s}{\thinspace .} 

An approximation to the parabola :math:`f(x)=10(x-1)^2-1` for :math:`x\in\Omega=[1,2]` from
the section :ref:`fem:approx:global:linear` can then be computed by the
``least_squares`` function from the section :ref:`fem:approx:global:LS:code`:

.. code-block:: python

    N = 3
    import sympy as sym
    x = sym.Symbol('x')
    psi = [sym.sin(sym.pi*(i+1)*x) for i in range(N+1)]
    f = 10*(x-1)**2 - 1
    Omega = [0, 1]
    u, c = least_squares(f, psi, Omega)
    comparison_plot(f, u, Omega)

Figure :ref:`fem:approx:global:fig:parabola:sine1` (left) shows the oscillatory approximation
of :math:`\sum_{j=0}^Nc_j\sin ((j+1)\pi x)` when :math:`N=3`.
Changing :math:`N` to 11 improves the approximation considerably, see
Figure :ref:`fem:approx:global:fig:parabola:sine1` (right).

.. _fem:approx:global:fig:parabola:sine1:

.. figure:: parabola_ls_sines4_12.png
   :width: 800

   *Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions*

There is an error :math:`f(0)-u(0)=9` at :math:`x=0` in Figure :ref:`fem:approx:global:fig:parabola:sine1` regardless of how large :math:`N` is, because all :math:`{\psi}_i(0)=0` and hence
:math:`u(0)=0`. We may help the approximation to be correct at :math:`x=0` by
seeking

.. _Eq:_auto21:

.. math::

    \tag{55}
    u(x) = f(0) + \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x)
        {\thinspace .}
        
        

However, this adjustment introduces a new problem at :math:`x=1` since
we now get an error :math:`f(1)-u(1)=f(1)-0=-1` at this point. A more
clever adjustment is to replace the :math:`f(0)` term by a term that
is :math:`f(0)` at :math:`x=0` and :math:`f(1)` at :math:`x=1`. A simple linear combination
:math:`f(0)(1-x) + xf(1)` does the job:

.. _Eq:_auto22:

.. math::

    \tag{56}
    u(x) = f(0)(1-x) + xf(1) + \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x)
        {\thinspace .}
        
        

This adjustment of :math:`u` alters the linear system slightly. In the general
case, we set

.. math::
         u(x) = B(x) +  \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j(x),

and the linear system becomes

.. math::
         \sum_{j\in{\mathcal{I}_s}}({\psi}_i,{\psi}_j)c_j = (f-B,{\psi}_i),\quad i\in{\mathcal{I}_s}{\thinspace .}

The calculations can still utilize the ``least_squares`` or
``least_squares_orth`` functions, but solve for :math:`u-b`:

.. code-block:: python

    f0 = 0;  f1 = -1
    B = f0*(1-x) + x*f1
    u_sum, c = least_squares_orth(f-b, psi, Omega)
    u = B + u_sum

Figure :ref:`fem:approx:global:fig:parabola:sine2` shows the result
of the technique for
ensuring right boundary values. Even 3 sines can now adjust the
:math:`f(0)(1-x) + xf(1)` term such that :math:`u` approximates the parabola really
well, at least visually.

.. _fem:approx:global:fig:parabola:sine2:

.. figure:: parabola_ls_sines4_12_wfterm.png
   :width: 800

   *Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions with a boundary term*

.. _fem:approx:global:orth:

Orthogonal basis functions          (2)
---------------------------------------

The choice of sine functions :math:`{\psi}_i(x)=\sin ((i+1)\pi x)` has a great
computational advantage: on :math:`\Omega=[0,1]` these basis functions are
*orthogonal*, implying that :math:`A_{i,j}=0` if :math:`i\neq j`. This
result is realized by trying

.. code-block:: python

    integrate(sin(j*pi*x)*sin(k*pi*x), x, 0, 1)

in `WolframAlpha <http://wolframalpha.com>`__
(avoid ``i`` in the integrand as this symbol means
the imaginary unit :math:`\sqrt{-1}`).
Asking WolframAlpha also
about :math:`\int_0^1\sin^2 (j\pi x) {\, \mathrm{d}x}`, we find that it equals
1/2.
With a diagonal matrix we can easily solve for the coefficients
by hand:

.. _Eq:_auto23:

.. math::

    \tag{57}
    c_i = 2\int_0^1 f(x)\sin ((i+1)\pi x) {\, \mathrm{d}x},\quad i\in{\mathcal{I}_s},
        
        

which is nothing but the classical formula for the coefficients of
the Fourier sine series of :math:`f(x)` on :math:`[0,1]`. In fact, when
:math:`V` contains the basic functions used in a Fourier series expansion,
the approximation method derived in the section :ref:`fem:approx:global`
results in the classical Fourier series for :math:`f(x)` (see :ref:`fem:approx:exer:Fourier`
for details).

With orthogonal basis functions we can make the
``least_squares`` function (much) more efficient since we know that
the matrix is diagonal and only the diagonal elements need to be computed:

.. code-block:: python

    def least_squares_orth(f, psi, Omega):
        N = len(psi) - 1
        A = [0]*(N+1)
        b = [0]*(N+1)
        x = sym.Symbol('x')
        for i in range(N+1):
            A[i] = sym.integrate(psi[i]**2, (x, Omega[0], Omega[1]))
            b[i] = sym.integrate(psi[i]*f,  (x, Omega[0], Omega[1]))
        c = [b[i]/A[i] for i in range(len(b))]
        u = 0
        for i in range(len(psi)):
            u += c[i]*psi[i]
        return u, c

As mentioned in the section :ref:`fem:approx:global:LS:code`, symbolic integration
may fail or take a very long time. It is therefore natural to extend the
implementation above with a version where we can choose between symbolic
and numerical integration and fall back on the latter if the former
fails:

.. code-block:: python

    def least_squares_orth(f, psi, Omega, symbolic=True):
        N = len(psi) - 1
        A = [0]*(N+1)       # plain list to hold symbolic expressions
        b = [0]*(N+1)
        x = sym.Symbol('x')
        for i in range(N+1):
            # Diagonal matrix term
            A[i] = sym.integrate(psi[i]**2, (x, Omega[0], Omega[1]))
    
            # Right-hand side term
            integrand = psi[i]*f
            if symbolic:
                I = sym.integrate(integrand,  (x, Omega[0], Omega[1]))
            if not symbolic or isinstance(I, sym.Integral):
                print 'numerical integration of', integrand
                integrand = sym.lambdify([x], integrand)
                I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
            b[i] = I
        c = [b[i]/A[i] for i in range(len(b))]
        u = 0
        u = sum(c[i,0]*psi[i] for i in range(len(psi)))
        return u, c

This function is found in the file ``approx1D.py``. Observe that
we here assume that
:math:`\int_\Omega{\varphi}_i^2{\, \mathrm{d}x}` can always be symbolically computed,
which is not an unreasonable assumption
when the basis functions are orthogonal, but there is no guarantee,
so an improved version of the function above would implement
numerical integration also for the ``A[i,i]`` term.

Numerical computations
----------------------

Sometimes the basis functions :math:`{\psi}_i` and/or the function :math:`f`
have a nature that makes symbolic integration CPU-time
consuming or impossible.
Even though we implemented a fallback on numerical integration
of :math:`\int f{\varphi}_i {\, \mathrm{d}x}`, considerable time might still be required
by ``sympy`` just by *attempting* to integrate symbolically.
Therefore, it will be handy to have function for fast
*numerical integration and numerical solution
of the linear system*. Below is such a method. It requires
Python functions ``f(x)`` and ``psi(x,i)`` for :math:`f(x)` and :math:`{\psi}_i(x)`
as input. The output is a mesh function
with values ``u`` on the mesh with points in the array ``x``.
Three numerical integration methods are offered:
``scipy.integrate.quad`` (precision set to :math:`10^{-8}`),
``sympy.mpmath.quad`` (about machine precision), and a Trapezoidal
rule based on the points in ``x`` (unknown accuracy, but
increasing with the number of mesh points in ``x``).

.. code-block:: python

    def least_squares_numerical(f, psi, N, x,
                                integration_method='scipy',
                                orthogonal_basis=False):
        import scipy.integrate
        A = np.zeros((N+1, N+1))
        b = np.zeros(N+1)
        Omega = [x[0], x[-1]]
        dx = x[1] - x[0]
    
        for i in range(N+1):
            j_limit = i+1 if orthogonal_basis else N+1
            for j in range(i, j_limit):
                print '(%d,%d)' % (i, j)
                if integration_method == 'scipy':
                    A_ij = scipy.integrate.quad(
                        lambda x: psi(x,i)*psi(x,j),
                        Omega[0], Omega[1], epsabs=1E-9, epsrel=1E-9)[0]
                elif integration_method == 'sympy':
                    A_ij = sym.mpmath.quad(
                        lambda x: psi(x,i)*psi(x,j),
                        [Omega[0], Omega[1]])
                else:
                    values = psi(x,i)*psi(x,j)
                    A_ij = trapezoidal(values, dx)
                A[i,j] = A[j,i] = A_ij
    
            if integration_method == 'scipy':
                b_i = scipy.integrate.quad(
                    lambda x: f(x)*psi(x,i), Omega[0], Omega[1],
                    epsabs=1E-9, epsrel=1E-9)[0]
            elif integration_method == 'sympy':
                b_i = sym.mpmath.quad(
                    lambda x: f(x)*psi(x,i), [Omega[0], Omega[1]])
            else:
                values = f(x)*psi(x,i)
                b_i = trapezoidal(values, dx)
            b[i] = b_i
    
        c = b/np.diag(A) if orthogonal_basis else np.linalg.solve(A, b)
        u = sum(c[i]*psi(x, i) for i in range(N+1))
        return u, c
    
    def trapezoidal(values, dx):
        """Integrate values by the Trapezoidal rule (mesh size dx)."""
        return dx*(np.sum(values) - 0.5*values[0] - 0.5*values[-1])

Here is an example on calling the function:

.. code-block:: python

    from numpy import linspace, tanh, pi
    
    def psi(x, i):
        return sin((i+1)*x)
    
    x = linspace(0, 2*pi, 501)
    N = 20
    u, c = least_squares_numerical(lambda x: tanh(x-pi), psi, N, x,
                                   orthogonal_basis=True)

**Remark.**
The ``scipy.integrate.quad`` integrator is usually much faster than
``sympy.mpmath.quad``.

