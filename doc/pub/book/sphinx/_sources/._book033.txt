.. !split

.. _ch:pc:

Uncertainty quantification and polynomial chaos expansions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

.. index:: polynomial chaos

Differential equations often contain unknown parameters, but
mathematically these must be prescribed to run the solution method.
If some parameters are uncertain, how uncertain is then the answer?
The simplest *conceptual* approach to this question is to let each
parameter vary in an interval and use *interval arithmetics* to
compute the interval of solution at the vertices of the
mesh. Unfortunately, this approach is terribly inaccurate and not
feasible. Also, any deterministic approach to the question hides fact
that uncertainty is strongly connected to statistical
variations. Therefore, a more sound approach is to prescribe some
statistics of the input parameters and then compute some statistics of
the output. This is called *stochastic uncertainty quantification*, or
just the name uncertainty quantification, abbreviated UQ, implies a
stochastic description of the input of the model. There has been a
heavy development of the UQ field during the last couple of decades.

One of the most exciting and promising methods of UQ for differential
equation is *polynomial chaos expansions*. The literature on this method
might seem complicated unless one has firm background in probability
theory. However, it turns out that all the seemingly different variations
of this method are all strongly connected if one knows the approximation
framework of the chapter :ref:`ch:approx:global` of this book and some very basic
probability theory. It is therefore natural to present polynomial chaos
expansions from perspective and make the ideas more accessible to a larger
audience. This is exactly the purpose of the present chapter.

There are some excellent expositions of polynomial chaos in the recent
literature by Dongbin Xiu [Ref12]_ [Ref13]_. However, for the reader
with an understanding of the chapter :ref:`ch:approx:global` in this book, the authors
think the *ideas* can be conveyed much faster using those approximation
concepts rather than the more traditional exposition in the literature.

**Remark on a major probabilistic simplification.**

We assume that we have some function :math:`u(\boldsymbol{x},t;\zeta)`, where :math:`\zeta` is
a vector of *random parameters* :math:`(\zeta_0,\ldots,\zeta_N)` that
introduces uncertainty in the problem. Note that the uncertainty is
described by :math:`M+1` stochastic scalar parameters and is therefore
*discretized* from a statistical point of view.  If we have temporal
or spatial stochastic processes or stochastic space-time fields, these
are continuous, and the mathematical formulations become considerably
more complicated. Assuming the uncertainty to be described by a set of
discrete stochastic *variables* represents the same simplification as
we introduced in earlier chapters where we assumed the solution to be
discrete, :math:`u(\boldsymbol{x})\approx\sum_ic_i{\psi}_i(\boldsymbol{x})`, and worked with
variational forms in finite-dimensional spaces. We could, as briefly
shown in the chapter :ref:`ch:approx:global`, equally well worked with
variational forms in *infinite-dimensional* spaces, but the theory
would involve Sobolev spaces rather than the far more intuitive vector
spaces. In the same manner, stochastic processes and fields and their
mathematical theory are avoided when assuming the uncertainty to be
already discretized in terms of random variables.

Sample problems
===============

We shall look at two driving examples to illustrate the methods.

ODE for decay processes
-----------------------

A simple ODE, governing decay processes [Ref14]_
in space or time, reads

.. _Eq:_auto187:

.. math::

    \tag{461}
    u'(t) = a(t)u(t),\quad u(0)=I,
        
        

where :math:`a` and :math:`I` can be uncertain input parameter, making :math:`u` an
uncertain output parameter for each :math:`t`.

If :math:`a` is uncertain, we assume that it
consists of uncertain piecewise constants: :math:`a=a_i` is a random variable
in some time interval :math:`[t_i,t_{i+1}]`. Typically, we can write
:math:`a(t) = \sum_{i=0}^M a_i\hat{\psi}_i(\boldsymbol{x})`, with :math:`\hat{\psi}_i` being P0
elements in time, and :math:`a_i` are the unknown coefficients (in this case we
count to :math:`i` from 0 to :math:`M` and consider :math:`I` as certain).


.. admonition:: Stochastic ODE

   To be specific, we let :math:`a` be a scalar uncertain constant and let also
   :math:`I` be uncertain. Then
   we have two stochastic variables, :math:`M=2`: :math:`\zeta_0=a` and :math:`\zeta_1=I`.
   
   The analytical solution to this ODE is :math:`u(t)=Ie^{-at}`, regardless of
   which variables that are random or not. With only :math:`I` as
   random variable, we see that the solution is simply a linear function
   of :math:`I`, so if :math:`I` has a Gaussian probability distribution, :math:`u` is for
   any :math:`t`, the well-known transformation of expectation and variance
   a normally distributed random variable:
   
   .. math::
           \begin{align*}
           {\hbox{E}\lbrack u \rbrack} &= {\hbox{E}\lbrack I \rbrack}e^{-at},\\ 
           {\hbox{Var}\lbrack u \rbrack} &= {\hbox{Var}\lbrack I \rbrack}e^{2at}{\thinspace .}
           \end{align*}
   
   However, as soon as :math:`a` is stochastic, the problem is, from a stochastic
   view point *nonlinear*, so there is no simple transformation of the
   probability distribution for :math:`a` nor its statistics.
   
   The basic statistics of expectance and variance for a random variable :math:`x`
   varying continuously on the interval :math:`\Omega_X` is as
   
   .. math::
           \begin{align*}
           {\hbox{E}\lbrack u(x) \rbrack} &= \int_\Omega u(x)p(x){\, \mathrm{d}x},\\ 
           {\hbox{Var}\lbrack u \rbrack} &= \int_\Omega (u(x)-{\hbox{E}\lbrack u \rbrack})^2 p(x){\, \mathrm{d}x}{\thinspace .}
           \end{align*}




The stochastic Poisson equation
-------------------------------

We also address the stationary PDE

.. _Eq:_auto188:

.. math::

    \tag{462}
    -\nabla\cdot({\alpha}(\boldsymbol{x})\nabla u(\boldsymbol{x})) = f(\boldsymbol{x})\mbox{in }\Omega,
        
        

with appropriate Dirichlet, Neumann, or Robin conditions at the
boundary, The coefficients :math:`{\alpha}` and :math:`f` are assumed to be stochastic
quantities discretized in terms of expansions, e.g., :math:`{\alpha} =
\sum_{i=0}^M{\alpha}_i\hat{\psi}_i(\boldsymbol{x})`, where :math:`\hat{\psi}_i(\boldsymbol{x})` is
some spatial discretization - think of, for instance, of piecewise
constant P0 finite elements or global sine functions, and the
parameters :math:`{\alpha}_i` as the random uncertain variables in the
problem. We form some final vector :math:`(\zeta_0,\ldots,\zeta_M)` with all
the random parameters, from :math:`{\alpha}(\boldsymbol{x})` only, from :math:`f(\boldsymbol{x})` only, or as
a combination of randomness in :math:`{\alpha}` and :math:`f`. Here, one can obviously
include randomness in boundary conditions as well.


.. admonition:: 1D model problem

   A specific 1D problem for heat conduction,
   
   .. math::
            \frac{\partial}{\partial x}\left({\alpha}(x)\frac{\partial u}{\partial x}
           \right) = 0,\quad x\in\Omega = [0,L],
   
   may have two materials with :math:`{\alpha}` as an uncertain constant in each
   material. Let :math:`\Omega = \Omega_0 = [0,\hat L] \cup\Omega_1 = [\hat L,
   0]`, where :math:`\Omega_i` is material number :math:`i`. Let :math:`{\alpha}_i` be the
   corresponding uncertain heat conduction coefficient value.  Now we
   have :math:`M=2`, and :math:`\zeta_0={\alpha}_0` and :math:`\zeta_1={\alpha}_1`.




Basic principles
================

The *fundamental idea* in polynomial chaos theory is that we now
assume the mapping from an input parameter
:math:`\boldsymbol{\zeta}=(\zeta_0,\ldots,\zeta_M) \in W` to :math:`u`, to be *smooth*,
because we known that smooth mappings can efficiently be approximated
by *the sum*

.. _Eq:pc:fem:approx:ufem2:

.. math::

    \tag{463}
    u(\boldsymbol{x};\boldsymbol{\zeta}) = \sum_{i=0}^{N} c_i{\psi}_i(\boldsymbol{\zeta})
        =\ \sum_{j\in{\mathcal{I}_s}} c_j{\psi}_j \in V {\thinspace .}
        
        

This is the same formula as we used for approximation in Chapter
:ref:`(463) <Eq:pc:fem:approx:ufem2>`.
Note that :math:`\hbox{dim} W = M+1`, while :math:`\hbox{dim} V = N+1`.

Another basic principle in polynomial chaos theory is to choose the
basis functions in :math:`V`, :math:`{\psi}_i(\boldsymbol{x})`, as *orthogonal
polynomials*. The reason is that this choice has the potential that
the sum :ref:`(463) <Eq:pc:fem:approx:ufem2>` may converge very fast so :math:`N` can
be kept small.  [**kam 46**: I don't get this. I arrested Jonathan also on this earlier and he did not provide an answer. I think orthogonality leads to easy or direct computations of moments such as E and Var.]


.. admonition:: Polynomial chaos expansions may involve many unknowns

   
   We have :math:`M+1` uncertain parameters so the approximation problem
   takes place in an :math:`N+1` dimensional space, contrary to previous chapters
   where :math:`N` would be physical space, :math:`N=1,2,3`. For each variable, we
   will need a set of basis functions to represent the variation in that
   variable. Suppose we use polynomials of order :math:`Q` and lower in each
   variable :math:`\zeta_i`, :math:`i=0,\ldots,M`. The total number of terms in the
   expansion is then
   
   .. _Eq:_auto189:

.. math::

    \tag{464}
    N + 1 = \frac{(Q + M)!}{Q!M!}{\thinspace .}
           
           
   
   Suppose we have 10 uncertain input variables (:math:`M+1=10`) and want to
   approximate each variable by a cubic polynomial (:math:`Q=3`). This results
   in :math:`N+1=220` terms in the expansion. And a cubic variation is modest,
   so going to degree 10, demands 92,378 terms. Consequently, for the
   polynomial chaos method to be effective, we need very fast convergence
   and that lower-order polynomial variation is sufficient. First of all
   this demands that the mapping from :math:`\boldsymbol{\zeta}` to :math:`u` is smooth, but
   the parameter is very often smooth even though the solution :math:`u(\boldsymbol{x})` in
   physical space is non-smooth.





.. admonition:: Remark on terminology

   
   The development of the theory in this chapter builds on the seminal
   work on polynomial chaos theory by Roger Ghanem and co-workers. This
   theory expands functions in Hermite polynomials, which was a great
   success for a range of linear Gaussian engineering safety problems.
   To obtain faster convergence in non-Gaussian problems, functions were
   expanded in other families of orthogonal polynomials, which is the
   view we take here, but that method is known as *generalized*
   polynomial chaos, often abbreviated gPC.  In this text, however, we
   use the shorter term polynomial chaos for all expansions of stochastic
   parameter dependence in terms of global orthogonal polynomials.




Basic statistical results
-------------------------

Our task is to compute the coefficients :math:`c_i`. When these are known, we
can easily compute the statistics of the response :math:`u` of the differential
equation, or any other quantity derived from :math:`u`. For example, the
expectation of :math:`u` is

.. _Eq:_auto190:

.. math::

    \tag{465}
    {\hbox{E}\lbrack u \rbrack} = {\hbox{E}\lbrack \sum_{j\in{\mathcal{I}_s} \rbrack} c_j{\psi}_j} = \sum_{j\in{\mathcal{I}_s}} {\hbox{E}\lbrack c_j \rbrack}{\psi}_j
        
        

A third pillar among assumption for the polynomial chaos method is that
the *random input variables are stochastically independent*. There are
methods to come around this problem, but these are very recent and we
keep the traditional assumption in the present of the theory here.

There are four methods that have been used to compute :math:`c_i`:

1. Least squares minimization or Galerkin projection with exact
   computation of integrals

2. Least-squares minimization or Galerkin projection with numerical
   computation of integrals

3. Interpolation or collocation

4. Regression

Method 2 and 4 are the dominating ones. These go under the names
pseudo-spectral methods and stochastic collocation methods, respectively.

Least-squares methods
---------------------

A major difference for deterministic approximation methods and
polynomial chaos based on least squares is that we introduce
probabilistic information in the inner product in the latter case.
This means that *we must know the joint probability distribution*
:math:`p(\boldsymbol{\zeta})` for the input parameters! Then we define the
inner product between two elements :math:`u` and :math:`w` in :math:`V` as

.. _Eq:_auto191:

.. math::

    \tag{466}
    (u,v) = {\hbox{E}\lbrack uv \rbrack} = \int_\Omega u(\boldsymbol{\zeta})v(\boldsymbol{\zeta}) p(\boldsymbol{\zeta})d\boldsymbol{\zeta}
        
        

with the associated norm

.. math::
         || u ||^2 = (u,u){\thinspace .}

The domain :math:`\Omega` is the hypercube of input parameters:

.. math::
         \Omega = [\zeta_{0,\min},\zeta_{0,\max}]\times
        [\zeta_{1,\min},\zeta_{1,\max}]\times\cdots\times
        [\zeta_{N,\min},\zeta_{N,\max}]{\thinspace .}

Since the random input variables are independent, we can factorize
the joint probability distribution as

.. math::
         p(\boldsymbol{\zeta}) = \Pi_{i=0}^N p_i(\zeta_i){\thinspace .}

We then have

.. math::
        
        {\hbox{E}\lbrack \boldsymbol{\zeta \rbrack}} = {\hbox{E}\lbrack \zeta_0 \rbrack}{\hbox{E}\lbrack \zeta_1 \rbrack}\cdots{\hbox{E}\lbrack \zeta_0 \rbrack}
        = \Pi_{i=0}^N
        \int\limits_{\zeta_{i,\,min}}^{\zeta_{i,max}}\zeta_i p_i(\zeta_0)d\zeta_i{\thinspace .}
        

Requiring that the polynomials are orthogonal means that

.. math::
         (u,v) = {\delta_{ij}}||u||,

where :math:`\delta_{ij}` is the Kronecker delta: :math:`\delta_{ij}=1` if and only if
:math:`i=j`, otherwise :math:`\delta_{ij}=0`.

Let :math:`f` be the solution computed by our mathematical model, either
approximately by a numerical method or exact.
When we do the least squares minimization, we know from the chapter :ref:`ch:approx:global` that the associated linear system is diagonal
and its solution is

.. _Eq:pc:ci:ls:

.. math::

    \tag{467}
    c_i = \frac{({\psi}_i, f)}{||{\psi}||^2} =
                 \frac{{\hbox{E}\lbrack f{\psi}_i \rbrack}}{{\hbox{E}\lbrack {\psi}^2 \rbrack}}{\thinspace .}
        
        

In case of orthonormal polynomials :math:`{\psi}_i` we simply have
:math:`c_i = {\hbox{E}\lbrack f{\psi}_i \rbrack}`.

Galerkin's method or project gives

.. math::
         (f-\sum_i c_i {\psi}_i, v) = 0,\quad\forall v\in V,

which is equivalent to

.. math::
         (f-\sum_i c_i {\psi}_i, {\psi}_j) = 0,\quad j=0,\ldots,N,

and consequently the formula :ref:`(467) <Eq:pc:ci:ls>` for :math:`c_i`.

The major question, from a computational point of view, is how to
evaluate the integrals in the formula for :math:`c_i`. Let us look at our
two primary examples.

Example: Least squares applied to the decay ODE
-----------------------------------------------

In this model problem, we are interested in the statistics of the
solution :math:`u(t)`. The response is :math:`u(t)=Ie^{-at} =
\zeta_0e^{-\zeta_1 t}` as our :math:`f(\zeta)` function.
Consequently,

.. math::
         u(\boldsymbol{x}) = \sum_{i\in{\mathcal{I}_s}} c_i(t){\psi}_i(\zeta_0,\zeta_1){\thinspace .}

Furthermore,

.. math::
         c_i(t) =
        \frac{(\zeta_0e^{-\zeta_1 t},{\psi}_i)}{||{\psi}_i||^2}
        = ||{\psi}_i||^{-2}\int\limits_{\zeta_{0,\min}}^{\zeta_{0,\max}}
        \int\limits_{\zeta_{1,\min}}^{\zeta_{1,\max}}
        \zeta_0e^{-\zeta_1 t}{\psi}_i(\zeta_0,\zeta_1) p_0(\zeta_0)p_1(\zeta_1)
        d\zeta_0 d\zeta_1{\thinspace .}
        

This is a quite heavy integral to perform by hand; it really depends on
the complexity of the probability density function :math:`p_0` for :math:`I` and
:math:`p_1` for :math:`a`. A tool like SymPy or Mathematica is indispensable,
but even these will soon give up when :math:`i` grows.

Uniform distributions!

Modeling the response
---------------------

Our aim is to approximate the mapping from input to output in a
simulation program. The output will often require solving a
differential equation. Let :math:`f` be the output response as a result of
solving the differential equation. We then want to find the mapping

.. math::
         u(\boldsymbol{x};\boldsymbol{\zeta}) = \sum_{i\in{\mathcal{I}_s}} c_i(\boldsymbol{x}){\psi}_i(\boldsymbol{\zeta}),

or if the response is some operation (functional) on the solution :math:`u`,
say :math:`f(u)`, we seek the mapping

.. math::
         f(\boldsymbol{\zeta}) = \sum_{i\in{\mathcal{I}_s}} c_i(\boldsymbol{x}){\psi}_i(\boldsymbol{\zeta}),

Normally, :math:`c_i` will depend on the spatial coordinate :math:`\boldsymbol{x}` as here,
but in some cases it

Give examples with flux etc!

Numerical integration          (3)
----------------------------------

Clenshaw-Curtis quadrature,
Gauss-Legendre quadrature,
Gauss-Patterson quadrature,
Genz-Keister quadrature,
Leja quadrature,
Monte Carlo integration,
Optimal Gaussian quadrature,

Stochastic collocation
----------------------

The stochastic collocation method is nothing but what we called
regression in the chapter :ref:`ch:approx:global`.  We simply choose a large
number :math:`P` of evaluation points in the parameter space :math:`W` and compute
the response at these points.  Then we fit an orthogonal polynomial
expansion to these points using the principle of least squares. Here,
in this educational text, the principle of least squares suffices,
but one should that for real-life computations a lot of difficulties
may happen, especially in large-scale problems. We then need to
stabilize the least squares computations in various ways, and for this
purpose, one needs tailored software like Chaospy to be briefly
introduced in the section :ref:`pc:chaospy`. Chaospy offers, for example, a
wide range of regression schemes.

Also, the choice of collocation points should make use of point families
that suit this kind of problems. Again, Chaospy offers many collections
of point families, and not surprisingly, numerical integration points
constitute popular families.

.. _pc:chaospy:

The Chaospy software
====================

.. index:: Chaospy software

Refer to our ref for comparison with Dakota and Turns.

.. _pc:intrusive:

Intrusive polynomial chaos methods
==================================

.. index:: intrusive polynomial chaos

.. index:: non-intrusive polynomial chaos

So far we have described *non-intrusive* polynomial chaos theory, which means
that the stochastic solution method can use the underlying differential
equation solver "as is" without any modifications. This is a great
advantage, and it also makes the theory easier to understand.
However, one may construct *intrusive* methods that applies the mapping
()
in the *solution process* of the differential equation model.

Let us address the decay ODE, :math:`u'=-au`, where we have the mapping
:math:`u=\sum_i c_i{\psi}_i` for :math:`u` which can be inserted in the ODE,
giving rise to a residual :math:`R`:

.. math::
         R = \sum_i c_i'(t){\psi}_i(\boldsymbol{\zeta}) -
        \sum_i c_i{\psi}_i(\boldsymbol{\zeta}){\thinspace .}

Galerkin's method is a common choice for incorporating the polynomial
chaos expansion when solving differential equations, so we require
as usual :math:`(R,v)=0` for all :math:`v\in V`, or expressed via basis functions:

.. math::
         \sum_j ({\psi}_i,{\psi}_j) c_i'(t) = -a ({\psi}_i,{\psi}_i)\,\quad
        i=0,\ldots,N tp

This is now a differential equation system in :math:`c_i`, which can be solved
by any ODE solver. However, the size of this system is :math:`(N+1)\times (N+1)`
rather than just a scalar ODE in the original model and the non-intrusive
method. This is characteristic for non-intrusive methods: the size of
the underlying system grows exponentially with the number of uncertain
parameters, and one always ends up in high-dimensional spaces for the ODEs
or the coordinates for PDEs.

For the Poisson equation we insert the mapping

.. math::
         u(\boldsymbol{x}; \boldsymbol{\zeta}) = \sum_{i\in{\mathcal{I}_s}} c_i(\boldsymbol{x}){\psi}_i(\boldsymbol{\zeta}),

and get a residual

.. _Eq:_auto192:

.. math::

    \tag{468}
    R = -\sum_{j\in{\mathcal{I}_s}}
        \nabla\cdot({\alpha}(\boldsymbol{x})\nabla c_j(\boldsymbol{x})){\psi}_j(\boldsymbol{\zeta}) +
        f(\boldsymbol{x}){\thinspace .}
        
        

Now we can apply Galerkin's method to get equations for :math:`c_i(\boldsymbol{x})`:

.. math::
         \sum_{i\in{\mathcal{I}_s}} ({\psi}_j(\boldsymbol{\zeta}), {\psi}_i(\boldsymbol{\zeta}))
        \nabla\cdot({\alpha}(\boldsymbol{x})\nabla c_j(\boldsymbol{x})) = (f(\boldsymbol{x}),{\psi}_i(\boldsymbol{\zeta})),\quad
        i=0,\ldots,N{\thinspace .}

Here, we think that we solve the Poisson equation by some finite difference
or element method on some mesh, so for each node :math:`\boldsymbol{x}` in the mesh, we get
the above system for :math:`c_j`.
[**hpl 47**: Integration by parts?]

