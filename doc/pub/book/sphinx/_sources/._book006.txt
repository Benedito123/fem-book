.. !split

Interpolation
=============

.. _fem:approx:global:interp:

The interpolation (or collocation) principle
--------------------------------------------

.. index:: collocation method (approximation)

.. index::
   single: approximation; collocation

.. index:: interpolation method (approximation)

.. index::
   single: approximation; interpolation

The principle of minimizing the distance between :math:`u` and :math:`f` is
an intuitive way of computing a best approximation :math:`u\in V` to :math:`f`.
However, there are other approaches as well.
One is to demand that :math:`u(x_{i}) = f(x_{i})` at some selected points
:math:`x_{i}`, :math:`i\in{\mathcal{I}_s}`:

.. _Eq:_auto24:

.. math::

    \tag{58}
    u(x_{i}) = \sum_{j\in{\mathcal{I}_s}} c_j {\psi}_j(x_{i}) = f(x_{i}),
        \quad i\in{\mathcal{I}_s}{\thinspace .}  
        

We recognize that the equation :math:`\sum_j c_j {\psi}_j(x_{i}) = f(x_{i})`
is actually a linear system with :math:`N+1` unknown coefficients :math:`\left\{ {c}_j \right\}_{j\in{\mathcal{I}_s}}`:

.. _Eq:_auto25:

.. math::

    \tag{59}
    \sum_{j\in{\mathcal{I}_s}} A_{i,j}c_j = b_i,\quad i\in{\mathcal{I}_s},
        
        

with coefficient matrix and right-hand side vector given by

.. _Eq:_auto26:

.. math::

    \tag{60}
    A_{i,j} = {\psi}_j(x_{i}),
        
        

.. _Eq:_auto27:

.. math::

    \tag{61}
    b_i = f(x_{i}){\thinspace .}   
        

This time the coefficient matrix is not symmetric because
:math:`{\psi}_j(x_{i})\neq {\psi}_i(x_{j})` in general.
The method is often referred to as an *interpolation method*
since some point values of :math:`f` are given (:math:`f(x_{i})`) and we
fit a continuous function :math:`u` that goes through the :math:`f(x_{i})` points.
In this case the :math:`x_{i}` points are called *interpolation points*.
When the same approach is used to approximate differential equations,
one usually applies the name *collocation method* and
:math:`x_{i}` are known as *collocation points*.

Given :math:`f`  as a ``sympy`` symbolic expression ``f``, :math:`\left\{ {{\psi}}_i \right\}_{i\in{\mathcal{I}_s}}`
as a list ``psi``, and a set of points :math:`\left\{ {x}_i \right\}_{i\in{\mathcal{I}_s}}`  as a list or array
``points``, the following Python function sets up and solves the matrix system
for the coefficients :math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}`:

.. code-block:: python

    def interpolation(f, psi, points):
        N = len(psi) - 1
        A = sym.zeros((N+1, N+1))
        b = sym.zeros((N+1, 1))
        psi_sym = psi  # save symbolic expression
        # Turn psi and f into Python functions
        x = sym.Symbol('x')
        psi = [sym.lambdify([x], psi[i]) for i in range(N+1)]
        f = sym.lambdify([x], f)
        for i in range(N+1):
            for j in range(N+1):
                A[i,j] = psi[j](points[i])
            b[i,0] = f(points[i])
        c = A.LUsolve(b)
        # c is a sympy Matrix object, turn to list
        c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]
        u = sym.simplify(sum(c[i,0]*psi_sym[i] for i in range(N+1)))
        return u, c

The ``interpolation`` function is a part of the ``approx1D``
module.

We found it convenient in the above function to turn the expressions ``f`` and
``psi`` into ordinary Python functions of ``x``, which can be called with
``float`` values in the list ``points`` when building the matrix and
the right-hand side. The alternative is to use the ``subs`` method
to substitute the ``x`` variable in an expression by an element from
the ``points`` list. The following session illustrates both approaches
in a simple setting:

.. code-block:: ipy

    >>> import sympy as sym
    >>> x = sym.Symbol('x')
    >>> e = x**2              # symbolic expression involving x
    >>> p = 0.5               # a value of x
    >>> v = e.subs(x, p)      # evaluate e for x=p
    >>> v
    0.250000000000000
    >>> type(v)
    sympy.core.numbers.Float
    >>> e = lambdify([x], e)  # make Python function of e
    >>> type(e)
    >>> function
    >>> v = e(p)              # evaluate e(x) for x=p
    >>> v
    0.25
    >>> type(v)
    float

A nice feature of the interpolation or collocation method is that it
avoids computing integrals. However, one has to decide on the location
of the :math:`x_{i}` points.  A simple, yet common choice, is to
distribute them uniformly throughout the unit interval.

Example          (2)
~~~~~~~~~~~~~~~~~~~~

Let us illustrate the interpolation method by approximating
our parabola :math:`f(x)=10(x-1)^2-1` by a linear function on :math:`\Omega=[1,2]`,
using two collocation points :math:`x_0=1+1/3` and :math:`x_1=1+2/3`:

.. code-block:: python

    import sympy as sym
    x = sym.Symbol('x')
    f = 10*(x-1)**2 - 1
    psi = [1, x]
    Omega = [1, 2]
    points = [1 + sym.Rational(1,3), 1 + sym.Rational(2,3)]
    u, c = interpolation(f, psi, points)
    comparison_plot(f, u, Omega)

The resulting linear system becomes

.. math::
        
        \left(\begin{array}{ll}
        1 & 4/3\\ 
        1 & 5/3\\ 
        \end{array}\right)
        \left(\begin{array}{l}
        c_0\\ 
        c_1\\ 
        \end{array}\right)
        =
        \left(\begin{array}{l}
        1/9\\ 
        31/9\\ 
        \end{array}\right)
        

with solution :math:`c_0=-119/9` and :math:`c_1=10`.
Figure :ref:`fem:approx:global:linear:interp:fig1` (left) shows the resulting
approximation :math:`u=-119/9 + 10x`.
We can easily test other interpolation points, say :math:`x_0=1` and :math:`x_1=2`.
This changes the line quite significantly, see
Figure :ref:`fem:approx:global:linear:interp:fig1` (right).

.. _fem:approx:global:linear:interp:fig1:

.. figure:: parabola_inter.png
   :width: 800

   *Approximation of a parabola by linear functions computed by two interpolation points: 4/3 and 5/3 (left) versus 1 and 2 (right)*

.. _fem:approx:global:Lagrange:

Lagrange polynomials
--------------------

.. index:: Lagrange (interpolating) polynomial

In the section :ref:`fem:approx:global:Fourier` we explained the advantage
of having a diagonal matrix: formulas for the coefficients
:math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}` can then be derived by hand. For an interpolation (or
collocation) method a diagonal matrix implies that :math:`{\psi}_j(x_{i})
= 0` if :math:`i\neq j`. One set of basis functions :math:`{\psi}_i(x)` with this
property is the *Lagrange interpolating polynomials*, or just
*Lagrange polynomials*. (Although the functions are named after
Lagrange, they were first discovered by Waring in 1779, rediscovered
by Euler in 1783, and published by Lagrange in 1795.)  Lagrange
polynomials are key building blocks in the finite element method, so
familiarity with these polynomials will be required anyway.

A Lagrange polynomial can be written as

.. _Eq:fem:approx:global:Lagrange:poly:

.. math::

    \tag{62}
    {\psi}_i(x) =
        \prod_{j=0,j\neq i}^N
        \frac{x-x_{j}}{x_{i}-x_{j}}
        = \frac{x-x_0}{x_{i}-x_0}\cdots\frac{x-x_{i-1}}{x_{i}-x_{i-1}}\frac{x-x_{i+1}}{x_{i}-x_{i+1}}
        \cdots\frac{x-x_N}{x_{i}-x_N},
        
        

for :math:`i\in{\mathcal{I}_s}`.
We see from :ref:`(62) <Eq:fem:approx:global:Lagrange:poly>` that all the :math:`{\psi}_i`
functions are polynomials of degree :math:`N` which have the property

.. index:: Kronecker delta

.. _Eq:fem:inter:prop:

.. math::

    \tag{63}
    {\psi}_i(x_s) = \delta_{is},\quad \delta_{is} =
        \left\lbrace\begin{array}{ll}
        1, & i=s,\\ 
        0, & i\neq s,
        \end{array}\right.
        
        

when :math:`x_s` is an interpolation (collocation) point.
Here we have used the *Kronecker delta* symbol :math:`\delta_{is}`.
This property implies that :math:`A` is a diagonal matrix, that is
that  :math:`A_{i,j}=0` for :math:`i\neq j` and
:math:`A_{i,j}=1` when :math:`i=j`. The solution of the linear system is
then simply

.. _Eq:_auto28:

.. math::

    \tag{64}
    c_i = f(x_{i}),\quad i\in{\mathcal{I}_s},
        
        

and

.. _Eq:_auto29:

.. math::

    \tag{65}
    u(x) = \sum_{j\in{\mathcal{I}_s}} c_i {\psi}_i(x) = \sum_{j\in{\mathcal{I}_s}} f(x_{i}){\psi}_i(x){\thinspace .}   
        

We remark however that :ref:`(63) <Eq:fem:inter:prop>` does not necessary imply that the matrix
obtained by the least squares or project methods are diagonal.

The following function computes the Lagrange interpolating polynomial
:math:`{\psi}_i(x)` on the unit interval (0,1), given the interpolation points :math:`x_{0},\ldots,x_{N}` in
the list or array ``points``:

.. code-block:: python

    def Lagrange_polynomial(x, i, points):
        p = 1
        for k in range(len(points)):
            if k != i:
                p *= (x - points[k])/(points[i] - points[k])
        return p

The next function computes a complete basis, :math:`{\psi}_0,\ldots,{\psi}_N`, using equidistant points throughout
:math:`\Omega`:

.. code-block:: python

    def Lagrange_polynomials_01(x, N):
        if isinstance(x, sym.Symbol):
            h = sym.Rational(1, N-1)
        else:
            h = 1.0/(N-1)
        points = [i*h for i in range(N)]
        psi = [Lagrange_polynomial(x, i, points) for i in range(N)]
        return psi, points

When ``x`` is an ``sym.Symbol`` object, we let the spacing between the
interpolation points, ``h``, be a ``sympy`` rational number, so that we
get nice end results in the formulas for :math:`{\psi}_i`.  The other case,
when ``x`` is a plain Python ``float``, signifies numerical computing, and
then we let ``h`` be a floating-point number.  Observe that the
``Lagrange_polynomial`` function works equally well in the symbolic and
numerical case - just think of ``x`` being an ``sym.Symbol`` object or a
Python ``float``.  A little interactive session illustrates the
difference between symbolic and numerical computing of the basis
functions and points:

.. code-block:: ipy

    >>> import sympy as sym
    >>> x = sym.Symbol('x')
    >>> psi, points = Lagrange_polynomials_01(x, N=2)
    >>> points
    [0, 1/2, 1]
    >>> psi
    [(1 - x)*(1 - 2*x), 2*x*(2 - 2*x), -x*(1 - 2*x)]
    
    >>> x = 0.5  # numerical computing
    >>> psi, points = Lagrange_polynomials_01(x, N=2)
    >>> points
    [0.0, 0.5, 1.0]
    >>> psi
    [-0.0, 1.0, 0.0]

That is, when used symbolically, the ``Lagrange_polynomials_01``
function returns the symbolic expression for the Lagrange functions
while when ``x`` is a numerical valued the function returns the value of
the basis function evaluate in ``x``.  In the present example only the
second basis function should be 1 in the mid-point while the others
are zero according to :ref:`(63) <Eq:fem:inter:prop>`.

Approximation of a polynomial
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Galerkin or least squares methods lead to an exact approximation
if :math:`f` lies in the space spanned by the basis functions. It could be
of interest to see how the interpolation method with Lagrange
polynomials as basis is able to approximate a polynomial, e.g., a
parabola. Running

.. code-block:: python

    for N in 2, 4, 5, 6, 8, 10, 12:
        f = x**2
        psi, points = Lagrange_polynomials_01(x, N)
        u = interpolation(f, psi, points)

shows the result that up to ``N=4`` we achieve an exact approximation,
and then round-off errors start to grow, such that
``N=15`` leads to a 15-degree polynomial for :math:`u` where
the coefficients in front of :math:`x^r` for :math:`r>2` are
of size :math:`10^{-5}` and smaller. As the matrix is ill-conditioned
and we use floating-point arithmetic, we do not obtain the exact
solution. Still, we get  a solution that is visually identical to the
exact solution. The reason is that the ill-conditioning causes
the system to have many solutions very close to the true solution.
While we are lucky for ``N=15`` and obtain a solution that is
visually identical to the true solution, ill-conditioning may also
result in completely wrong results. As we continue with higher values,  ``N=20`` reveals that the
procedure is starting to fall apart as the approximate solution is around 0.9 at :math:`x=1.0`,
where it should have
been :math:`1.0`. At ``N=30`` the approximate solution is around :math:`5\cdot10^8` at :math:`x=1`.

[**kam 2**: Not sure of this. Need to test. Seems like an unstable variant of Lagrange which is somewhat strange when combined with interpolation which should produce the identity matrix]

Successful example
~~~~~~~~~~~~~~~~~~

Trying out the Lagrange polynomial basis for approximating
:math:`f(x)=\sin 2\pi x` on :math:`\Omega =[0,1]` with the least squares
and the interpolation techniques can be done by

.. code-block:: python

    x = sym.Symbol('x')
    f = sym.sin(2*sym.pi*x)
    psi, points = Lagrange_polynomials_01(x, N)
    Omega=[0, 1]
    u, c = least_squares(f, psi, Omega)
    comparison_plot(f, u, Omega)
    u, c = interpolation(f, psi, points)
    comparison_plot(f, u, Omega)

Figure :ref:`fem:approx:global:Lagrange:fig:sine:ls:colloc` shows the results.
There is a difference between the least squares and the interpolation
technique but the difference decreases rapidly with  Increasing :math:`N`.

.. _fem:approx:global:Lagrange:fig:sine:ls:colloc:

.. figure:: Lagrange_ls_interp_sin_4.png
   :width: 800

   *Approximation via least squares (left) and interpolation (right) of a sine function by Lagrange interpolating polynomials of degree 3*

.. index:: Runge's phenomenon

Less successful example
~~~~~~~~~~~~~~~~~~~~~~~

The next example concerns interpolating :math:`f(x)=|1-2x|` on :math:`\Omega
=[0,1]` using Lagrange polynomials. Figure
:ref:`fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14` shows a peculiar
effect: the approximation starts to oscillate more and more as :math:`N`
grows. This numerical artifact is not surprising when looking at the
individual Lagrange polynomials. Figure
:ref:`fem:approx:global:Lagrange:fig:abs:Lag:unif:osc` shows two such
polynomials, :math:`\psi_2(x)` and :math:`\psi_7(x)`, both of degree 11 and
computed from uniformly spaced points :math:`x_{i}=i/11`,
:math:`i=0,\ldots,11`, marked with circles.  We clearly see the property of
Lagrange polynomials: :math:`\psi_2(x_{i})=0` and :math:`\psi_7(x_{i})=0` for
all :math:`i`, except :math:`\psi_2(x_{2})=1` and :math:`\psi_7(x_{7})=1`.  The most
striking feature, however, is the dominating oscillation near the
boundary where :math:`\psi_2>5` and :math:`\psi_7=-10` in some points. The reason is easy to understand: since we force the
functions to be zero at so many points, a polynomial of high degree is
forced to oscillate between the points.  The phenomenon is named
*Runge's phenomenon* and you can read a more detailed explanation on
`Wikipedia <http://en.wikipedia.org/wiki/Runge%27s_phenomenon>`__.

.. index:: Chebyshev nodes

Remedy for strong oscillations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The oscillations can be reduced by a more clever choice of
interpolation points, called the *Chebyshev nodes*:

.. _Eq:_auto30:

.. math::

    \tag{66}
    x_{i} = \frac{1}{2} (a+b) + \frac{1}{2}(b-a)\cos\left( \frac{2i+1}{2(N+1)}pi\right),\quad i=0\ldots,N,
        
        

on the interval :math:`\Omega = [a,b]`.
Here is a flexible version of the ``Lagrange_polynomials_01`` function above,
valid for any interval :math:`\Omega =[a,b]` and with the possibility to generate
both uniformly distributed points and Chebyshev nodes:

.. code-block:: python

    def Lagrange_polynomials(x, N, Omega, point_distribution='uniform'):
        if point_distribution == 'uniform':
            if isinstance(x, sym.Symbol):
                h = sym.Rational(Omega[1] - Omega[0], N)
            else:
                h = (Omega[1] - Omega[0])/float(N)
            points = [Omega[0] + i*h for i in range(N+1)]
        elif point_distribution == 'Chebyshev':
            points = Chebyshev_nodes(Omega[0], Omega[1], N)
        psi = [Lagrange_polynomial(x, i, points) for i in range(N+1)]
        return psi, points
    
    def Chebyshev_nodes(a, b, N):
        from math import cos, pi
        return [0.5*(a+b) + 0.5*(b-a)*cos(float(2*i+1)/(2*N+1))*pi) \ 
                for i in range(N+1)]

All the functions computing Lagrange polynomials listed
above are found in the module file ``Lagrange.py``.

Figure :ref:`fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14` shows the
improvement of using Chebyshev nodes, compared with the equidistant
points in Figure
:ref:`fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14`.  The reason for
this improvement is that the corresponding Lagrange polynomials have
much smaller oscillations, which can be seen by comparing Figure
:ref:`fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc` (Chebyshev
points) with Figure
:ref:`fem:approx:global:Lagrange:fig:abs:Lag:unif:osc` (equidistant
points). Note the different scale on the vertical axes in the two
figures and also that the Chebyshev points tend to cluster
more around the element boundaries.

Another cure for undesired oscillations of higher-degree interpolating
polynomials is to use lower-degree Lagrange polynomials on many small
patches of the domain. This is actually the idea pursued in the finite
element method. For instance, linear Lagrange polynomials on :math:`[0,1/2]`
and :math:`[1/2,1]` would yield a perfect approximation to :math:`f(x)=|1-2x|` on
:math:`\Omega = [0,1]` since :math:`f` is piecewise linear.

.. _fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14:

.. figure:: Lagrange_interp_abs_8_15.png
   :width: 800

   *Interpolation of an absolute value function by Lagrange polynomials and uniformly distributed interpolation points: degree 7 (left) and 14 (right)*

.. _fem:approx:global:Lagrange:fig:abs:Lag:unif:osc:

.. figure:: Lagrange_basis_12.png
   :width: 400

   *Illustration of the oscillatory behavior of two Lagrange polynomials based on 12 uniformly spaced points (marked by circles)*

.. _fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14:

.. figure:: Lagrange_interp_abs_Cheb_8_15.png
   :width: 800

   *Interpolation of an absolute value function by Lagrange polynomials and Chebyshev nodes as interpolation points: degree 7 (left) and 14 (right)*

.. _fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc:

.. figure:: Lagrange_basis_Cheb_12.png
   :width: 400

   *Illustration of the less oscillatory behavior of two Lagrange polynomials based on 12 Chebyshev points (marked by circles)*

How does the least squares or projection methods work with Lagrange
polynomials?
We can just call the ``least_squares`` function, but
``sympy`` has problems integrating the :math:`f(x)=|1-2x|`
function times a polynomial, so we need to fall back on numerical
integration.

.. code-block:: python

    def least_squares(f, psi, Omega):
        N = len(psi) - 1
        A = sym.zeros((N+1, N+1))
        b = sym.zeros((N+1, 1))
        x = sym.Symbol('x')
        for i in range(N+1):
            for j in range(i, N+1):
                integrand = psi[i]*psi[j]
                I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
                if isinstance(I, sym.Integral):
                    # Could not integrate symbolically, fall back
                    # on numerical integration with mpmath.quad
                    integrand = sym.lambdify([x], integrand)
                    I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
                A[i,j] = A[j,i] = I
            integrand = psi[i]*f
            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
            if isinstance(I, sym.Integral):
                integrand = sym.lambdify([x], integrand)
                I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
            b[i,0] = I
        c = A.LUsolve(b)
        c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]
        u = sum(c[i]*psi[i] for i in range(len(psi)))
        return u, c

.. Convergence of Lagrange polynomials.

.. _fem:approx:global:Lagrange:fig:abs:Lag:unif:ls:

.. figure:: Lagrange_ls_abs_12.png
   :width: 400

   *Illustration of an approximation of the absolute value function using the least square method*

.. _fem:approx:global:Bernstein:

Bernstein polynomials
---------------------

.. index:: Bernstein(interpolating) polynomial

An alternative to the Taylor and Lagrange families of polynomials
are the Bernstein polynomials.
These polynomials are popular in visualization and we include a
presentation of them for completeness. Furthermore, as we
will demonstrate, the choice of basis functions may matter
in terms of accuracy and efficiency.
In fact, in finite element methods,
a main challenge, from a numerical analysis point of view,
is to determine appropriate basis functions for
a particular purpose or equation.

On the unit interval, the Bernstein
polynomials are defined in terms of powers of :math:`x` and :math:`1-x`
(the barycentric coordinates of the unit interval) as

.. _Eq:bernstein:basis:

.. math::

    \tag{67}
    B_{i,n} = \binom{n}{i} x^i (1-x)^{n-i}, \quad i=0, \ldots, n .
        
        

.. _Bernstein_basis_8:

.. figure:: Bernstein_basis8.png

   *The nine functions of a Bernstein basis of order 8*

.. _Lagrange_basis_8:

.. figure:: Lagrange_basis8.png

   *The nine functions of a Lagrange basis of order 8*

The Figure :ref:`Bernstein_basis_8` shows the  basis functions of a Bernstein basis of order 8.
This figure should be compared against Figure :ref:`Lagrange_basis_8`, which
shows the corresponding Lagrange basis of order 8.
The Lagrange basis is convenient because it is a nodal basis, that is; the basis functions are 1 in their nodal points and zero at all other nodal points as described by :ref:`(63) <Eq:fem:inter:prop>`.
However, looking at Figure :ref:`Lagrange_basis_8`
we also notice that the basis function are oscillatory and have absolute
values that are significantly larger than 1 between the nodal points.
Consider for instance the basis function represented by the purple color.
It is 1 in :math:`x=0.5` and 0 at all other nodal points
and hence this basis function represents the value at the mid-point.
However, this function also has strong
negative contributions close to the element boundaries where
it takes negative values less than :math:`-2`.
For the Bernstein basis, all  functions are positive and
all functions output values in :math:`[0,1]`. Therefore there is no oscillatory behavior.
The main disadvantage of the Bernstein basis is that the basis is not
a nodal basis. In fact, all functions contribute everywhere except :math:`x=0` and :math:`x=1`.

Both Lagrange and Bernstein polynomials take larger values towards the element boundaries than in
the middle of the element, but the Bernstein polynomials always remain less or equal to 1.

We  remark that the Bernstein basis is easily extended to polygons in 2D and
3D in terms of the barycentric coordinates. For example, consider the reference triangle in
2D consisting of the faces :math:`x=0`, :math:`y=0`, and :math:`x+y=1`. The barycentric coordinates
are :math:`b_1(x,y)=x`, :math:`b_2(x,y)`, and :math:`b_3(x,y)=1-x-y` and the Bernstein basis functions
of order :math:`n` is of the form

.. math::
        
        B_{i,j,k} = \frac{n!}{i! j! k!} x^i y^j (1-x-y)^k, \quad \mbox{ for }  i+j+k = n {\thinspace .}
        

Approximation properties and convergence rates
==============================================

We will now compare the different approximation methods in terms of
accuracy and efficiency.  We consider four different series for
generating approximations: Taylor, Lagrange, sinusoidal, and
Bernstein. For all families we expect that the approximations improve
as we increase the number of basis functions in our
representations. We also expect that the computational complexity
increases. Let us therefore try to quantify the accuracy and
efficiency of the different methods in terms of the number of basis
functions :math:`N`. In the present example we consider the least square
method.

Let us consider the approximation of a Gaussian bell function, i.e.,
that the exact solution is

.. math::
        
        u_e = \exp(-(x-0.5)^2) - \exp(-0.5^2)
        

We remark that :math:`u_e` is zero in :math:`x=0` and :math:`x=1` and that
we have chosen the bell function because it cannot be expressed
as a finite sum of either polynomials or sines.
We may therefore study the behavior as :math:`N\rightarrow\infty`.

To quantify the behavior of the error as well as the
complexity of the computations we  compute the approximation
with increasing number of basis functions and time
the computations by using ``time.clock`` (returning the CPU time so far
in the program). A code example goes as
follows:

.. code-block:: python

    def convergence_rate_analysis(series_type, func):
        N_values = [2, 4, 8, 16]
        norms = []
        cpu_times = []
        for N in N_values:
    
            psi = series(series_type, N)
            t0 = time.clock()
            u, c = least_squares_non_verbose(
    	       gauss_bell, psi, Omega, False)
            t1 = time.clock()
    
            error2 = sym.lambdify([x], (func - u)**2)
            L2_norm = scipy.integrate.quad(error2, Omega[0], Omega[1])
            L2_norm = scipy.sqrt(L2_norm)
            norms.append(L2_norm[0])
            cpu_times.append(t1-t0)
    
        return N_values, norms, cpu_times

We run the analysis as follows

.. code-block:: python

    Omega = [0, 1]
    x = sym.Symbol("x")
    gaussian_bell = sym.exp(-(x-0.5)**2) - sym.exp(-0.5**2)
    step = sym.Piecewise((1, 0.25 < x), (0, True)) - \ 
           sym.Piecewise((1, 0.75 < x), (0, True))
    func = gaussian_bell
    
    import pylab
    series_types = ["Taylor", "Sinusoidal", "Bernstein", "Lagrange"]
    for series_type in series_types:
        N_values, norms, cpu_times = \ 
            convergence_rate_analysis(series_type, func)
        plt.loglog(N_values, norms)
    plt.show()

Below we list the numerical error for different :math:`N`  when approximating the Gaussian bell function.

=========  ========  ========  ========  ========  
    N         2         4         8         16     
=========  ========  ========  ========  ========  
  Taylor   9.83e-02  2.63e-03  7.83e-07  3.57e-10  
   sine    2.70e-03  6.10e-04  1.20e-04  2.17e-05  
Bernstein  2.10e-03  4.45e-05  8.73e-09  4.49e-15  
 Lagrange  2.10e-03  4.45e-05  8.73e-09  2.45e-12  
=========  ========  ========  ========  ========  

It is quite clear that the different methods have different
properties.  For example, the Lagrange basis for :math:`N=16` is 145 times
more accurate than the Taylor basis. However, Bernstein is actually
more than 500 times more accurate than the Lagrange basis! The
approximations obtained by sines are far behind the polynomial
approximations for :math:`N>4`.

The corresponding CPU time of the required computations also vary quite a bit:

=========  ======  ======  ======  =====  
    N        2       4       8       16   
=========  ======  ======  ======  =====  
  Taylor   0.0123  0.0325  0.108   0.441  
   sine    0.0113  0.0383  0.229   1.107  
Bernstein  0.0384  0.1100  0.3368  1.187  
 Lagrange  0.0807  0.3820  2.5233  26.52  
=========  ======  ======  ======  =====  

Here, the timings are in seconds.  The Taylor basis is the most
efficient and is in fact more than 60 times faster than the Lagrange
basis for :math:`N=16` (with our naive implementation of basic formulas).

In order to get a more precise idea of how the approximation methods
behave as :math:`N` increases, we investigate two simple data models which
may be used in a regression analysis.  These two are the polynomial
and exponential model defined by

.. _Eq:sec:approx:pol:model:

.. math::

    \tag{68}
    E_{1}(N) = \alpha_{1} N^{\beta_{1}}, 
        

.. _Eq:sec:approx:exp:model:

.. math::

    \tag{69}
    E_{2}(N) = \alpha_{2} \exp(\beta_2 N)
        

Taking the logarithm of :ref:`(68) <Eq:sec:approx:pol:model>` we
obtain

.. math::
        
        \log (E_1(N)) = \beta_1 \log(N) + log(\alpha_1)
        

Hence, letting :math:`x=\log(N)` be the independent variable and :math:`y=\log
(E_1(N))` the dependent one, we simply have the straight line :math:`y = a x
+ b` with :math:`a=\beta_1` and :math:`b= log(\alpha_1)`.  Then, we may perform a
regression analysis as earlier with respect to the basis functions
:math:`(1,x)` and obtain an estimate of the order of convergence in terms of
:math:`\beta_1` . For the second model :ref:`(69) <Eq:sec:approx:exp:model>`, we take
the natural logarithm and obtain

.. math::
        
        \ln (E_2(N)) = \beta_2 N + \ln(\alpha_2)
        

Again, regression analysis provides the means to estimate the convergence,
but here we let :math:`x=N` be the independent variable,
:math:`y=\ln (E_2(N))`, :math:`a=\beta_2` and :math:`b= \ln(\alpha_2)`.
To summarize, the polynomial model should have the data around a straight
line in a log-log plot, while the exponential model has its date around
a straight line in a log plot with the logarithmic scale on the :math:`y` axis.

Before we perform the regression analysis, a good rule is to inspect
the behavior visually in log and log-log plots. Figure
:ref:`fem:approx:bell:loglogfig` shows a log-log plot of the error with
respect to :math:`N` for the various methods. Clearly, the sinusoidal basis
seems to have a polynomial convergence rate as the log-log plot is a
linear line. The Bernstein, Lagrange, and Taylor methods appear to
have convergence that is faster than polynomial. It is then
interesting to consider a log plot and see if the behavior is
exponential. Figure :ref:`fem:approx:bell:semilogfig` is a log
plot. Here, the Bernstein approximation appears to be a linear line
which suggests that the convergence is exponential.

.. _fem:approx:bell:loglogfig:

.. figure:: Bell_convergence_loglog.png

   *Convergence of least square approximation using basis function in terms of the Taylor, sinusoidal, Bernstein and Lagrange basis in a log-log plot*

.. _fem:approx:bell:semilogfig:

.. figure:: Bell_convergence_semilogy.png

   *Convergence of least square approximation using basis function in terms of the Taylor, sinusoidal, Bernstein and Lagrange basis in a log plot*

The following program computes the order of convergence for
the sines using the polynomial model :ref:`(68) <Eq:sec:approx:pol:model>`
while the Bernstein approximation is estimates
in terms of model :ref:`(69) <Eq:sec:approx:exp:model>`. We avoid to
compute estimates for the Taylor and Lagrange approximations as neither
the log-log plot nor the log plot demonstrated linear behavior.

[**hpl 3**: Any comment about the ``regression_with_noise`` function?]

.. code-block:: python

    N_values = [2, 4, 8, 16, 32]
    Taylor     = [0.0983, 0.00263,  7.83e-07, 3.57e-10]
    Sinusoidal = [0.0027, 0.00061,  0.00012,  2.17e-05]
    Bernstein  = [0.0021, 4.45e-05, 8.73e-09, 4.49e-15]
    Lagrange   = [0.0021, 4.45e-05, 8.73e-09, 2.45e-12]
    
    x = sym.Symbol('x')
    psi = [1, x]
    
    u, c = regression_with_noise(log2(Sinusoidal), psi, log2(N_values))
    print "estimated model for sine: %3.2e*N**(%3.2e)" % \ 
          (2**(c[0]), c[1])
    
    # Check the numbers estimated by the model by manual inspection
    for N in N_values:
        print 2**c[0] * N**c[1]
    
    u, c = regression_with_noise(log(Bernstein), psi, N_values)
    print "estimated model for Bernstein: %3.2e*exp(%3.2e*N)" % \ 
          (exp(c[0]), c[1])
    
    # Check the numbers estimated by the model by manual inspection
    for N in N_values:
        print exp(c[0]) * exp(N * c[1])

The program estimates the sine approximation convergences as
:math:`1.43e-02\cdot N^{-2.3}`, which means that the convergence is slightly
above second order.  The Bernstein approximation on the other hand is
:math:`8.01e-02 \cdot \exp(-1.92e+00 N)`.

The CPU time in this example here would be significantly faster if the
algorithms were implemented in a compiled language like C/C++ or
Fortran and we should not be careful in drawing conclusion about the
efficiency of the different methods based on this example
alone. However, for completeness we include a log-log plot in Figure
:ref:`fem:comp:bell:loglogfig` to illustrate the polynomial increase in
CPU time with respect to N.

.. _fem:comp:bell:loglogfig:

.. figure:: Bell_computations_loglog.png

   *CPU timings of the approximation with the difference basis in a log-log plot*

The complete code can be found in
`convergence_rate_local.py <http://tinyurl.com/znpudbt/convergence_rate_local.py>`__.

