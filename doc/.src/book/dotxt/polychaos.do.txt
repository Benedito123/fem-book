Show how polynomial chaos fits right into the framework from the second
and third chapter.

We shall now look at *uncertainty analysis* of differential equations. We
assume that we have some function $u(\x,t;\zeta)$, where $\zeta$ is a
vector of *random variables* $(\zeta_0,\ldots,\zeta_N)$ that introduces
uncertainty in the problem. Note that the uncertainty is described by
$N+1$ stochastic parmeters and is therefore *discretized*. If we have
temporal or spatial stochastic processes or stochastic space-time fields,
these are continuous, and the mathematical formulations become considerably
more complicated. Assuming the uncertainty to be described by a set of
discrete stochastic *variables* represents the same simplification as we
introduced in earlier chapters where we assumed the solution to be
discrete, $u(\x)\approx\sum_ic_i\baspsi_i(\x)$, and worked with
variational forms in finite-dimensional spaces. We could, as briefly shown,
equally well worked with variational forms in *infinite-dimensional* spaces,
but the theory would involve Sobelov spaces rather than the far more
intuitive vector spaces. In the same manner, stochastic processes and fields
and their mathematical theory are avoided when assuming the uncertainty to
be already discretized in terms of random variables.

We shall look at two driving examples, the simple ODE

!bt
\begin{equation}
u'(t) = a(t)u(t),\quad u(0)=I,
\end{equation}
!et
where $a$ and $I$ can be uncertain. If $a$ is uncertain, we assume that it
consists of uncertain piecewise constants: $a=a_i$ is a random variable
in some time interval $[t_i,t_{t_{i+1}}]$. Typically, we can write
$a(t) = \sum_{i=0}^N a_i\hat\baspsi_i(\x)$, with $\hat\baspsi_i$ being P0
elements in time, and $a_i$ are the unknown coefficients (in this case we
count to $i$ from 0 to $N$ and consider $I$ as certain).

We also address stationary PDE

!bt
\begin{equation}
-\nabla\cdot(\dfc(x)\nabla u(\x)) = -f(\x)\mbox{ in }\Omega,
\end{equation}
!et
with appropriate Dirichlet, Neumann, or Robin conditions at the boundary,
The coefficients $\dfc$ and $f$ are assumed to be stochastic quantities
discretized in terms expansions, e.g., $\dfc = \sum_{i=0}^N\dfc_i\hat\baspsi_i(\x)$,
where $\hat\baspsi_i(\x)$ is some spatial discretization -- think of,
for instance, of piecewise constant P0 finite elements or global sine
functions.


A third pillar among assumption for the polynomial chaos method is that
the *random input variables are stochstically independent*. There are
methods to come around this problem, but these are very recent and we
keep the traditional assumption in the present of the theory here.

# Determining c_i

 o Least-squares minimization or Galerkin projection with exact
   computation of integrals
 o Least-squares minimization or Galerkin projection with numerical
   computation of integrals
 o Interpolation or collocation
 o Regression

Method 2 and 4 are the domininating ones. These go under the names
psedo-spectral methods and stochastic collocation methods, respectively.

===== Least-squares methods =====

A major difference for deterministic approximation methods and
polynomial chaos based on least squares is that we introduce
probabilistic information in the inner product in the latter case.
This means that *we must know the joint probability distrbution*
$p(\bm{\zeta})$ for the input parameters! Then we define the
inner product between two elements $u$ and $w$ in $V$ as

!bt
\begin{equation}
(u,v) = \E{uv} = \int_\Omega u(\bm{\zeta})v(\bm{\zeta}) p(\bm{\zeta})d\bm{\zeta}
\end{equation}
!et
with the associated norm

!bt
\[ || u ||^2 = (u,u)\tp\]
!et
The domain $\Omega$ is the hypercube of input parameters:

!bt
\[ \Omega = [\zeta_{0,\min},\zeta_{0,\max}]\times
[\zeta_{1,\min},\zeta_{1,\max}]\times\cdots\times
[\zeta_{N,\min},\zeta_{N,\max}]\tp\]
!et
Since the random input variables are independent, we can factorize
the joint probability distribution as

!bt
\[ p(\bm{\zeta}) = \Pi_{i=0}^N p_i(\zeta_i)\tp\]
We then have

!bt
\[
\E{\bm{\zeta}} = \E{\zeta_0}\E{\zeta_1}\cdots\E{\zeta_0}
= \Pi_{i=0}^N
\int\limits_{\zeta_{i,\,min}}^{\zeta_{i,max}}\zeta_i p_i(\zeta_0)d\zeta_i\tp
\]
!et

Requiring that the polynomials are orthogonal means that

!bt
\[ (u,v) = {\delta_{ij}}||u||,\]
!et
where $\delta_{ij}$ is the Kronecker delta: $\delta_{ij}=1$ if and only if
$i=j$, otherwise $\delta_{ij}=0$.

Let $f$ be the solution computed by our mathematical model, either
approximately by a numerical method or exact.
When we do the least squares minimization, we know from Chapter
ref{ch:approx:global} that the associated linear system is diagonal
and its solution is

!bt
\begin{equation}
c_i = \frac{(\baspsi_i, f)}{||\baspsi||^2} =
         \frac{\E{f\baspsi_i}}{\E{\baspsi^2}}\tp
label{pc:ci:ls}
\end{equation}
!et
In case of orthonormal polynomials $\baspsi_i$ we simply have
$c_i = \E{f\baspsi_i}$.

Galerkin's method or project gives

!bt
\[ (f-\sum_i c_i \baspsi_i, v) = 0,\quad\forall v\in V,\]
!et
which is equivalent to

!bt
\[ (f-\sum_i c_i \baspsi_i, \baspsi_j) = 0,\quad j=0,\ldots,N,\]
!et
and consequently the formula (ref{pc:ci:ls}) for $c_i$.

The major question, from a computational point of view, is how to
evaluate the integrals in the formula for $c_i$. Let us look at our
two primary examples.

===== Example: Least squares applied to the decay ODE =====

In this model problem, we are interested in the statistics of the
solution $u(t)$. The response is $u(t)=Ie^{-at} =
\zeta_0e^{-\zeta_1 t}$ as our $f(\zeta)$ function.
Consequently,

!bt
\[ u(\x) = \sum_{i\in\If} c_i(t)\baspsi_i(\zeta_0,\zeta_1)\tp\]
Furthermore,

!bt
\[ c_i(t) =
\frac{(\zeta_0e^{-\zeta_1 t},\baspsi_i)}{||\baspsi_i||^2}
= ||\baspsi_i||^{-2}\int\limits_{\zeta_{0,\min}}^{\zeta_{0,\max}}
\int\limits_{\zeta_{1,\min}}^{\zeta_{1,\max}}
\zeta_0e^{-\zeta_1 t}\baspsi_i(\zeta_0,\zeta_1) p_0(\zeta_0)p_1(\zeta_1)
d\zeta_0 d\zeta_1\tp
\]
!et
This is a quite heavy integral to perform by hand; it really depends on
the complexity of the probability density function $p_0$ for $I$ and
$p_1$ for $a$. A tool like SymPy or Mathematica is indispensable,
but even these will soon give up when $i$ grows.

Uniform distributions!

===== Modeling the response =====

Our aim is to approximate the mapping from input to output in a
simulation program. The output will often require solving a
differential equation. Let $f$ be the output response as a result of
solving the differential equation. We then want to find the mapping

!bt
\[ u(\x;\bm{\zeta}) = \sum_{i\in\If} c_i(\x)\baspsi_i(\bm{\zeta}),\]
!et
or if the reponse is some operation (functional) on the solution $u$,
say $f(u)$, we seek the mapping

!bt
\[ f(\bm{\zeta}) = \sum_{i\in\If} c_i(\x)\baspsi_i(\bm{\zeta}),\]
!et
Normally, $c_i$ will depend on the spatial coordinate $\x$ as here,
but in some cases it

Give examples with flux etc!

===== Numerical integration =====

Clenshaw-Curtis quadrature,
Gauss-Legendre quadrature,
Gauss-Patterson quadrature,
Genz-Keister quadrature,
Leja quadrature,
Monte Carlo integration,
Optimal Gaussian quadrature,

===== Stochastic collocation ======

The stochastic collocation method is nothing but what we called
regression in Chapter ref{ch:approx:global}.  We simply choose a large
number $P$ of evaluation points in the parameter space $W$ and compute
the response at these points.  Then we fit an orthogonal polynomial
expansion to these points using the principle of least squares. Here,
in this educational text, the principle of least squares sufficies,
but one should that for real-life computations a lot of difficulties
may happen, especially in large-scale problems. We then need to
stabilize the least squares computations in various ways, and for this
purpose, one needs tailored software like Chaospy to be briefly
introduced in Section ref{pc:chaospy}. Chaospy offers, for example, a
wide range of regression schemes.

Also, the choice of collocation points should make use of point families
that suit this kind of problems. Again, Chaospy offers many collections
of point families, and not surprisingly, numerical integration points
constitute popular families.


======= The Chaospy software ========
label{pc:chaospy}

idx{Chaospy software}

Refer to our ref for comparison with Dakota and Turns.

======= Intrusive polynomial chaos methods =======
label{pc:intrusive}

idx{intrusive polynomial chaos}
idx{non-intrusive polynomial chaos}

So far we have described *non-intrusive* polynomial chaos theory, which means
that the stocahstic solution method can use the underlying differential
equation solver ``as is'' without any modifications. This is a great
advantage, and it also makes the theory easier to understand.
However, one may construct *intrusive* methods that applies the mapping
()
in the *solution process* of the differential equation model.

Let us address the decay ODE, $u'=-au$, where we have the mapping
$u=\sum_i c_i\baspsi_i$ for $u$ which can be inserted in the ODE,
giving rise to a residual $R$:

!bt
\[ R = \sum_i c_i'(t)\baspsi_i(\bm{\zeta}) -
\sum_i c_i\baspsi_i(\bm{\zeta})\tp\]
Galerkin's method is a common choice for incorporating the polynomial
chaos expansion when solving differential equations, so we require
as usual $(R,v)=0$ for all $v\in V$, or expressed via basis functions:

!bt
\[ \sum_j (\baspsi_i,\baspsi_j) c_i'(t) = -a (\baspsi_i,\baspsi_i)\,\quad
i=0,\ldots,N tp\]
!et
This is now a differential equation system in $c_i$, which can be solved
by any ODE solver. However, the size of this system is $(N+1)\times (N+1)$
rather than just a scalar ODE in the original model and the non-intrusive
method. This is characteristic for non-intrusive methods: the size of
the underlying system grows exponentially with the number of uncertain
parameters, and one always ends up in high-dimensional spaces for the ODEs
or the coordinates for PDEs.