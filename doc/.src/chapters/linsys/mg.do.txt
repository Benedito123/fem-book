======= Multigrid and Domain Decomposition Methods =======
label{ch:linalg:MLDD}

The classical iterative methods and the Conjugate Gradient-like procedures
we have sketched so far are general algorithms that have proven to
be successful in a wide range of problems.
A particularly attractive feature is the simplicity of these algorithms.
However, the methods are not *optimal* in the sense that
they can solve a linear system with $n$ unknowns in $\mathcal{O}(n)$ operations.
The MILU preconditioned Conjugate Gradient method typically demands
$\mathcal{O}(n^{1+1/2d})$ operations, which means $\mathcal{O}(n^{1.17})$
in a 3D problem. RILU preconditioning in general leads to
$\mathcal{O}(n^{1.33})$ operations in 3D.
This unfavorable asymptotic behavior has motivated the research for
optimal algorithms.

Two classes of optimal iterative strategies that can solve a system with
$n$ unknowns in $\mathcal{O}(n)$ operations, are the *multigrid*
and *domain decomposition*
methods.
These methods are more complicated to formulate and analyze, and the
associated algorithms are problem dependent, both with respect to
the algorithmic details and the performance.
On parallel computers, however, multigrid and domain decomposition
algorithms are much easier to deal with than RILU-like preconditioned
iterative methods.
The recent book by Smith et al. cite{SmithBjorstadGropp96} gives
an excellent introduction to the algorithms and analysis of
domain decomposition and multigrid methods and their applications
to several types of stationary PDEs.
Our very brief introduction to the subject is meant as an appetizer for
studying cite{SmithBjorstadGropp96} and applying, for instance, Diffpack's tools for
multigrid and domain decomposition methods cite{DD9}.

===== Domain Decomposition =====

idx{domain decomposition}

We consider again our model problem $-\nabla^2u =f$ in $\Omega\in\Re^d$
with $u=g$ on the boundary $\partial\Omega$.
As the name implies, domain decomposition methods consists in decomposing
the domain $\Omega$ into subdomains $\Omega_s$, $s=1,\ldots,D$.
The basic idea is then to solve the PDE in each subdomain rather than in
the complete $\Omega$. A fundamental problem is to assign appropriate
boundary conditions on the inner non-physical boundaries.
The classical *alternating Schwarz* method accomplishes this problem
through an iterative procedure.
The subdomains must in this case be
*overlapping*.
Let $u_s^k$ be the solution on
$\Omega_s$ after $k$ iterations.
Given an initial guess $u^0_s$, we solve for $s=1,\ldots,D$ the
PDE $-\nabla^2u_s^k=f$  on $\Omega_s$ using $u_s^k=g$ on physical
boundaries. On the inner boundaries of $\Omega_s$ we apply Dirichlet
conditions with values taken from the most recently computed solutions
in the overlapping neighboring subdomains.
These values are of course not correct, but by repeating the procedure
we can hope for convergence towards the solution $u$.
This can indeed be proved for the present model problem.

The Schwarz method can be viewed
as a kind of block Gauss-Seidel method, where each block corresponds to
a subdomain. If we only use Dirichlet values on inner boundaries from
the previous iteration step, a block Jacobi-like procedure is obtained.
All the subproblems in an iteration step can now be solved in parallel.
This is a very attractive feature of the domain decomposition approach
and forms the background for many parallel PDE solvers.
Parallel computing with Diffpack is also founded on such domain
decomposition strategies cite{DPparallel}.
The subdomain solvers can be iterative or direct and perhaps based
on different discretization techniques. The approach can also be extended
to letting the physical model, i.e. the PDEs, vary among the subdomains.

Although domain decomposition can be used as an efficient
stand-alone solver, the Schwarz method is frequently applied as
preconditioner for  a Krylov subspace iteration method.
There is only need for an approximate solve on each subdomain in this case.
A popular class of
domain decomposition-based preconditioners employs *non-overlapping*
grids. We refer to cite{linalgtemplates93} for a quick overview
of domain decomposition preconditioners and to cite{SmithBjorstadGropp96} for a
thorough explanation of the ideas and the associated
computational algorithms.

The primitive alternating Schwarz method as explained above exhibits
rather slow
convergence unless it is combined with a so-called \emph{coarse grid
correction}. This means that we also
solve the PDE on the complete $\Omega$ in each iteration,
using a coarse grid. The coarse grid solution over $\Omega$ is then
combined with the
fine grid subdomain solutions. Such combination of different levels
of discretizations is the key point in *multigrid methods*.

===== Multigrid Methods =====
label{ch:linalg:ML}

idx{multigrid}
idx{linear solvers!multigrid}

In Example ref{ch:linalg:examp1} we developed a demo program for the
2D Poisson equation, where the linear system arising from a finite
difference discretization is solved by Gauss-Seidel iteration.
The animation of the error as the iteration index $k$ grows, shows that
the error is efficiently smoothed and damped during the first iterations,
but then further reduction of the error slows down.
Considering a one-dimensional equation for simplicity, $-u''=f$,
we can easily write down the Gauss-Seidel iteration in terms of
quantities $u_j^{\ell}$, where $j=1,\ldots,m$
is the spatial grid index and $\ell$ \emph{represents
the iteration number:}

!bt
\begin{equation*} 2u_j^\ell =  u_{j-1}^{\ell} + u_{j+1}^{\ell-1} + \dx^2f_j \tp \end{equation*}
!et
The error $e_i^\ell =u_i^\ell - u_i^\infty$ satisfies
the homogeneous version of the difference equation:

!bt
\begin{equation}
2e_j^\ell =  e_{j-1}^{\ell} + e_{j+1}^{\ell-1}\tp
label{linalg:MG1}
\end{equation}
!et
As explained in Section ref{ch:fdm:analdiscr}, we may anticipate that
the solution of (ref{linalg:MG1}) can be written as a sum of
wave components:

!bt
\begin{equation*} e_j^\ell = \sum_k A_k\exp{(i(kj\dx - \tilde\omega\ell\Delta t ))}\tp \end{equation*}
!et
An alternative form, which  simplifies the expressions a bit, reads

!bt
\begin{equation*} e_j^\ell = \sum_k A_k\xi^\ell\exp{(ikj\dx )},\quad \xi =
\exp{(-i\tilde\omega\Delta t )}\tp \end{equation*}
!et
Inserting a wave component in the Gauss-Seidel scheme results in

!bt
\begin{equation}
\xi = \exp{(-i\tilde\omega\Delta t)} = {\exp{(ik\dx )}\over
2 - \exp{(-ik\dx )}} \tp
label{linalg:MG2}
\end{equation}
!et
The reduction in amplitude due to one iteration is represented by
the factor $\xi$, because
$e_j^{\ell} = \xi e_j^{\ell -1}$.
 The absolute value of this
damping factor, here named $F(k\dx )$, follows from
(ref{linalg:MG2}): $F(k\dx ) =(5-4\cos k\dx )^{-1/2}$.
In the grid we can represent waves with wave
length $\lambda \rightarrow \infty$ (a constant)
down to waves with wave length $\lambda = 2\dx$. The corresponding
range of $k$ becomes $(0,\pi /\dx ]$.
Figure ref{linalg:MG:fig1} shows a graph of $F(p)$, $p=k\dx\in [0,\pi]$.
As we see, only the shortest waves are significantly damped.
This means that if the error is rough, i.e., wave components with short
wave length have significant amplitude, the Gauss-Seidel
iteration will quickly damp
these components out, and the error becomes smooth relative to the grid.
The convergence is then slow because the wave components with longer
wave length are only slightly damped from iteration to iteration.

FIGURE: [figs/GSsmoother.ps, width=400] Damping factor $F(p)$ of the error in one Gauss-Seidel iteration for the equation $-u''=f$, where $p=k\dx$ ($\lambda =2\pi /k$ being the wave length and $\dx$ being the grid cell size). label{linalg:MG:fig1}

idx{smoother (multigrid)}.

The basic observation in this analysis is that $F(p)$ decreases as
$p=k\dx $ increases. Hence, if the error is dominated by wave components
whose $p$ values are too small for efficient damping,
we can increase $\dx$.
That is, transferring the error to a coarser grid turns low-frequency
wave components into high-frequency wave components on the coarser grid.
These high-frequency components are efficiently damped by a few
Gauss-Seidel iterations.
We can repeat this process, and when the grid is coarse enough, we can
solve the error equation exactly (e.g., by a direct method).
The error must then be transferred back to the original fine grid
such that we can derive the solution of the fine-grid problem.
An iterative method that is used to damp high-frequency components of
the error on a grid is often called a
Instead of Gauss-Seidel iteration, one  can use a relaxed version of
Jacobi's method or
incomplete factorization techniques as smoothers cite{Wesseling92}.

We now assume that we have a sequence of $K$ grids, $G^q$, $q=1,\ldots,K$,
where the cell size decreases with increasing $q$. That is, $q=1$
corresponds to the coarsest grid and $q=K$ is the fine grid on which our
PDE problem of interest is posed.
With each of these grids we associate a linear system
$\A^q\u^q =\b^q$, arising from discretization of the PDE on the grid
$G^q$.
Any vector $\v^q$ of grid point values can be
transferred to
grid $G^{q-1}$ by the *restriction operator* $\bfr^q$: $\v^{q-1}=\bfr^q\v^q$.
The opposite action, i.e.,
transferring a vector $\v^{q}$ on $G^q$ to $\v^{q+1}$
on a finer grid $G^{q+1}$, is obtained by the
*prolongation operator* $\P^q$: $\v^{q+1} = \P^q\v^{q}$.
We mention briefly how the restriction and prolongation
operators can be defined. Consider a one-dimensional problem and
assume that if $G^q$ has grid size $\dx$; then $G^{q-1}$ has grid
size $2\dx$.
Going from the fine grid $G^q$ to the coarse grid $G^{q-1}$ can
be done by a local weighted average as illustrated in
Figure ref{linalg:MG:fig3}a, whereas the prolongation step can make
use of standard linear interpolation (Figure ref{linalg:MG:fig3}b).


FIGURE: [figs/MGrestrict2.xfig.eps, width=400] Example on (a) restriction and (b) prolongation on one-dimensional grids, where the cell size ratio of $G^{q-1}$ and $G^q$ equals two. The weighted restriction sets a coarse grid value $u_j$ equal to $0.25 u_{2j-1}^f + 0.5 u_{2j}^f + 0.25u_{2j+1}^f$, where superscript $f$ denotes a fine grid value and $j$ is a coarse grid-point number in the figure. At the ends, the boundary conditions (here zero) must be fulfilled. label{linalg:MG:fig3}

On every grid we can define a smoother and introduce the
notation
$S(\tilde\u^q,\v^q,\f^q,\nu_q,q)$ for running $\nu_q$
iterations of the smoother
on the system $\A^q\x^q =\f^q$, yielding the approximation $\v^q$ to $\x^q$,
with $\tilde\u^q$ as start vector for the iterations.

In the multigrid method, we start with some guess $\tilde\u^q$ for
the exact solution on some grid $G^q$ and proceed with smoothing
operations on successively coarser grids until we reach the coarsest
grid $q=1$, where we solve the associated linear system by a direct method.
One can of course also use an iterative method at the coarsest level; the point
is to solve the linear system with sufficient accuracy.
The method is commonly expressed as a recursive procedure, as the one
in Algorithm ref{linalg2:cg:alg4}. The
$\mbox{LMG} (\tilde\u^q ,\v^q ,\f^q, q)$ function in that algorithm
has the arguments
$\tilde\u^q$ for the start vector for the current iteration, $\v^q$ for
the returned (improved) approximation, $\f^q$ for the right-hand side in
the linear system, and $q$ is the grid-level number.

\algor{Multigrid method for linear systems. label{linalg2:cg:alg4}}{
routine $\mbox{LMG} (\tilde\u^q ,\v^q ,\f^q, q)$\\
\> if $q=1$\\
\>\> solve $\A^q\v^q =\f^q$ sufficiently accurately\\
\> else\\
\>\> $S(\tilde\u^q ,\v^q,\f^q,\nu_q,q)$\\
\>\> $\bfr^q = \f^q - \A^q\v^q$\\
\>\> $\f^{q-1} = \bfr^{q}\bfr^q$\\
\>\> $\tilde\u^{q-1}=\mathbf{0}$\\
\>\> for $i=1,\ldots,\gamma_q$\\
\>\>\> $\mbox{LMG}(\tilde\u^{q-1},\v^{q-1},\f^{q-1},q-1)$\\
\>\>\> $\tilde\u^{q-1} = \v^{q-1}$\\
\>\> $\v^q\leftarrow \v^q + \P^{q-1}\v^{q-1}$\\
\>\> $S(\v^q,\v^q,\f^q,\mu_q ,q)$
}

Let us write out Algorithm ref{linalg2:cg:alg4}
in detail for the case of two grids, i.e.,
$K=2$.
We start the algorithm by calling $\mbox{LMG}(\tilde\u^2,\u^2,\b,2)$,
where $\tilde\u^2$ is an initial guess for the solution $\u^2$ of the
discrete PDE problem on
the fine grid.
The algorithm then runs a smoother for $\nu_2$ iterations and
computes the residual $\bfr^2=\b -\A^2\v^2$, where now $\v^2$ is
the result after $\nu_2$ iterations of the smoother.
This is called *pre-smoothing*.
We then restrict $\bfr^2$ to $\bfr^{1}$.
In a two-grid algorithm we branch into the $q=1$ part of the LMG
routine in the next recursive call $\mbox{LMG}(\mathbf{0},\v^{1},\f^{1},1)$.
The system $\A^{1}\v^{1}=\bfr^{1}$ is then solved for the error,
represented by the symbol $\v^{1}$ on this grid level (notice that
we change the
right-hand side from $\b$ on the finest level to the residual $\bfr^q$
on coarser levels, and the solution of linear systems with a residual
on the right-hand side is then an error quantity, see
(ref{linalg:erroreq})).
In a two-grid algorithm it does not make sense to perform the inner
call to LMG more than once, hence $\gamma_k=1$.
We then proceed with prolongating the error $\v^{1}$ to the fine grid
and add this error as a correction to the result $\v^2$ obtained after the
first smoothing process. Finally, we run the smoothing procedure
$\mu_2$ times, referred to as *post-smoothing*, to improve the $\v^2$
values.

The parameters $\nu_q$, $\mu_q$, and $\gamma_q$ can be varied, yielding
different versions of the multigrid strategy.
Figure ref{linalg:MG:fig2} illustrates how the algorithm visits the
various grid levels in the particular example of $K=4$ and two
choices of $\gamma_q$: 1 and 2. This results in the well-known
$V$- and $W$-cycles. The reader is encouraged to work through
the LMG algorithm in order to understand Figure ref{linalg:MG:fig2}.
To solve a given system of linear equations by multigrid, one runs
a number of cycles, i.e., the LMG routine is embedded in a loop that runs
until a suitable termination criterion is fulfilled.

FIGURE: [figs/MGcycle.xfig.eps, width=400] The LMG algorithm and the sequence of smoothing and coarse grids solves for the particular choice of $K=4$; $\gamma_q=1$ (V-cycle) and $\gamma_q=2$ (W-cycle). label{linalg:MG:fig2}

Multigrid can also be used as a preconditioner for Conjugate Gradient-like
methods. Solving the system $\M\y =\w$ can then consist in
running one multigrid cycle on $\A\y =\w$.

The perhaps most attractive feature of multigrid is that one can prove
for many model problems that the complexity of the algorithm is
$\mathcal{O}(n)$, where $n$ is the number of unknowns in the system
cite{Wesseling92}.  Moreover, the LMG algorithm can be slightly
modified and then applied to nonlinear systems.  We refer to
cite{Wesseling92} for intuitive analysis and extension of the
concepts.

Diffpack offers extensive support for multigrid calculations, but the
description of the software tools is beyond the scope of the present
text. Readers can consult the introductory report cite{DPMGintro}.
In Section ref{ch:NS:fdm} we present a simulator for
incompressible viscous fluid flow, where a special-purpose
multigrid implementation
is used for solving a 3D Poisson equation problem
that arises in a solution algorithm for the Navier-Stokes equations.

\exer{ label{linalg2:cg:exer4}}{Implement the line SOR method in
the program from
Example ref{ch:linalg:examp1}.}

\exer{ label{linalg2:cg:exer1}}{Write the algorithm above on
implementational form, ready for coding. Store the $\A\q_j$ vectors
for computational efficiency, but otherwise try to minimize the
storage requirements.}
\answer{ME-IN 324 book.}

\exer{ label{linalg2:cg:exer2}}{Let (ref{linalg:eBort}) be the
principle for determining $\alpha_1,\ldots,\alpha_k$ in an
expansion (ref{linalg:xk:update}).
The updating formula for $\q_{k+1}$, like (ref{linalg:q:update1}) and
(ref{linalg:q:update2}), can be written more generally as
$\q_{k+1} = \z_k + \sum_{j=1}^k\beta_j\q_k$,
where different choices of $\z_k$ yield different methods.
Derive the corresponding generalized
algorithm and present it on the same form as
Algorithms ref{linalg2:cg:alg1} or ref{linalg2:cg:alg2}.
}


\exer{ label{linalg2:cg:exer3}}{Apply the relaxed Jacobi method from
page (_PROBLEM: pageref_) \pageref{ch:linalg2:relaxed:Jacobi} to the one-dimensional
error equation $[\delta_x\delta_x e]_j =0$ and analyze its damping
properties.
}
